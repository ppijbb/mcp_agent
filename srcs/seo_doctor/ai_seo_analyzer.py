#!/usr/bin/env python3
"""
AI SEO Analyzer
Gemini 2.5 Flashë¥¼ í™œìš©í•œ SEO ë°ì´í„° ë¶„ì„ ë° ì¶”ì²œ ìƒì„±
"""

import logging
from typing import Dict, List, Any
import google.generativeai as genai
from .config_loader import seo_config

logger = logging.getLogger(__name__)


class SEOAIAnalyzer:
    """SEO ë°ì´í„° AI ë¶„ì„ê¸° - Gemini 2.5 Flash í™œìš©"""

    def __init__(self):
        self.model_name = seo_config.get_ai_model_config()
        self.api_key = seo_config.get_ai_api_key()
        self.prompts = seo_config.get_analysis_prompts()

        # Gemini ì„¤ì •
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)

        logger.info(f"SEO AI ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.model_name}")

    async def analyze_lighthouse_data(self, lighthouse_data: Dict[str, Any]) -> Dict[str, Any]:
        """Lighthouse ë°ì´í„° ë¶„ì„ ë° SEO ì§„ë‹¨ ìƒì„±"""
        try:
            if not lighthouse_data or lighthouse_data.get('error'):
                return {
                    "analysis": "Lighthouse ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "recommendations": [],
                    "critical_issues": [],
                    "quick_fixes": []
                }

            # ë¶„ì„ìš© ë°ì´í„° ì¤€ë¹„
            analysis_input = self._prepare_lighthouse_analysis_input(lighthouse_data)

            # Geminië¥¼ í†µí•œ ë¶„ì„
            prompt = self._build_seo_analysis_prompt(analysis_input)
            response = await self._generate_analysis(prompt)

            # ê²°ê³¼ íŒŒì‹± ë° êµ¬ì¡°í™”
            analysis_result = self._parse_seo_analysis(response, lighthouse_data)

            logger.info(f"Lighthouse ë°ì´í„° ë¶„ì„ ì™„ë£Œ - ì ìˆ˜: {lighthouse_data.get('overall_score', 0)}")
            return analysis_result

        except Exception as e:
            logger.error(f"Lighthouse ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "analysis": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "recommendations": [],
                "critical_issues": [],
                "quick_fixes": []
            }

    async def analyze_competitors(self, competitors_data: List[Dict[str, Any]],
                                 target_url: str) -> List[Dict[str, Any]]:
        """ê²½ìŸì‚¬ ë°ì´í„° ë¶„ì„ ë° ì „ëµì  ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            if not competitors_data:
                return []

            competitor_analyses = []

            for competitor in competitors_data:
                # ê²½ìŸì‚¬ë³„ ë¶„ì„
                prompt = self._build_competitor_analysis_prompt(competitor, target_url)
                response = await self._generate_analysis(prompt)

                # ê²°ê³¼ íŒŒì‹±
                analysis = self._parse_competitor_analysis(response, competitor)
                competitor_analyses.append(analysis)

            logger.info(f"ê²½ìŸì‚¬ ë¶„ì„ ì™„ë£Œ - {len(competitor_analyses)}ê°œ ê²½ìŸì‚¬")
            return competitor_analyses

        except Exception as e:
            logger.error(f"ê²½ìŸì‚¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []

    async def generate_seo_prescription(self,
                                       lighthouse_analysis: Dict[str, Any],
                                       competitor_analyses: List[Dict[str, Any]],
                                       target_url: str) -> Dict[str, Any]:
        """í†µí•© SEO ì²˜ë°©ì „ ìƒì„±"""
        try:
            # í†µí•© ë¶„ì„ ë°ì´í„° ì¤€ë¹„
            combined_data = {
                "target_url": target_url,
                "lighthouse_analysis": lighthouse_analysis,
                "competitor_analyses": competitor_analyses
            }

            # Geminië¥¼ í†µí•œ ì²˜ë°©ì „ ìƒì„±
            prompt = self._build_prescription_prompt(combined_data)
            response = await self._generate_analysis(prompt)

            # ê²°ê³¼ íŒŒì‹±
            prescription = self._parse_prescription(response)

            logger.info("SEO ì²˜ë°©ì „ ìƒì„± ì™„ë£Œ")
            return prescription

        except Exception as e:
            logger.error(f"ì²˜ë°©ì „ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "emergency_treatment": [],
                "weekly_medicine": [],
                "monthly_checkup": [],
                "competitive_moves": [],
                "expected_results": "ì²˜ë°©ì „ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            }

    def _prepare_lighthouse_analysis_input(self, lighthouse_data: Dict[str, Any]) -> str:
        """Lighthouse ë°ì´í„°ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„"""
        scores = lighthouse_data.get('scores', {})
        metrics = lighthouse_data.get('metrics', {})
        issues = lighthouse_data.get('issues', [])

        input_text = f"""
URL: {lighthouse_data.get('url', 'N/A')}
ì „ì²´ ì ìˆ˜: {lighthouse_data.get('overall_score', 0)}/100

ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜:
- ì„±ëŠ¥: {scores.get('performance', 0)}/100
- SEO: {scores.get('seo', 0)}/100
- ì ‘ê·¼ì„±: {scores.get('accessibility', 0)}/100
- ëª¨ë²” ì‚¬ë¡€: {scores.get('best_practices', 0)}/100

ì£¼ìš” ë©”íŠ¸ë¦­:
- LCP (ìµœëŒ€ ì½˜í…ì¸  í‘œì‹œ ì‹œê°„): {metrics.get('lcp', 'N/A')}
- FCP (ì²« ì½˜í…ì¸  í‘œì‹œ ì‹œê°„): {metrics.get('fcp', 'N/A')}
- CLS (ëˆ„ì  ë ˆì´ì•„ì›ƒ ì´ë™): {metrics.get('cls', 'N/A')}
- TBT (ì´ ì°¨ë‹¨ ì‹œê°„): {metrics.get('tbt', 'N/A')}
- Speed Index: {metrics.get('speed_index', 'N/A')}

ë°œê²¬ëœ ë¬¸ì œì  ({len(issues)}ê°œ):
""" + "\n".join([f"- {issue}" for issue in issues[:10]])

        return input_text

    def _build_seo_analysis_prompt(self, analysis_input: str) -> str:
        """SEO ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = self.prompts.get('seo_analysis', '')

        return f"""
{base_prompt}

{analysis_input}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. í˜„ì¬ SEO ìƒíƒœ í‰ê°€ (í•œ ë¬¸ì¥ ìš”ì•½)
2. ì£¼ìš” ë¬¸ì œì  3-5ê°œ (ìš°ì„ ìˆœìœ„ ìˆœ)
3. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹ ë¥¸ ìˆ˜ì •ì‚¬í•­ 3-5ê°œ
4. ë‹¨ê³„ë³„ ê°œì„  ë°©ì•ˆ
5. ì˜ˆìƒ íšŒë³µ ì‹œê°„ ë° ì „ëµ
"""

    def _build_competitor_analysis_prompt(self, competitor: Dict[str, Any],
                                         target_url: str) -> str:
        """ê²½ìŸì‚¬ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = self.prompts.get('competitor_analysis', '')

        return f"""
{base_prompt}

ë¶„ì„ ëŒ€ìƒ ì‚¬ì´íŠ¸: {target_url}
ê²½ìŸì‚¬: {competitor.get('url', 'N/A')}

ê²½ìŸì‚¬ ë°ì´í„°:
{competitor}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. ê²½ìŸì‚¬ SEO ê°•ì  ë¶„ì„
2. ê²½ìŸì‚¬ ì•½ì  ë° ì·¨ì•½ì 
3. í›”ì¹  ë§Œí•œ ì „ìˆ  3-5ê°œ
4. ì½˜í…ì¸  ê°­ ë¶„ì„
5. ì°¨ë³„í™” ì „ëµ ì œì•ˆ
"""

    def _build_prescription_prompt(self, combined_data: Dict[str, Any]) -> str:
        """ì²˜ë°©ì „ ìƒì„± í”„ë¡¬í”„íŠ¸"""
        lighthouse_analysis = combined_data['lighthouse_analysis']
        competitor_analyses = combined_data['competitor_analyses']

        competitor_summary = "\n".join([
            f"- {comp.get('competitor_url', 'N/A')}: {comp.get('threat_level', 'N/A')}"
            for comp in competitor_analyses[:5]
        ])

        return f"""
ë‹¤ìŒ SEO ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì²˜ë°©ì „ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ì´íŠ¸: {combined_data['target_url']}

Lighthouse ë¶„ì„ ê²°ê³¼:
{lighthouse_analysis.get('analysis', 'N/A')}

ê²½ìŸì‚¬ ë¶„ì„ ìš”ì•½:
{competitor_summary}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì²˜ë°©ì „ì„ ì œê³µí•´ì£¼ì„¸ìš”:
1. ì‘ê¸‰ ì²˜ì¹˜ (ì¦‰ì‹œ ì‹¤í–‰í•´ì•¼ í•  3-5ê°œ í•­ëª©)
2. ì£¼ê°„ ì²˜ë°© (ë§¤ì£¼ ì‹¤í–‰í•  3-5ê°œ í•­ëª©)
3. ì›”ê°„ ì²´í¬ì—… (ë§¤ì›” ì ê²€í•  3-5ê°œ í•­ëª©)
4. ê²½ìŸ ìš°ìœ„ í™•ë³´ ì „ëµ (3-5ê°œ í•­ëª©)
5. ì˜ˆìƒ ê²°ê³¼ (êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨)
"""

    async def _generate_analysis(self, prompt: str) -> str:
        """Geminië¥¼ í†µí•œ ë¶„ì„ ìƒì„±"""
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.error(f"Gemini ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {e}")
            raise

    def _parse_seo_analysis(self, response: str, lighthouse_data: Dict[str, Any]) -> Dict[str, Any]:
        """SEO ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        return {
            "analysis": response,
            "recommendations": self._extract_recommendations(response),
            "critical_issues": lighthouse_data.get('issues', [])[:5],
            "quick_fixes": self._extract_quick_fixes(response)
        }

    def _parse_competitor_analysis(self, response: str, competitor: Dict[str, Any]) -> Dict[str, Any]:
        """ê²½ìŸì‚¬ ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        return {
            "competitor_url": competitor.get('url', 'N/A'),
            "analysis": response,
            "threat_level": self._assess_threat_level(response),
            "steal_worthy_tactics": self._extract_tactics(response),
            "vulnerabilities": self._extract_vulnerabilities(response),
            "content_gaps": self._extract_content_gaps(response)
        }

    def _parse_prescription(self, response: str) -> Dict[str, Any]:
        """ì²˜ë°©ì „ íŒŒì‹±"""
        return {
            "emergency_treatment": self._extract_list_items(response, "ì‘ê¸‰ ì²˜ì¹˜"),
            "weekly_medicine": self._extract_list_items(response, "ì£¼ê°„ ì²˜ë°©"),
            "monthly_checkup": self._extract_list_items(response, "ì›”ê°„ ì²´í¬ì—…"),
            "competitive_moves": self._extract_list_items(response, "ê²½ìŸ ìš°ìœ„"),
            "expected_results": self._extract_expected_results(response)
        }

    def _extract_recommendations(self, text: str) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ì¶”ì¶œ"""
        lines = text.split('\n')
        recommendations = []
        for line in lines:
            if any(keyword in line.lower() for keyword in ['ì¶”ì²œ', 'recommend', 'ê°œì„ ', 'ë°©ì•ˆ']):
                clean_line = line.strip().lstrip('- ').lstrip('* ').lstrip('1234567890. ')
                if len(clean_line) > 10:
                    recommendations.append(clean_line)
        return recommendations[:5]

    def _extract_quick_fixes(self, text: str) -> List[str]:
        """ë¹ ë¥¸ ìˆ˜ì •ì‚¬í•­ ì¶”ì¶œ"""
        lines = text.split('\n')
        fixes = []
        for line in lines:
            if any(keyword in line.lower() for keyword in ['ë¹ ë¥¸', 'quick', 'ì¦‰ì‹œ', 'ìˆ˜ì •']):
                clean_line = line.strip().lstrip('- ').lstrip('* ').lstrip('1234567890. ')
                if len(clean_line) > 10:
                    fixes.append(clean_line)
        return fixes[:5]

    def _assess_threat_level(self, text: str) -> str:
        """ìœ„í˜‘ ë ˆë²¨ í‰ê°€"""
        if any(keyword in text.lower() for keyword in ['ì§€ë°°', 'dominating', 'ê°•ë ¥']):
            return "ğŸ‘‘ ì§€ë°°ì¤‘"
        elif any(keyword in text.lower() for keyword in ['ìƒìŠ¹', 'rising', 'ê¸‰ì¦']):
            return "ğŸ“ˆ ê¸‰ìƒìŠ¹"
        elif any(keyword in text.lower() for keyword in ['í•˜ë½', 'declining', 'ì•½í™”']):
            return "ğŸ“‰ í•˜ë½"
        elif any(keyword in text.lower() for keyword in ['ì•½í•¨', 'weak', 'ì·¨ì•½']):
            return "ğŸ˜´ ì•½í•¨"
        else:
            return "â¡ï¸ ì•ˆì •"

    def _extract_tactics(self, text: str) -> List[str]:
        """ì „ìˆ  ì¶”ì¶œ"""
        return self._extract_list_items(text, "ì „ìˆ ")

    def _extract_vulnerabilities(self, text: str) -> List[str]:
        """ì•½ì  ì¶”ì¶œ"""
        return self._extract_list_items(text, "ì•½ì ")

    def _extract_content_gaps(self, text: str) -> List[str]:
        """ì½˜í…ì¸  ê°­ ì¶”ì¶œ"""
        return self._extract_list_items(text, "ê°­")

    def _extract_list_items(self, text: str, keyword: str) -> List[str]:
        """íŠ¹ì • í‚¤ì›Œë“œ ì„¹ì…˜ì˜ ë¦¬ìŠ¤íŠ¸ í•­ëª© ì¶”ì¶œ"""
        lines = text.split('\n')
        items = []
        in_section = False

        for line in lines:
            if keyword.lower() in line.lower():
                in_section = True
                continue

            if in_section:
                if line.strip().startswith(('-', '*', 'â€¢')) or line.strip()[0:2].replace('.', '').isdigit():
                    clean_line = line.strip().lstrip('- ').lstrip('* ').lstrip('â€¢ ').lstrip('1234567890. ')
                    if len(clean_line) > 5:
                        items.append(clean_line)
                elif line.strip() == '':
                    continue
                elif not line.strip().startswith((' ', '\t')):
                    in_section = False

        return items[:5]

    def _extract_expected_results(self, text: str) -> str:
        """ì˜ˆìƒ ê²°ê³¼ ì¶”ì¶œ"""
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if 'ì˜ˆìƒ ê²°ê³¼' in line or 'expected results' in line.lower():
                # ë‹¤ìŒ ëª‡ ì¤„ì„ ê²°í•©
                result_lines = lines[i:i+5]
                return ' '.join([l.strip() for l in result_lines if l.strip()])
        return "SEO ê°œì„ ì„ í†µí•œ íŠ¸ë˜í”½ ì¦ê°€ ì˜ˆìƒ"
