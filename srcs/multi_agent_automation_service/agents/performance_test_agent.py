"""
ì„±ëŠ¥ ë° í…ŒìŠ¤íŠ¸ Agent
===================

ì„±ëŠ¥ ë¶„ì„, í…ŒìŠ¤íŠ¸ ìƒì„±, ë³‘ëª© ì§€ì  ë°œê²¬
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

@dataclass
class PerformanceTestResult:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    test_id: str
    timestamp: str
    performance_metrics: Dict[str, Any]
    bottlenecks_found: List[Dict[str, Any]]
    optimization_suggestions: List[str]
    test_coverage: float
    new_tests_created: List[str]
    gemini_cli_commands: List[str]

class PerformanceTestAgent:
    """ì„±ëŠ¥ ë° í…ŒìŠ¤íŠ¸ ì „ë‹´ Agent"""
    
    def __init__(self):
        # mcp_agent App ì´ˆê¸°í™”
        self.app = MCPApp(
            name="performance_test_agent",
            human_input_callback=None
        )
        
        # Agent ì„¤ì •
        self.agent = Agent(
            name="performance_tester",
            instruction="""
            ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì„±ëŠ¥ ë¶„ì„ê°€ì´ì í…ŒìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
            
            1. ì½”ë“œ ì„±ëŠ¥ ë¶„ì„ (CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ëŸ‰)
            2. ë³‘ëª© ì§€ì  ë°œê²¬ ë° ìµœì í™” ì œì•ˆ
            3. ìë™ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
            4. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
            5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            6. Gemini CLI ëª…ë ¹ì–´ ìƒì„± (ì‹¤ì œ ìµœì í™” ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©)
            
            ëª¨ë“  ì„±ëŠ¥ ë¶„ì„ì€ ì •í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
            """,
            server_names=["performance-mcp", "testing-mcp", "monitoring-mcp"],
            llm_factory=lambda: OpenAIAugmentedLLM(
                model="gpt-4",
            ),
        )
        
        self.test_history: List[PerformanceTestResult] = []
    
    async def analyze_performance(self, target_paths: List[str] = None) -> PerformanceTestResult:
        """ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        try:
            async with self.app.run() as app_context:
                context = app_context.context
                logger = app_context.logger
                
                logger.info("ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
                
                # 1. ì„±ëŠ¥ ë¶„ì„ ìš”ì²­
                analysis_prompt = f"""
                ë‹¤ìŒ ê²½ë¡œì˜ ì½”ë“œ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {target_paths or ['í˜„ì¬ ë””ë ‰í† ë¦¬']}
                
                ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
                1. CPU ì‚¬ìš©ëŸ‰ ë¶„ì„
                2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
                3. ë„¤íŠ¸ì›Œí¬ I/O ë¶„ì„
                4. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
                5. ë³‘ëª© ì§€ì  ë°œê²¬
                6. ìµœì í™” ë°©ì•ˆ ì œì•ˆ
                7. Gemini CLI ëª…ë ¹ì–´ ìƒì„± (ì‹¤ì œ ìµœì í™” ì‹¤í–‰ìš©)
                
                ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”:
                {{
                    "performance_metrics": {{
                        "cpu_usage": "í‰ê·  CPU ì‚¬ìš©ë¥ ",
                        "memory_usage": "í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ",
                        "response_time": "í‰ê·  ì‘ë‹µ ì‹œê°„",
                        "throughput": "ì²˜ë¦¬ëŸ‰"
                    }},
                    "bottlenecks_found": [
                        {{
                            "type": "ë³‘ëª© íƒ€ì… (CPU/Memory/Network/DB)",
                            "location": "ë°œìƒ ìœ„ì¹˜",
                            "severity": "high/medium/low",
                            "description": "ë³‘ëª© ì„¤ëª…",
                            "impact": "ì„±ëŠ¥ ì˜í–¥ë„"
                        }}
                    ],
                    "optimization_suggestions": ["ìµœì í™” ì œì•ˆ ëª©ë¡"],
                    "test_coverage": 0.85,
                    "new_tests_created": ["ìƒˆë¡œ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ëª©ë¡"],
                    "gemini_cli_commands": [
                        "gemini 'íŠ¹ì • í•¨ìˆ˜ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•´ì¤˜'",
                        "gemini 'ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¬¸ì œë¥¼ í•´ê²°í•´ì¤˜'",
                        "gemini 'ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•´ì¤˜'"
                    ]
                }}
                """
                
                # Agent ì‹¤í–‰
                result = await context.call_tool(
                    "performance_analysis",
                    {
                        "prompt": analysis_prompt,
                        "target_paths": target_paths
                    }
                )
                
                # ê²°ê³¼ íŒŒì‹±
                perf_data = json.loads(result.get("content", "{}"))
                
                # PerformanceTestResult ìƒì„±
                perf_result = PerformanceTestResult(
                    test_id=f"perf_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    timestamp=datetime.now().isoformat(),
                    performance_metrics=perf_data.get("performance_metrics", {}),
                    bottlenecks_found=perf_data.get("bottlenecks_found", []),
                    optimization_suggestions=perf_data.get("optimization_suggestions", []),
                    test_coverage=perf_data.get("test_coverage", 0.0),
                    new_tests_created=perf_data.get("new_tests_created", []),
                    gemini_cli_commands=perf_data.get("gemini_cli_commands", [])
                )
                
                # íˆìŠ¤í† ë¦¬ ì €ì¥
                self.test_history.append(perf_result)
                
                logger.info(f"ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ: {len(perf_result.bottlenecks_found)}ê°œ ë³‘ëª© ì§€ì  ë°œê²¬")
                
                return perf_result
                
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    async def generate_tests(self, target_paths: List[str] = None) -> PerformanceTestResult:
        """ìë™ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        try:
            async with self.app.run() as app_context:
                context = app_context.context
                
                # ì½”ë“œ ë¶„ì„
                code_analysis = await context.call_tool(
                    "analyze_code_for_testing",
                    {"target_paths": target_paths}
                )
                
                # í…ŒìŠ¤íŠ¸ ìƒì„± ìš”ì²­
                test_prompt = f"""
                ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
                {code_analysis}
                
                ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
                1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unit Tests)
                2. í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests)
                3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Performance Tests)
                4. ë³´ì•ˆ í…ŒìŠ¤íŠ¸ (Security Tests)
                5. Gemini CLI ëª…ë ¹ì–´ (ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©)
                """
                
                result = await context.call_tool(
                    "generate_tests",
                    {"prompt": test_prompt}
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                test_data = json.loads(result.get("content", "{}"))
                
                return PerformanceTestResult(
                    test_id=f"test_gen_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    timestamp=datetime.now().isoformat(),
                    performance_metrics={},
                    bottlenecks_found=[],
                    optimization_suggestions=[],
                    test_coverage=test_data.get("test_coverage", 0.0),
                    new_tests_created=test_data.get("new_tests_created", []),
                    gemini_cli_commands=test_data.get("gemini_cli_commands", [])
                )
                
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def run_benchmarks(self) -> PerformanceTestResult:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        try:
            async with self.app.run() as app_context:
                context = app_context.context
                
                # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
                benchmark_result = await context.call_tool(
                    "run_performance_benchmarks",
                    {}
                )
                
                # ë²¤ì¹˜ë§ˆí¬ ë¶„ì„
                analysis_prompt = f"""
                ë‹¤ìŒ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
                {benchmark_result}
                
                ë‹¤ìŒì„ ë¶„ì„í•˜ì„¸ìš”:
                1. ì„±ëŠ¥ ì§€í‘œ (CPU, Memory, Response Time)
                2. ì´ì „ ë²¤ì¹˜ë§ˆí¬ì™€ì˜ ë¹„êµ
                3. ì„±ëŠ¥ ê°œì„ /ì•…í™” ì§€ì 
                4. ìµœì í™” ê¶Œì¥ì‚¬í•­
                5. Gemini CLI ëª…ë ¹ì–´ (ì‹¤ì œ ìµœì í™” ì‹¤í–‰ìš©)
                """
                
                result = await context.call_tool(
                    "analyze_benchmarks",
                    {"prompt": analysis_prompt}
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                bench_data = json.loads(result.get("content", "{}"))
                
                return PerformanceTestResult(
                    test_id=f"bench_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    timestamp=datetime.now().isoformat(),
                    performance_metrics=bench_data.get("performance_metrics", {}),
                    bottlenecks_found=bench_data.get("bottlenecks_found", []),
                    optimization_suggestions=bench_data.get("optimization_suggestions", []),
                    test_coverage=0.0,
                    new_tests_created=[],
                    gemini_cli_commands=bench_data.get("gemini_cli_commands", [])
                )
                
        except Exception as e:
            print(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
    
    async def optimize_code(self, target_paths: List[str] = None) -> PerformanceTestResult:
        """ì½”ë“œ ìµœì í™” ì‹¤í–‰"""
        try:
            async with self.app.run() as app_context:
                context = app_context.context
                
                # ìµœì í™” ì „ ì„±ëŠ¥ ì¸¡ì •
                before_perf = await self.analyze_performance(target_paths)
                
                # ìµœì í™” ì‹¤í–‰
                optimization_result = await context.call_tool(
                    "optimize_code",
                    {
                        "target_paths": target_paths,
                        "bottlenecks": before_perf.bottlenecks_found
                    }
                )
                
                # ìµœì í™” í›„ ì„±ëŠ¥ ì¸¡ì •
                after_perf = await self.analyze_performance(target_paths)
                
                # ê°œì„  íš¨ê³¼ ë¶„ì„
                improvement = {
                    "cpu_improvement": f"{((before_perf.performance_metrics.get('cpu_usage', 0) - after_perf.performance_metrics.get('cpu_usage', 0)) / before_perf.performance_metrics.get('cpu_usage', 1)) * 100:.2f}%",
                    "memory_improvement": f"{((before_perf.performance_metrics.get('memory_usage', 0) - after_perf.performance_metrics.get('memory_usage', 0)) / before_perf.performance_metrics.get('memory_usage', 1)) * 100:.2f}%",
                    "response_time_improvement": f"{((before_perf.performance_metrics.get('response_time', 0) - after_perf.performance_metrics.get('response_time', 0)) / before_perf.performance_metrics.get('response_time', 1)) * 100:.2f}%"
                }
                
                return PerformanceTestResult(
                    test_id=f"optimize_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    timestamp=datetime.now().isoformat(),
                    performance_metrics=after_perf.performance_metrics,
                    bottlenecks_found=after_perf.bottlenecks_found,
                    optimization_suggestions=[f"ì„±ëŠ¥ ê°œì„  íš¨ê³¼: {improvement}"],
                    test_coverage=after_perf.test_coverage,
                    new_tests_created=after_perf.new_tests_created,
                    gemini_cli_commands=after_perf.gemini_cli_commands
                )
                
        except Exception as e:
            print(f"ì½”ë“œ ìµœì í™” ì‹¤íŒ¨: {e}")
            raise
    
    def get_performance_summary(self, perf_result: PerformanceTestResult) -> str:
        """ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        summary = f"""
ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ìš”ì•½
==================

ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:
- CPU ì‚¬ìš©ë¥ : {perf_result.performance_metrics.get('cpu_usage', 'N/A')}
- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {perf_result.performance_metrics.get('memory_usage', 'N/A')}
- ì‘ë‹µ ì‹œê°„: {perf_result.performance_metrics.get('response_time', 'N/A')}
- ì²˜ë¦¬ëŸ‰: {perf_result.performance_metrics.get('throughput', 'N/A')}

ğŸš¨ ë³‘ëª© ì§€ì : {len(perf_result.bottlenecks_found)}ê°œ
ğŸ’¡ ìµœì í™” ì œì•ˆ: {len(perf_result.optimization_suggestions)}ê°œ
ğŸ§ª í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: {perf_result.test_coverage:.2f}%
ğŸ“ ìƒˆë¡œ ìƒì„±ëœ í…ŒìŠ¤íŠ¸: {len(perf_result.new_tests_created)}ê°œ

ì£¼ìš” ë³‘ëª© ì§€ì :
"""
        
        for bottleneck in perf_result.bottlenecks_found[:5]:  # ìƒìœ„ 5ê°œë§Œ
            summary += f"- {bottleneck['type']}: {bottleneck['description']}\n"
        
        summary += f"\nGemini CLI ëª…ë ¹ì–´ ({len(perf_result.gemini_cli_commands)}ê°œ):\n"
        for cmd in perf_result.gemini_cli_commands[:3]:  # ìƒìœ„ 3ê°œë§Œ
            summary += f"- {cmd}\n"
        
        return summary
    
    def get_critical_bottlenecks(self, perf_result: PerformanceTestResult) -> List[Dict[str, Any]]:
        """ì‹¬ê°í•œ ë³‘ëª© ì§€ì ë§Œ í•„í„°ë§"""
        return [
            bottleneck for bottleneck in perf_result.bottlenecks_found
            if bottleneck.get("severity") == "high"
        ]

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    agent = PerformanceTestAgent()
    
    # ì„±ëŠ¥ ë¶„ì„
    perf_result = await agent.analyze_performance()
    print(agent.get_performance_summary(perf_result))
    
    # í…ŒìŠ¤íŠ¸ ìƒì„±
    test_result = await agent.generate_tests()
    print(f"ìƒì„±ëœ í…ŒìŠ¤íŠ¸: {len(test_result.new_tests_created)}ê°œ")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    bench_result = await agent.run_benchmarks()
    print(f"ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {bench_result.performance_metrics}")
    
    # ì½”ë“œ ìµœì í™”
    optimize_result = await agent.optimize_code()
    print(f"ìµœì í™” ì™„ë£Œ: {optimize_result.optimization_suggestions}")

if __name__ == "__main__":
    asyncio.run(main()) 