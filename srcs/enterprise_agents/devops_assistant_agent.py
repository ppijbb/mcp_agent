"""
DevOps Assistant Agent - Real MCP Agent Implementation
======================================================
MCP ê¸°ë°˜ ê°œë°œìž ìƒì‚°ì„± ìžë™í™” ì—ì´ì „íŠ¸

Features:
- ðŸ” GitHub ì½”ë“œ ë¦¬ë·° ìžë™í™”
- ðŸš€ CI/CD íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§
- ðŸŽ¯ ì´ìŠˆ ìš°ì„ ìˆœìœ„ ë¶„ì„
- ðŸ‘¥ íŒ€ ìŠ¤íƒ ë“œì—… ìžë™ ìƒì„±
- ðŸ“Š ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”

Model: gemini-2.5-flash-lite-preview-0607
"""

import asyncio
import os
import json
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum

# MCP Agent imports
from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.orchestrator.orchestrator import Orchestrator
from mcp_agent.workflows.llm.augmented_llm import RequestParams
from mcp_agent.workflows.llm.augmented_llm_google import GoogleAugmentedLLM

@dataclass
class CodeReviewRequest:
    """ì½”ë“œ ë¦¬ë·° ìš”ì²­"""
    owner: str
    repo: str
    pull_number: int
    title: str = ""
    author: str = ""
    changes_summary: str = ""

@dataclass
class DeploymentStatus:
    """ë°°í¬ ìƒíƒœ"""
    service: str
    status: str
    last_deployed: str
    health_check: str
    error_count: int = 0

@dataclass
class IssueAnalysis:
    """ì´ìŠˆ ë¶„ì„ ê²°ê³¼"""
    issue_id: int
    title: str
    priority: str  # high, medium, low
    category: str  # bug, feature, security
    estimated_hours: int
    assigned_to: str = ""

@dataclass
class TeamActivity:
    """íŒ€ í™œë™ ë°ì´í„°"""
    team_name: str
    commits_today: int
    prs_opened: int
    prs_merged: int
    issues_resolved: int
    build_success_rate: float
    avg_review_time: float

class DevOpsTaskType(Enum):
    """DevOps ìž‘ì—… íƒ€ìž…"""
    CODE_REVIEW = "ðŸ” ì½”ë“œ ë¦¬ë·°"
    DEPLOYMENT_CHECK = "ðŸš€ ë°°í¬ ìƒíƒœ í™•ì¸"
    ISSUE_ANALYSIS = "ðŸŽ¯ ì´ìŠˆ ë¶„ì„"
    TEAM_STANDUP = "ðŸ‘¥ íŒ€ ìŠ¤íƒ ë“œì—…"
    PERFORMANCE_ANALYSIS = "ðŸ“Š ì„±ëŠ¥ ë¶„ì„"
    SECURITY_SCAN = "ðŸ”’ ë³´ì•ˆ ìŠ¤ìº”"

@dataclass
class DevOpsResult:
    """DevOps ìž‘ì—… ê²°ê³¼"""
    task_type: DevOpsTaskType
    status: str
    result_data: Dict[str, Any]
    recommendations: List[str]
    timestamp: str
    processing_time: float

class DevOpsAssistantMCPAgent:
    """
    ðŸš€ DevOps Assistant MCP Agent
    
    Features:
    - GitHub ì½”ë“œ ë¦¬ë·° ìžë™í™”
    - CI/CD íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§  
    - ì´ìŠˆ ìš°ì„ ìˆœìœ„ ë¶„ì„
    - íŒ€ ìŠ¤íƒ ë“œì—… ì¤€ë¹„
    - ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”
    - ë³´ì•ˆ ìŠ¤ìº” ë° ê¶Œìž¥ì‚¬í•­
    
    Model: gemini-2.5-flash-lite-preview-0607
    """
    
    def __init__(self, output_dir: str = "devops_assistant_reports"):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # MCP App ì´ˆê¸°í™”
        from srcs.common.utils import setup_agent_app
        self.app = setup_agent_app("devops_assistant")
        
        # DevOps ìž‘ì—… ížˆìŠ¤í† ë¦¬
        self.task_history: List[DevOpsResult] = []
        self.active_monitors: Dict[str, Any] = {}
        self.team_metrics: Dict[str, TeamActivity] = {}
        
        # ì„¤ì •
        self.model_name = "gemini-2.5-flash-lite-preview-0607"
        self.default_review_criteria = [
            "ì½”ë“œ í’ˆì§ˆ ë° ê°€ë…ì„±",
            "ë³´ì•ˆ ì·¨ì•½ì ",
            "ì„±ëŠ¥ ìµœì í™”",
            "í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€",
            "ë¬¸ì„œí™” ìˆ˜ì¤€"
        ]
        
    async def analyze_code_review(self, request: CodeReviewRequest) -> DevOpsResult:
        """
        GitHub Pull Request ì½”ë“œ ë¦¬ë·° ë¶„ì„
        """
        start_time = datetime.now()
        
        async with self.app.run() as agent_app:
            # LLMì„ í†µí•œ ì½”ë“œ ë¦¬ë·° ìƒì„±
            llm = GoogleAugmentedLLM(model=self.model_name)
            
            analysis_prompt = f"""
            ë‹¤ìŒ Pull Requestë¥¼ ì „ë¬¸ ê°œë°œìž ê´€ì ì—ì„œ ë¦¬ë·°í•´ì£¼ì„¸ìš”:

            **Repository**: {request.owner}/{request.repo}
            **PR #{request.pull_number}**: {request.title}
            **ìž‘ì„±ìž**: {request.author}
            **ë³€ê²½ì‚¬í•­**: {request.changes_summary}

            **ë¦¬ë·° ê¸°ì¤€**:
            1. ì½”ë“œ í’ˆì§ˆ ë° ê°€ë…ì„±
            2. ë³´ì•ˆ ì·¨ì•½ì  ì²´í¬
            3. ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥ì„±
            4. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
            5. ë¬¸ì„œí™” ë° ì£¼ì„

            **ì¶œë ¥ í˜•ì‹**:
            - ì „ì²´ í‰ê°€: [A/B/C/D]
            - ì£¼ìš” ê°•ì : [3ê°œ ì´í•˜]
            - ê°œì„  í•„ìš”ì‚¬í•­: [êµ¬ì²´ì  ì œì•ˆ]
            - ë³´ì•ˆ ì²´í¬í¬ì¸íŠ¸: [ë°œê²¬ëœ ì´ìŠˆ]
            - ê¶Œìž¥ ì•¡ì…˜: [ìŠ¹ì¸/ìˆ˜ì •ìš”ì²­/ìž¬ê²€í† ]

            ê±´ì„¤ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            response = await llm.generate(
                RequestParams(
                    prompt=analysis_prompt,
                    temperature=0.2,
                    max_tokens=1000
                )
            )
            
            # Mock ë°ì´í„°ë¡œ ì‹¤ì œ GitHub API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            mock_pr_data = {
                "files_changed": 5,
                "additions": 142,
                "deletions": 38,
                "commits": 3,
                "reviewers": ["senior-dev", "tech-lead"],
                "ci_status": "passing",
                "conflicts": False
            }
            
            recommendations = [
                f"ì½”ë“œ ë¦¬ë·° ì™„ë£Œ: {request.owner}/{request.repo}#{request.pull_number}",
                "CI/CD íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸ í•„ìš”",
                "í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ ìœ ì§€ ê¶Œìž¥",
                "ë³´ì•ˆ ìŠ¤ìº” ê²°ê³¼ ê²€í†  í•„ìš”"
            ]
            
            result = DevOpsResult(
                task_type=DevOpsTaskType.CODE_REVIEW,
                status="completed",
                result_data={
                    "pr_info": asdict(request),
                    "github_data": mock_pr_data,
                    "review_content": response.strip(),
                    "review_criteria": self.default_review_criteria
                },
                recommendations=recommendations,
                timestamp=datetime.now(timezone.utc).isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.task_history.append(result)
            await self._save_result(result)
            
            return result
    
    async def check_deployment_status(self, service_name: str, environment: str = "production") -> DevOpsResult:
        """
        ë°°í¬ ìƒíƒœ í™•ì¸ ë° ë¶„ì„
        """
        start_time = datetime.now()
        
        async with self.app.run() as agent_app:
            llm = GoogleAugmentedLLM(model=self.model_name)
            
            # Mock ë°°í¬ ë°ì´í„°
            mock_deployment_data = {
                "service": service_name,
                "environment": environment,
                "last_deployment": "2025-01-21T10:30:00Z",
                "status": "healthy",
                "replicas": {"desired": 3, "ready": 3, "available": 3},
                "health_checks": {"passing": 2, "failing": 1},
                "error_rate": "0.2%",
                "response_time": "145ms",
                "cpu_usage": "45%",
                "memory_usage": "62%"
            }
            
            analysis_prompt = f"""
            ë‹¤ìŒ ì„œë¹„ìŠ¤ì˜ ë°°í¬ ìƒíƒœë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            **ì„œë¹„ìŠ¤**: {service_name}
            **í™˜ê²½**: {environment}
            **ë°°í¬ ë°ì´í„°**: {json.dumps(mock_deployment_data, indent=2)}

            **ë¶„ì„ ìš”ì²­**:
            1. í˜„ìž¬ ì„œë¹„ìŠ¤ ìƒíƒœ ì¢…í•© í‰ê°€
            2. ìž ìž¬ì  ìœ„í—˜ ìš”ì†Œ ì‹ë³„
            3. ì„±ëŠ¥ ìµœì í™” ê¸°íšŒ
            4. ëª¨ë‹ˆí„°ë§ ì•ŒëžŒ í•„ìš”ì„±
            5. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•œ ì´ìŠˆ

            **ì¶œë ¥ í˜•ì‹**:
            - ì „ì²´ ìƒíƒœ: [ì •ìƒ/ì£¼ì˜/ìœ„í—˜]
            - ìœ„í—˜ë„: [ë‚®ìŒ/ë³´í†µ/ë†’ìŒ]
            - ì£¼ìš” ë©”íŠ¸ë¦­: [í•µì‹¬ ì§€í‘œ 3ê°œ]
            - ê¶Œìž¥ ì¡°ì¹˜: [ìš°ì„ ìˆœìœ„ë³„]

            ìš´ì˜ ê´€ì ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            response = await llm.generate(
                RequestParams(
                    prompt=analysis_prompt,
                    temperature=0.1,
                    max_tokens=800
                )
            )
            
            recommendations = [
                f"{service_name} ì„œë¹„ìŠ¤ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ",
                "í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ 1ê±´ ì¡°ì‚¬ í•„ìš”",
                "CPU ì‚¬ìš©ë¥  45% - ì •ìƒ ë²”ìœ„",
                "ì‘ë‹µì‹œê°„ 145ms - ì„±ëŠ¥ ì–‘í˜¸",
                "ì—ëŸ¬ìœ¨ 0.2% - í—ˆìš© ë²”ìœ„ ë‚´"
            ]
            
            result = DevOpsResult(
                task_type=DevOpsTaskType.DEPLOYMENT_CHECK,
                status="completed",
                result_data={
                    "deployment_info": mock_deployment_data,
                    "analysis": response.strip(),
                    "environment": environment
                },
                recommendations=recommendations,
                timestamp=datetime.now(timezone.utc).isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.task_history.append(result)
            await self._save_result(result)
            
            return result
    
    async def analyze_issues(self, owner: str, repo: str) -> DevOpsResult:
        """
        GitHub ì´ìŠˆ ìš°ì„ ìˆœìœ„ ë¶„ì„
        """
        start_time = datetime.now()
        
        async with self.app.run() as agent_app:
            llm = GoogleAugmentedLLM(model=self.model_name)
            
            # Mock ì´ìŠˆ ë°ì´í„°
            mock_issues = [
                {
                    "id": 123,
                    "title": "Login fails with OAuth providers",
                    "labels": ["bug", "critical", "oauth"],
                    "created": "2025-01-20",
                    "description": "Users report login failures when using Google/GitHub OAuth"
                },
                {
                    "id": 124,
                    "title": "Add dark mode theme",
                    "labels": ["enhancement", "ui"],
                    "created": "2025-01-19",
                    "description": "Implement dark mode for better user experience"
                },
                {
                    "id": 125,
                    "title": "SQL injection vulnerability in search",
                    "labels": ["security", "critical"],
                    "created": "2025-01-18", 
                    "description": "Search endpoint vulnerable to SQL injection attacks"
                }
            ]
            
            analysis_prompt = f"""
            ë‹¤ìŒ GitHub ì´ìŠˆë“¤ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë¶„ì„í•˜ê³  ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

            **Repository**: {owner}/{repo}
            **ì´ìŠˆ ëª©ë¡**: {json.dumps(mock_issues, indent=2, ensure_ascii=False)}

            **ë¶„ì„ ê¸°ì¤€**:
            1. ì‚¬ìš©ìž ì˜í–¥ë„ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)
            2. ë³´ì•ˆ ì¤‘ìš”ë„ (ê¸´ê¸‰/ì¤‘ìš”/ì¼ë°˜)
            3. ê¸°ìˆ ì  ë³µìž¡ì„± (ë³µìž¡/ë³´í†µ/ë‹¨ìˆœ)
            4. ë¹„ì¦ˆë‹ˆìŠ¤ ìš°ì„ ìˆœìœ„
            5. ì˜ˆìƒ ìž‘ì—… ì‹œê°„

            **ê° ì´ìŠˆë³„ ë¶„ì„ ê²°ê³¼**:
            - ìš°ì„ ìˆœìœ„: [P0/P1/P2/P3]
            - ì¹´í…Œê³ ë¦¬: [ë²„ê·¸/ê¸°ëŠ¥/ë³´ì•ˆ/ê°œì„ ]
            - ì˜ˆìƒ ì‹œê°„: [ì‹œê°„ ë‹¨ìœ„]
            - ë‹´ë‹¹ìž ì¶”ì²œ: [ì—­í• ë³„]
            - í•´ê²° ë°©í–¥: [êµ¬ì²´ì  ë°©ë²•]

            ê°œë°œíŒ€ì˜ ìƒì‚°ì„±ì„ ê³ ë ¤í•œ ì‹¤ìš©ì ì¸ ìš°ì„ ìˆœìœ„ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
            """
            
            response = await llm.generate(
                RequestParams(
                    prompt=analysis_prompt,
                    temperature=0.2,
                    max_tokens=1200
                )
            )
            
            # ì´ìŠˆ ë¶„ì„ ê²°ê³¼ ìƒì„±
            analyzed_issues = []
            for issue in mock_issues:
                if "critical" in issue["labels"]:
                    priority = "P0" if "security" in issue["labels"] else "P1"
                    estimated_hours = 8 if "security" in issue["labels"] else 6
                elif "bug" in issue["labels"]:
                    priority = "P2"
                    estimated_hours = 4
                else:
                    priority = "P3"
                    estimated_hours = 16
                    
                analyzed_issues.append(IssueAnalysis(
                    issue_id=issue["id"],
                    title=issue["title"],
                    priority=priority,
                    category="security" if "security" in issue["labels"] else "bug" if "bug" in issue["labels"] else "feature",
                    estimated_hours=estimated_hours,
                    assigned_to="security-team" if "security" in issue["labels"] else "backend-team"
                ))
            
            recommendations = [
                f"{len(mock_issues)}ê°œ ì´ìŠˆ ìš°ì„ ìˆœìœ„ ë¶„ì„ ì™„ë£Œ",
                "ë³´ì•ˆ ì´ìŠˆ 1ê±´ ì¦‰ì‹œ ì²˜ë¦¬ í•„ìš” (P0)",
                "Critical ë²„ê·¸ 1ê±´ ë‹¹ì¼ ì²˜ë¦¬ ê¶Œìž¥ (P1)",
                "UI ê°œì„ ì‚¬í•­ ìŠ¤í”„ë¦°íŠ¸ ë°±ë¡œê·¸ ì¶”ê°€ (P3)"
            ]
            
            result = DevOpsResult(
                task_type=DevOpsTaskType.ISSUE_ANALYSIS,
                status="completed",
                result_data={
                    "repository": f"{owner}/{repo}",
                    "raw_issues": mock_issues,
                    "analyzed_issues": [asdict(issue) for issue in analyzed_issues],
                    "analysis": response.strip()
                },
                recommendations=recommendations,
                timestamp=datetime.now(timezone.utc).isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.task_history.append(result)
            await self._save_result(result)
            
            return result
    
    async def generate_team_standup(self, team_name: str) -> DevOpsResult:
        """
        íŒ€ ìŠ¤íƒ ë“œì—… ìš”ì•½ ìƒì„±
        """
        start_time = datetime.now()
        
        async with self.app.run() as agent_app:
            llm = GoogleAugmentedLLM(model=self.model_name)
            
            # Mock íŒ€ í™œë™ ë°ì´í„°
            team_activity = TeamActivity(
                team_name=team_name,
                commits_today=15,
                prs_opened=4,
                prs_merged=3,
                issues_resolved=7,
                build_success_rate=94.5,
                avg_review_time=2.3
            )
            
            self.team_metrics[team_name] = team_activity
            
            analysis_prompt = f"""
            ë‹¤ìŒ íŒ€ì˜ 24ì‹œê°„ í™œë™ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤íƒ ë“œì—… ìš”ì•½ì„ ìž‘ì„±í•´ì£¼ì„¸ìš”:

            **íŒ€**: {team_name}
            **í™œë™ ë°ì´í„°**: {asdict(team_activity)}

            **ìŠ¤íƒ ë“œì—… í˜•ì‹**:
            1. ì–´ì œ ì™„ë£Œëœ ì£¼ìš” ìž‘ì—… (Yesterday)
               - ë¨¸ì§€ëœ PRê³¼ í•´ê²°ëœ ì´ìŠˆ ê¸°ì¤€
               - í•µì‹¬ ì„±ê³¼ í•˜ì´ë¼ì´íŠ¸
            
            2. ì˜¤ëŠ˜ ì˜ˆì •ëœ ìž‘ì—… (Today)
               - ì§„í–‰ ì¤‘ì¸ PR ê²€í† 
               - ìš°ì„ ìˆœìœ„ ë†’ì€ ì´ìŠˆ ì²˜ë¦¬
            
            3. ì°¨ë‹¨ ìš”ì†Œ (Blockers)
               - ë¹Œë“œ ì‹¤íŒ¨ ì›ì¸
               - ë¦¬ë·° ì§€ì—° ì‚¬í•­
               - ì˜ì¡´ì„± ì´ìŠˆ
            
            4. íŒ€ ë©”íŠ¸ë¦­ í•˜ì´ë¼ì´íŠ¸
               - ì„±ê³¼ ì§€í‘œ ìš”ì•½
               - ê°œì„  í¬ì¸íŠ¸
            
            ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ ìœ„ì£¼ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”.
            """
            
            response = await llm.generate(
                RequestParams(
                    prompt=analysis_prompt,
                    temperature=0.3,
                    max_tokens=800
                )
            )
            
            recommendations = [
                f"{team_name} íŒ€ ìŠ¤íƒ ë“œì—… ìš”ì•½ ìƒì„± ì™„ë£Œ",
                f"ë¹Œë“œ ì„±ê³µë¥  {team_activity.build_success_rate}% - ëª©í‘œ 95% ë‹¬ì„± ê·¼ì ‘",
                f"í‰ê·  ë¦¬ë·° ì‹œê°„ {team_activity.avg_review_time}ì‹œê°„ - ì–‘í˜¸",
                f"ì¼ì¼ ì»¤ë°‹ {team_activity.commits_today}ê±´ - í™œë°œí•œ ê°œë°œ í™œë™"
            ]
            
            result = DevOpsResult(
                task_type=DevOpsTaskType.TEAM_STANDUP,
                status="completed", 
                result_data={
                    "team_name": team_name,
                    "team_activity": asdict(team_activity),
                    "standup_summary": response.strip(),
                    "period": "ì§€ë‚œ 24ì‹œê°„"
                },
                recommendations=recommendations,
                timestamp=datetime.now(timezone.utc).isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.task_history.append(result)
            await self._save_result(result)
            
            return result
    
    async def analyze_performance(self, service_name: str, timeframe: str = "24h") -> DevOpsResult:
        """
        ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë¶„ì„
        """
        start_time = datetime.now()
        
        async with self.app.run() as agent_app:
            llm = GoogleAugmentedLLM(model=self.model_name)
            
            # Mock ì„±ëŠ¥ ë©”íŠ¸ë¦­
            performance_metrics = {
                "service": service_name,
                "timeframe": timeframe,
                "response_time": {
                    "avg": "156ms",
                    "p95": "324ms", 
                    "p99": "892ms"
                },
                "throughput": "2,450 req/min",
                "error_rate": "0.18%",
                "availability": "99.94%",
                "resource_usage": {
                    "cpu": "52%",
                    "memory": "68%",
                    "disk": "34%"
                },
                "database": {
                    "query_time_avg": "23ms",
                    "connections": 45,
                    "slow_queries": 3
                }
            }
            
            analysis_prompt = f"""
            ë‹¤ìŒ ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ë¶„ì„í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

            **ì„œë¹„ìŠ¤**: {service_name}
            **ë¶„ì„ ê¸°ê°„**: {timeframe}
            **ì„±ëŠ¥ ë°ì´í„°**: {json.dumps(performance_metrics, indent=2)}

            **ë¶„ì„ ì˜ì—­**:
            1. ì‘ë‹µ ì‹œê°„ íŠ¸ë Œë“œ ë° ë³‘ëª© ì§€ì 
            2. ì²˜ë¦¬ëŸ‰ ë° í™•ìž¥ì„±
            3. ì—ëŸ¬ìœ¨ ë° ê°€ìš©ì„±
            4. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ìµœì í™”
            5. ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥

            **ì¶œë ¥ í˜•ì‹**:
            - ì „ì²´ ì„±ëŠ¥ ì ìˆ˜: [A/B/C/D]
            - ì£¼ìš” ë³‘ëª©: [ìƒìœ„ 3ê°œ]
            - ìµœì í™” ìš°ì„ ìˆœìœ„: [ì¦‰ì‹œ/ë‹¨ê¸°/ìž¥ê¸°]
            - êµ¬ì²´ì  ê°œì„ ì•ˆ: [ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©ë²•]
            - ëª¨ë‹ˆí„°ë§ ê°•í™” í¬ì¸íŠ¸: [ì¶”ê°€ ë©”íŠ¸ë¦­]

            SRE ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
            """
            
            response = await llm.generate(
                RequestParams(
                    prompt=analysis_prompt,
                    temperature=0.2,
                    max_tokens=1000
                )
            )
            
            recommendations = [
                f"{service_name} ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ ({timeframe})",
                "P99 ì‘ë‹µì‹œê°„ 892ms - ìµœì í™” í•„ìš”",
                "ê°€ìš©ì„± 99.94% - SLA ëª©í‘œ ë‹¬ì„±",
                "ìŠ¬ë¡œìš° ì¿¼ë¦¬ 3ê±´ - DB íŠœë‹ ê¶Œìž¥",
                "CPU ì‚¬ìš©ë¥  52% - ì ì • ìˆ˜ì¤€"
            ]
            
            result = DevOpsResult(
                task_type=DevOpsTaskType.PERFORMANCE_ANALYSIS,
                status="completed",
                result_data={
                    "service": service_name,
                    "timeframe": timeframe,
                    "metrics": performance_metrics,
                    "analysis": response.strip()
                },
                recommendations=recommendations,
                timestamp=datetime.now(timezone.utc).isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.task_history.append(result)
            await self._save_result(result)
            
            return result
    
    async def run_security_scan(self, target: str, scan_type: str = "full") -> DevOpsResult:
        """
        ë³´ì•ˆ ìŠ¤ìº” ì‹¤í–‰ ë° ë¶„ì„
        """
        start_time = datetime.now()
        
        async with self.app.run() as agent_app:
            llm = GoogleAugmentedLLM(model=self.model_name)
            
            # Mock ë³´ì•ˆ ìŠ¤ìº” ê²°ê³¼
            security_scan_results = {
                "target": target,
                "scan_type": scan_type,
                "vulnerabilities": {
                    "critical": 1,
                    "high": 3,
                    "medium": 7,
                    "low": 12
                },
                "findings": [
                    {
                        "severity": "critical",
                        "type": "SQL Injection",
                        "location": "/api/search",
                        "description": "User input not properly sanitized"
                    },
                    {
                        "severity": "high", 
                        "type": "XSS",
                        "location": "/user/profile",
                        "description": "Reflected XSS in user profile page"
                    },
                    {
                        "severity": "medium",
                        "type": "Insecure Headers",
                        "location": "Global",
                        "description": "Missing security headers"
                    }
                ],
                "compliance": {
                    "OWASP_Top10": "7/10 passed",
                    "CIS_Controls": "85% compliant"
                }
            }
            
            analysis_prompt = f"""
            ë‹¤ìŒ ë³´ì•ˆ ìŠ¤ìº” ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ëŒ€ì‘ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”:

            **ìŠ¤ìº” ëŒ€ìƒ**: {target}
            **ìŠ¤ìº” ìœ í˜•**: {scan_type}
            **ìŠ¤ìº” ê²°ê³¼**: {json.dumps(security_scan_results, indent=2, ensure_ascii=False)}

            **ë¶„ì„ ìš”ì²­**:
            1. ì „ì²´ ë³´ì•ˆ ìœ„í—˜ë„ í‰ê°€
            2. ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš” ì·¨ì•½ì 
            3. ìš°ì„ ìˆœìœ„ë³„ ìˆ˜ì • ê³„íš
            4. ì˜ˆë°© ì¡°ì¹˜ ë°©ì•ˆ
            5. ì»´í”Œë¼ì´ì–¸ìŠ¤ ê°œì„  ì‚¬í•­

            **ì¶œë ¥ í˜•ì‹**:
            - ìœ„í—˜ë„: [ê¸´ê¸‰/ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]
            - ì¦‰ì‹œ ì¡°ì¹˜: [Critical/High ì·¨ì•½ì ]
            - ë‹¨ê¸° ê³„íš: [1-2ì£¼ ë‚´ ìˆ˜ì •]
            - ìž¥ê¸° ê³„íš: [ë³´ì•ˆ ê°•í™” ë°©ì•ˆ]
            - ëª¨ë‹ˆí„°ë§: [ì§€ì† ê°ì‹œ í¬ì¸íŠ¸]

            ë³´ì•ˆíŒ€ê³¼ ê°œë°œíŒ€ì´ í˜‘ë ¥í•  ìˆ˜ ìžˆëŠ” ì‹¤í–‰ ê³„íšì„ ì œì‹œí•´ì£¼ì„¸ìš”.
            """
            
            response = await llm.generate(
                RequestParams(
                    prompt=analysis_prompt,
                    temperature=0.1,
                    max_tokens=1000
                )
            )
            
            recommendations = [
                f"{target} ë³´ì•ˆ ìŠ¤ìº” ì™„ë£Œ - {scan_type} ëª¨ë“œ",
                "Critical ì·¨ì•½ì  1ê±´ - ì¦‰ì‹œ íŒ¨ì¹˜ í•„ìš”",
                "High ì·¨ì•½ì  3ê±´ - ì´ë²ˆ ì£¼ ë‚´ ìˆ˜ì •",
                "OWASP Top 10 - 70% ì¤€ìˆ˜ (ê°œì„  í•„ìš”)",
                "CIS Controls - 85% ì¤€ìˆ˜ (ì–‘í˜¸)"
            ]
            
            result = DevOpsResult(
                task_type=DevOpsTaskType.SECURITY_SCAN,
                status="completed",
                result_data={
                    "target": target,
                    "scan_type": scan_type,
                    "scan_results": security_scan_results,
                    "analysis": response.strip()
                },
                recommendations=recommendations,
                timestamp=datetime.now(timezone.utc).isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
            
            self.task_history.append(result)
            await self._save_result(result)
            
            return result
    
    async def _save_result(self, result: DevOpsResult):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ìž¥"""
        filename = f"{result.task_type.name.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)
    
    def get_task_history(self) -> List[DevOpsResult]:
        """ìž‘ì—… ížˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        return self.task_history
    
    def get_team_metrics(self) -> Dict[str, TeamActivity]:
        """íŒ€ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.team_metrics
    
    def get_summary_report(self) -> Dict[str, Any]:
        """ì¢…í•© ìš”ì•½ ë¦¬í¬íŠ¸"""
        if not self.task_history:
            return {"message": "ì•„ì§ ìˆ˜í–‰ëœ ìž‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        task_counts = {}
        total_processing_time = 0
        
        for task in self.task_history:
            task_type = task.task_type.value
            task_counts[task_type] = task_counts.get(task_type, 0) + 1
            total_processing_time += task.processing_time
        
        return {
            "total_tasks": len(self.task_history),
            "task_breakdown": task_counts,
            "total_processing_time": f"{total_processing_time:.2f}ì´ˆ",
            "avg_processing_time": f"{total_processing_time / len(self.task_history):.2f}ì´ˆ",
            "model_used": self.model_name,
            "last_updated": datetime.now(timezone.utc).isoformat()
        }

# íŽ¸ì˜ í•¨ìˆ˜ë“¤
async def create_devops_assistant(output_dir: str = "devops_assistant_reports") -> DevOpsAssistantMCPAgent:
    """DevOps Assistant Agent ìƒì„±"""
    return DevOpsAssistantMCPAgent(output_dir=output_dir)

async def run_code_review(agent: DevOpsAssistantMCPAgent, owner: str, repo: str, pull_number: int) -> DevOpsResult:
    """ì½”ë“œ ë¦¬ë·° ì‹¤í–‰"""
    request = CodeReviewRequest(
        owner=owner,
        repo=repo, 
        pull_number=pull_number,
        title=f"Feature update for {repo}",
        author="developer",
        changes_summary="Added new authentication system and updated API endpoints"
    )
    return await agent.analyze_code_review(request)

async def run_deployment_check(agent: DevOpsAssistantMCPAgent, service_name: str, environment: str = "production") -> DevOpsResult:
    """ë°°í¬ ìƒíƒœ í™•ì¸"""
    return await agent.check_deployment_status(service_name, environment)

async def run_issue_analysis(agent: DevOpsAssistantMCPAgent, owner: str, repo: str) -> DevOpsResult:
    """ì´ìŠˆ ë¶„ì„ ì‹¤í–‰"""
    return await agent.analyze_issues(owner, repo)

async def run_team_standup(agent: DevOpsAssistantMCPAgent, team_name: str) -> DevOpsResult:
    """íŒ€ ìŠ¤íƒ ë“œì—… ìƒì„±"""
    return await agent.generate_team_standup(team_name)

async def run_performance_analysis(agent: DevOpsAssistantMCPAgent, service_name: str, timeframe: str = "24h") -> DevOpsResult:
    """ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
    return await agent.analyze_performance(service_name, timeframe)

async def run_security_scan(agent: DevOpsAssistantMCPAgent, target: str, scan_type: str = "full") -> DevOpsResult:
    """ë³´ì•ˆ ìŠ¤ìº” ì‹¤í–‰"""
    return await agent.run_security_scan(target, scan_type) 