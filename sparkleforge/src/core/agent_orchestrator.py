"""
Agent Orchestrator for Multi-Agent System

LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ
4ëŒ€ í•µì‹¬ ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨í•˜ì—¬ í˜‘ì—… ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
"""

import asyncio
import logging
import json
import operator
import os
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Literal, Annotated
from datetime import datetime
from dataclasses import dataclass, field

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from src.core.shared_memory import get_shared_memory, MemoryScope
from src.core.skills_manager import get_skill_manager
from src.core.skills_selector import get_skill_selector, SkillMatch
from src.core.skills_loader import Skill
from src.core.agent_result_sharing import SharedResultsManager, AgentDiscussionManager
from src.core.researcher_config import get_agent_config
from src.core.mcp_auto_discovery import FastMCPMulti
from src.core.mcp_tool_loader import MCPToolLoader
from src.core.agent_tool_selector import AgentToolSelector, AgentType

logger = logging.getLogger(__name__)

# HTTP ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§ í´ë˜ìŠ¤
class HTTPErrorFilter(logging.Filter):
    """HTML ì—ëŸ¬ ì‘ë‹µì„ í•„í„°ë§í•˜ì—¬ ê°„ë‹¨í•œ ë©”ì‹œì§€ë§Œ ì¶œë ¥"""
    def filter(self, record):
        message = record.getMessage()
        
        # HTML ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ ë° í•„í„°ë§
        if '<!DOCTYPE html>' in message or '<html' in message.lower():
            # HTMLì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ ì‹œë„
            import re
            
            # HTTP ìƒíƒœ ì½”ë“œ ì¶”ì¶œ
            status_match = re.search(r'HTTP (\d{3})', message)
            status_code = status_match.group(1) if status_match else "Unknown"
            
            # ì—ëŸ¬ ì œëª© ì¶”ì¶œ ì‹œë„
            title_match = re.search(r'<title>([^<]+)</title>', message, re.IGNORECASE)
            error_title = title_match.group(1).strip() if title_match else None
            
            # ê°„ë‹¨í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            if error_title:
                record.msg = f"HTTP {status_code}: {error_title}"
            else:
                # ìƒíƒœ ì½”ë“œì— ë”°ë¥¸ ê¸°ë³¸ ë©”ì‹œì§€
                if status_code == "502":
                    record.msg = f"HTTP {status_code}: Bad Gateway - Server temporarily unavailable"
                elif status_code == "504":
                    record.msg = f"HTTP {status_code}: Gateway Timeout - Server response timeout"
                elif status_code == "503":
                    record.msg = f"HTTP {status_code}: Service Unavailable - Server temporarily unavailable"
                elif status_code == "401":
                    record.msg = f"HTTP {status_code}: Unauthorized - Authentication failed"
                elif status_code == "404":
                    record.msg = f"HTTP {status_code}: Not Found"
                elif status_code == "500":
                    record.msg = f"HTTP {status_code}: Internal Server Error"
                else:
                    record.msg = f"HTTP {status_code}: Server Error"
            
            record.args = ()  # args ì´ˆê¸°í™”
        
        return True

# Loggerê°€ handlerê°€ ì—†ìœ¼ë©´ root loggerì˜ handler ì‚¬ìš©
if not logger.handlers:
    logger.setLevel(logging.INFO)
    # Root loggerì˜ handler ì‚¬ìš© (main.pyì—ì„œ ì„¤ì •ëœ handler)
    parent_logger = logging.getLogger()
    if parent_logger.handlers:
        logger.handlers = parent_logger.handlers
        logger.propagate = True
    else:
        # Fallback: ê¸°ë³¸ handler ì„¤ì •
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        handler.addFilter(HTTPErrorFilter())  # HTTP ì—ëŸ¬ í•„í„° ì¶”ê°€
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
else:
    # ê¸°ì¡´ handlerì— í•„í„° ì¶”ê°€
    for handler in logger.handlers:
        if not any(isinstance(f, HTTPErrorFilter) for f in handler.filters):
            handler.addFilter(HTTPErrorFilter())

# FastMCP Runner ë¡œê±°ì—ë„ í•„í„° ì¶”ê°€ (ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê¹… í•„í„°ë§)
# Runner ë¡œê±°ëŠ” ë‚˜ì¤‘ì— ìƒì„±ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, propagateë¥¼ í™œì„±í™”í•˜ê³  root loggerì˜ í•„í„° ì‚¬ìš©
def setup_runner_logger_filter():
    """Runner ë¡œê±°ì— HTML í•„í„° ì¶”ê°€ (ì§€ì—° ì´ˆê¸°í™”)"""
    runner_logger = logging.getLogger("Runner")
    if runner_logger:
        runner_logger.propagate = True  # Root loggerë¡œ ì „íŒŒí•˜ì—¬ í•„í„° ì ìš©
        # ê¸°ì¡´ handlerì— í•„í„° ì¶”ê°€ (í˜¹ì‹œ ì§ì ‘ handlerê°€ ìˆëŠ” ê²½ìš°)
        for handler in runner_logger.handlers:
            if not any(isinstance(f, HTTPErrorFilter) for f in handler.filters):
                handler.addFilter(HTTPErrorFilter())

# ì´ˆê¸° ì„¤ì •
setup_runner_logger_filter()


###################
# State Definitions
###################

def override_reducer(current_value, new_value):
    """Reducer function that allows overriding values in state."""
    if isinstance(new_value, dict) and new_value.get("type") == "override":
        return new_value.get("value", new_value)
    else:
        return operator.add(current_value, new_value)


class AgentState(TypedDict):
    """Main agent state containing messages and research data."""
    
    messages: Annotated[list, add_messages]
    user_query: str
    research_plan: Optional[str]
    research_tasks: Annotated[list, override_reducer]  # List of research tasks for parallel execution
    research_results: Annotated[list, override_reducer]  # Changed: supports both dict and str
    verified_results: Annotated[list, override_reducer]  # Changed: supports both dict and str
    final_report: Optional[str]
    current_agent: Optional[str]
    iteration: int
    session_id: Optional[str]
    research_failed: bool
    verification_failed: bool
    report_failed: bool
    error: Optional[str]


###################
# Agent Definitions
###################

@dataclass
class AgentContext:
    """Agent execution context."""
    agent_id: str
    session_id: str
    shared_memory: Any
    config: Any = None
    shared_results_manager: Optional[SharedResultsManager] = None
    discussion_manager: Optional[AgentDiscussionManager] = None


class PlannerAgent:
    """Planner agent - creates research plans (YAML-based configuration)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "planner"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # YAML ì„¤ì • ë¡œë“œ
        from src.core.skills.agent_loader import load_agent_config
        self.config = load_agent_config("planner")
        self.instruction = self.config.instructions
    
    async def execute(self, state: AgentState) -> AgentState:
        """Execute planning task with Skills-based instruction and detailed logging."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting research planning")
        logger.info(f"Query: {state['user_query']}")
        logger.info(f"Session: {state['session_id']}")
        logger.info(f"=" * 80)
        
        # Read from shared memory - ONLY search within current session to prevent cross-task contamination
        memory = self.context.shared_memory
        current_session_id = state['session_id']
        
        # Search only within current session to prevent mixing previous task memories
        previous_plans = memory.search(
            state['user_query'], 
            limit=3,
            scope=MemoryScope.SESSION,
            session_id=current_session_id  # Critical: filter by current session only
        )
        
        logger.info(f"[{self.name}] Previous plans found in current session ({current_session_id}): {len(previous_plans) if previous_plans else 0}")
        
        # If no plans found in current session, explicitly set to empty to avoid confusion
        if not previous_plans:
            previous_plans = []
            logger.info(f"[{self.name}] No previous plans in current session - starting fresh task")
        
        # Skills-based instruction ì‚¬ìš©
        instruction = self.instruction if self.skill else "You are a research planning agent."
        
        logger.info(f"[{self.name}] Using skill: {self.skill is not None}")
        
        # LLM í˜¸ì¶œì€ llm_managerë¥¼ í†µí•´ Gemini ì§ê²° ì‚¬ìš©
        from src.core.llm_manager import execute_llm_task, TaskType
        
        # Use YAML-based prompt
        from src.core.skills.agent_loader import get_prompt
        
        # Format previous_plans for prompt - only include if from current session
        if previous_plans:
            # Filter to ensure only current session plans are included
            current_session_plans = [
                p for p in previous_plans 
                if p.get("session_id") == current_session_id
            ]
            if current_session_plans:
                previous_plans_text = "\n".join([
                    f"- {p.get('key', 'plan')}: {str(p.get('value', ''))[:200]}"
                    for p in current_session_plans
                ])
            else:
                previous_plans_text = "No previous research found in current session. This is a NEW task - focus only on the current query."
        else:
            previous_plans_text = "No previous research found in current session. This is a NEW task - focus only on the current query."
        
        prompt = get_prompt("planner", "planning",
                           instruction=self.instruction,
                           user_query=state['user_query'],
                           previous_plans=previous_plans_text)

        logger.info(f"[{self.name}] Calling LLM for planning...")
        # Gemini ì‹¤í–‰
        model_result = await execute_llm_task(
            prompt=prompt,
            task_type=TaskType.PLANNING,
            model_name=None,
            system_message=None
        )
        plan = model_result.content or 'No plan generated'
        
        logger.info(f"[{self.name}] âœ… Plan generated: {len(plan)} characters")
        logger.info(f"[{self.name}] Plan preview: {plan[:200]}...")
        
        # Council í™œì„±í™” í™•ì¸ ë° ì ìš©
        use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
        if use_council is None:
            # ìë™ í™œì„±í™” íŒë‹¨
            from src.core.council_activator import get_council_activator
            activator = get_council_activator()
            activation_decision = activator.should_activate(
                process_type='planning',
                query=state['user_query'],
                context={'domains': [], 'steps': []}  # ì»¨í…ìŠ¤íŠ¸ëŠ” í–¥í›„ í™•ì¥ ê°€ëŠ¥
            )
            use_council = activation_decision.should_activate
            if use_council:
                logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
        
        # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if use_council:
            try:
                from src.core.llm_council import run_full_council
                logger.info(f"[{self.name}] ğŸ›ï¸ Running Council review for research plan...")
                
                # Councilì— ê³„íš ê²€í†  ìš”ì²­
                council_query = f"""Review and improve the following research plan. Provide feedback on completeness, feasibility, and quality.

Research Query: {state['user_query']}

Research Plan:
{plan}

Provide an improved version of the plan that addresses any gaps or issues you identify."""
                
                stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                    council_query
                )
                
                # Council ê²°ê³¼ë¥¼ ê³„íšì— ë°˜ì˜
                council_improved_plan = stage3_result.get('response', plan)
                plan = council_improved_plan
                
                logger.info(f"[{self.name}] âœ… Council review completed. Plan improved with consensus.")
                logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                
                # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                state['council_metadata'] = {
                    'planning': {
                        'stage1_results': stage1_results,
                        'stage2_results': stage2_results,
                        'stage3_result': stage3_result,
                        'metadata': metadata
                    }
                }
            except Exception as e:
                logger.warning(f"[{self.name}] Council review failed: {e}. Using original plan.")
                # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê³„íš ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        
        state['research_plan'] = plan
        
        # ì‘ì—… ë¶„í• : ì—°êµ¬ ê³„íšì„ ì—¬ëŸ¬ ë…ë¦½ì ì¸ ì‘ì—…ìœ¼ë¡œ ë¶„í• 
        logger.info(f"[{self.name}] Splitting research plan into parallel tasks...")
        
        # Use YAML-based prompt template for task decomposition
        from src.core.skills.agent_loader import get_prompt
        task_split_prompt = get_prompt(
            "planner",
            "task_decomposition",
            plan=plan,
            query=state['user_query']
        )

        try:
            task_split_result = await execute_llm_task(
                prompt=task_split_prompt,
                task_type=TaskType.PLANNING,
                model_name=None,
                system_message="You are a task decomposition agent. Split research plans into independent parallel tasks."
            )
            
            task_split_text = task_split_result.content or ""
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            import re
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'\{[\s\S]*\}', task_split_text)
            if json_match:
                task_split_json = json.loads(json_match.group())
                tasks = task_split_json.get('tasks', [])
            else:
                # JSONì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì‘ì—… ì¶”ì¶œ ì‹œë„
                tasks = []
                lines = task_split_text.split('\n')
                current_task = None
                for line in lines:
                    line = line.strip()
                    if 'task_id' in line.lower() or 'task' in line.lower() and ':' in line:
                        if current_task:
                            tasks.append(current_task)
                        task_id_match = re.search(r'task[_\s]*(\d+)', line, re.IGNORECASE)
                        task_id = f"task_{task_id_match.group(1) if task_id_match else len(tasks) + 1}"
                        current_task = {
                            "task_id": task_id,
                            "description": "",
                            "search_queries": [],
                            "priority": len(tasks) + 1,
                            "estimated_time": "medium",
                            "dependencies": []
                        }
                    elif current_task:
                        if 'description' in line.lower() or 'ì„¤ëª…' in line:
                            desc_match = re.search(r':\s*(.+)', line)
                            if desc_match:
                                current_task["description"] = desc_match.group(1).strip()
                        elif 'query' in line.lower() or 'ì¿¼ë¦¬' in line:
                            query_match = re.search(r':\s*(.+)', line)
                            if query_match:
                                current_task["search_queries"].append(query_match.group(1).strip())
                
                if current_task:
                    tasks.append(current_task)
            
            # ì‘ì—…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ì—… ìƒì„±
            if not tasks:
                logger.warning(f"[{self.name}] Failed to parse tasks, creating default task")
                tasks = [{
                    "task_id": "task_1",
                    "description": state['user_query'],
                    "search_queries": [state['user_query']],
                    "priority": 1,
                    "estimated_time": "medium",
                    "dependencies": []
                }]
            
            # ê° ì‘ì—…ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ ë° ê²€ìƒ‰ ì¿¼ë¦¬ ê²€ì¦
            user_query_lower = state['user_query'].lower()
            # ì˜ëª»ëœ ê²€ìƒ‰ ì¿¼ë¦¬ í‚¤ì›Œë“œ (ë©”íƒ€ ì •ë³´ ê´€ë ¨)
            invalid_keywords = [
                'ì‘ì—… ë¶„í• ', 'íƒœìŠ¤í¬ ë¶„í• ', 'ë³‘ë ¬í™”', 'ë³‘ë ¬ ì‹¤í–‰', 'task decomposition',
                'task split', 'parallel', 'parallelization', 'ì—°êµ¬ ë°©ë²•ë¡ ', 'ì—°êµ¬ ì „ëµ',
                'ì—°êµ¬ ê³„íš', 'research methodology', 'research strategy', 'research plan',
                'í•˜ìœ„ ì—°êµ¬ ì£¼ì œ ë¶„í•´', 'ë…ë¦½ì  ì—°êµ¬ íƒœìŠ¤í¬', 'ì—°êµ¬ ì‘ì—… ë³‘ë ¬í™”'
            ]
            
            for i, task in enumerate(tasks):
                if 'task_id' not in task:
                    task['task_id'] = f"task_{i + 1}"
                if 'description' not in task:
                    task['description'] = state['user_query']
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ ê²€ì¦ ë° í•„í„°ë§
                if 'search_queries' in task and task['search_queries']:
                    # ì˜ëª»ëœ ê²€ìƒ‰ ì¿¼ë¦¬ í•„í„°ë§
                    valid_queries = []
                    for query in task['search_queries']:
                        query_str = str(query).strip()
                        query_lower = query_str.lower()
                        
                        # {query} í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì™„ì „ ì œì™¸
                        if "{query}" in query_str or "{query}" in query_lower:
                            logger.warning(f"[{self.name}] Task {task.get('task_id')}: Filtered out query with placeholder: '{query_str[:50]}...'")
                            continue
                        
                        # ë©”íƒ€ ì •ë³´ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì œì™¸
                        is_invalid = any(keyword in query_lower for keyword in invalid_keywords)
                        # ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì¿¼ë¦¬ ì œì™¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ ê²½ìš°)
                        is_too_generic = len(query_str) < 10
                        
                        if not is_invalid and not is_too_generic:
                            valid_queries.append(query_str)
                        else:
                            logger.warning(f"[{self.name}] Task {task.get('task_id')}: Filtered out invalid query: '{query_str[:50]}...' (invalid={is_invalid}, generic={is_too_generic})")
                    
                    # ìœ íš¨í•œ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ ì‚¬ìš©
                    if not valid_queries:
                        logger.warning(f"[{self.name}] Task {task.get('task_id')} has no valid search queries, using user query: '{state['user_query']}'")
                        valid_queries = [state['user_query']]
                    
                    task['search_queries'] = valid_queries
                    logger.info(f"[{self.name}] Task {task.get('task_id')}: Final search queries: {valid_queries}")
                else:
                    # search_queriesê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ ì‚¬ìš©
                    task['search_queries'] = [state['user_query']]
                
                if 'priority' not in task:
                    task['priority'] = i + 1
                if 'estimated_time' not in task:
                    task['estimated_time'] = "medium"
                if 'dependencies' not in task:
                    task['dependencies'] = []
            
            state['research_tasks'] = tasks
            logger.info(f"[{self.name}] âœ… Split research plan into {len(tasks)} parallel tasks")
            for task in tasks:
                queries = task.get('search_queries', [])
                queries_preview = [q[:40] + '...' if len(q) > 40 else q for q in queries[:3]]
                logger.info(f"[{self.name}]   - {task.get('task_id')}: {task.get('description', '')[:50]}... ({len(queries)} queries: {queries_preview})")
                
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Failed to split tasks: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ì—… ìƒì„±
            state['research_tasks'] = [{
                "task_id": "task_1",
                "description": state['user_query'],
                "search_queries": [state['user_query']],
                "priority": 1,
                "estimated_time": "medium",
                "dependencies": []
            }]
            logger.warning(f"[{self.name}] Using default single task")
        
        state['current_agent'] = self.name
        
        # Write to shared memory
        memory.write(
            key=f"plan_{state['session_id']}",
            value=plan,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        memory.write(
            key=f"tasks_{state['session_id']}",
            value=state['research_tasks'],
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Plan and tasks saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class ExecutorAgent:
    """Executor agent - executes research tasks using tools (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "executor"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("research_executor")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a research execution agent."
    
    async def execute(self, state: AgentState, assigned_task: Optional[Dict[str, Any]] = None) -> AgentState:
        """Execute research tasks with detailed logging."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting research execution")
        logger.info(f"Agent ID: {self.context.agent_id}")
        logger.info(f"Query: {state['user_query']}")
        logger.info(f"Session: {state['session_id']}")
        logger.info(f"=" * 80)
        
        # ì‘ì—… í• ë‹¹: assigned_taskê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ stateì—ì„œ ì°¾ê¸°
        if assigned_task is None:
            # state['research_tasks']ì—ì„œ ì´ ì—ì´ì „íŠ¸ì—ê²Œ í• ë‹¹ëœ ì‘ì—… ì°¾ê¸°
            tasks = state.get('research_tasks', [])
            if tasks:
                # agent_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—… í• ë‹¹ (ë¼ìš´ë“œë¡œë¹ˆ)
                agent_id = self.context.agent_id
                if agent_id.startswith("executor_"):
                    try:
                        agent_index = int(agent_id.split("_")[1])
                        if agent_index < len(tasks):
                            assigned_task = tasks[agent_index]
                            logger.info(f"[{self.name}] Assigned task {assigned_task.get('task_id', 'unknown')} to {agent_id}")
                        else:
                            # ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì²« ë²ˆì§¸ ì‘ì—… í• ë‹¹
                            assigned_task = tasks[0]
                            logger.info(f"[{self.name}] Agent index out of range, using first task")
                    except (ValueError, IndexError):
                        assigned_task = tasks[0] if tasks else None
                        logger.info(f"[{self.name}] Using first task (fallback)")
                else:
                    # agent_idê°€ executor_ í˜•ì‹ì´ ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ ì‘ì—… ì‚¬ìš©
                    assigned_task = tasks[0] if tasks else None
            else:
                # ì‘ì—…ì´ ì—†ìœ¼ë©´ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
                memory = self.context.shared_memory
                tasks = memory.read(
                    key=f"tasks_{state['session_id']}",
                    scope=MemoryScope.SESSION,
                    session_id=state['session_id']
                ) or []
                if tasks:
                    assigned_task = tasks[0] if tasks else None
        
        # Read plan from shared memory
        memory = self.context.shared_memory
        plan = memory.read(
            key=f"plan_{state['session_id']}",
            scope=MemoryScope.SESSION,
            session_id=state['session_id']
        )
        
        logger.info(f"[{self.name}] Research plan loaded: {plan is not None}")
        if plan:
            logger.info(f"[{self.name}] Plan preview: {plan[:200]}...")
        
        # ì‹¤ì œ ì—°êµ¬ ì‹¤í–‰ - MCP Hubë¥¼ í†µí•œ ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰
        query = state['user_query']
        results = []
        
        try:
            # MCP Hub ì´ˆê¸°í™” í™•ì¸
            from src.core.mcp_integration import get_mcp_hub, execute_tool, ToolCategory
            
            hub = get_mcp_hub()
            logger.info(f"[{self.name}] MCP Hub status: {len(hub.mcp_sessions) if hub.mcp_sessions else 0} servers connected")
            
            if not hub.mcp_sessions:
                logger.info(f"[{self.name}] Initializing MCP Hub...")
                await hub.initialize_mcp()
                logger.info(f"[{self.name}] MCP Hub initialized: {len(hub.mcp_sessions)} servers")
            
            # ì‘ì—… í• ë‹¹ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì‘ì—…ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš©
            search_queries = []
            if assigned_task:
                raw_queries = assigned_task.get('search_queries', [])
                
                # ì˜ëª»ëœ ê²€ìƒ‰ ì¿¼ë¦¬ í•„í„°ë§ (ë©”íƒ€ ì •ë³´ ê´€ë ¨)
                invalid_keywords = [
                    'ì‘ì—… ë¶„í• ', 'íƒœìŠ¤í¬ ë¶„í• ', 'ë³‘ë ¬í™”', 'ë³‘ë ¬ ì‹¤í–‰', 'task decomposition',
                    'task split', 'parallel', 'parallelization', 'ì—°êµ¬ ë°©ë²•ë¡ ', 'ì—°êµ¬ ì „ëµ',
                    'ì—°êµ¬ ê³„íš', 'research methodology', 'research strategy', 'research plan',
                    'í•˜ìœ„ ì—°êµ¬ ì£¼ì œ ë¶„í•´', 'ë…ë¦½ì  ì—°êµ¬ íƒœìŠ¤í¬', 'ì—°êµ¬ ì‘ì—… ë³‘ë ¬í™”'
                ]
                
                for q in raw_queries:
                    q_str = str(q).strip()
                    q_lower = q_str.lower()
                    
                    # {query} í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì™„ì „ ì œì™¸
                    if "{query}" in q_str or "{query}" in q_lower:
                        logger.warning(f"[{self.name}] Filtered out query with placeholder: '{q_str[:50]}...'")
                        continue
                    
                    is_invalid = any(keyword in q_lower for keyword in invalid_keywords)
                    is_too_generic = len(q_str) < 10
                    
                    if not is_invalid and not is_too_generic:
                        search_queries.append(q_str)
                    else:
                        logger.warning(f"[{self.name}] Filtered out invalid query: '{q_str[:50]}...' (invalid={is_invalid}, generic={is_too_generic})")
                
                # ìœ íš¨í•œ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ ì‚¬ìš©
                if not search_queries:
                    logger.warning(f"[{self.name}] No valid queries in assigned task, using user query")
                    search_queries = [query]
                else:
                    logger.info(f"[{self.name}] Using {len(search_queries)} valid queries from task {assigned_task.get('task_id', 'unknown')}")
            
            # ì‘ì—… í• ë‹¹ì´ ì—†ê±°ë‚˜ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            if not search_queries:
                search_queries = [query]  # ê¸°ë³¸ ì¿¼ë¦¬
                if plan:
                    # LLMìœ¼ë¡œ ì—°êµ¬ ê³„íšì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ
                    from src.core.llm_manager import execute_llm_task, TaskType

                    # Use YAML-based prompt for query generation
                    from src.core.skills.agent_loader import get_prompt
                    query_generation_prompt = get_prompt("planner", "query_generation",
                                                        plan=plan,
                                                        query=query)

                    try:
                        system_message = self.config.prompts["query_generation"]["system_message"]
                        query_result = await execute_llm_task(
                            prompt=query_generation_prompt,
                            task_type=TaskType.PLANNING,
                            model_name=None,
                            system_message=system_message
                        )

                        generated_queries = query_result.content or ""
                        # ê° ì¤„ì„ ì¿¼ë¦¬ë¡œ íŒŒì‹±
                        for line in generated_queries.split('\n'):
                            line = line.strip()
                            if line and not line.startswith('#') and len(line) > 5:
                                search_queries.append(line)

                        # ì¤‘ë³µ ì œê±°
                        search_queries = list(dict.fromkeys(search_queries))[:5]  # ìµœëŒ€ 5ê°œ
                        logger.info(f"[{self.name}] Generated {len(search_queries)} search queries from plan")
                    except Exception as e:
                        logger.warning(f"[{self.name}] Failed to generate search queries from plan: {e}, using original query only")
            
            # ìµœì†Œ 3-5ê°œì˜ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ë³´ì¥
            MIN_QUERIES = 3
            MAX_QUERIES = 8
            if len(search_queries) < MIN_QUERIES:
                logger.info(f"[{self.name}] Only {len(search_queries)} queries available, generating additional queries to ensure diversity...")
                # ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                base_query = query
                additional_queries = []
                
                # ë‹¤ì–‘í•œ ê´€ì ì˜ ì¿¼ë¦¬ ìƒì„±
                query_variations = [
                    f"{base_query} ë¶„ì„",
                    f"{base_query} ì „ë§",
                    f"{base_query} ë™í–¥",
                    f"{base_query} í˜„í™©",
                    f"{base_query} ì „ë¬¸ê°€ ì˜ê²¬"
                ]
                
                for variation in query_variations:
                    if variation not in search_queries and len(search_queries) < MAX_QUERIES:
                        search_queries.append(variation)
                        additional_queries.append(variation)
                
                if additional_queries:
                    logger.info(f"[{self.name}] Added {len(additional_queries)} additional query variations: {additional_queries}")
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            logger.info(f"[{self.name}] Executing {len(search_queries)} searches in parallel...")
            logger.info(f"[{self.name}] Search queries: {search_queries}")
            
            async def execute_single_search(search_query: str, query_index: int) -> Dict[str, Any]:
                """ë‹¨ì¼ ê²€ìƒ‰ ì‹¤í–‰."""
                try:
                    # ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°’ ë¡œê·¸ ì¶œë ¥
                    logger.info(f"[{self.name}] Search {query_index + 1}/{len(search_queries)}: '{search_query}'")
                    # ê° ê²€ìƒ‰ë§ˆë‹¤ ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘ (ìµœì†Œ 5ê°œ ì¶œì²˜ ë³´ì¥ì„ ìœ„í•´)
                    search_result = await execute_tool(
                        "g-search",
                        {"query": search_query, "max_results": 15}  # 10 -> 15ë¡œ ì¦ê°€
                    )
                    return {
                        "query": search_query,
                        "index": query_index,
                        "result": search_result,
                        "success": search_result.get('success', False)
                    }
                except Exception as e:
                    logger.error(f"[{self.name}] Search {query_index + 1} failed: {e}")
                    return {
                        "query": search_query,
                        "index": query_index,
                        "result": {"success": False, "error": str(e)},
                        "success": False
                    }
            
            # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            search_tasks = [execute_single_search(q, i) for i, q in enumerate(search_queries)]
            search_results_list = await asyncio.gather(*search_tasks)
            
            logger.info(f"[{self.name}] âœ… Completed {len(search_results_list)} parallel searches")
            
            # ëª¨ë“  ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
            successful_results = [sr for sr in search_results_list if sr.get('success') and sr.get('result', {}).get('data')]
            
            if not successful_results:
                # ì‹¤íŒ¨í•œ ê²€ìƒ‰ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                failed_searches = [sr for sr in search_results_list if not sr.get('success')]
                error_details = []
                for fs in failed_searches:
                    query = fs.get('query', 'unknown')
                    result = fs.get('result', {})
                    error = result.get('error', 'Unknown error')
                    error_details.append(f"  - Query: '{query[:60]}...' â†’ Error: {str(error)[:100]}")
                
                logger.error(f"[{self.name}] âŒ ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨ ({len(failed_searches)}/{len(search_results_list)} ì‹¤íŒ¨)")
                logger.error(f"[{self.name}] ğŸ“‹ ì‹¤íŒ¨ ìƒì„¸:")
                for detail in error_details:
                    logger.error(f"[{self.name}] {detail}")
                
                # MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
                try:
                    from src.core.mcp_integration import get_mcp_hub
                    mcp_hub = get_mcp_hub()
                    connected_servers = list(mcp_hub.mcp_sessions.keys()) if mcp_hub.mcp_sessions else []
                    logger.error(f"[{self.name}] ğŸ”Œ í˜„ì¬ ì—°ê²°ëœ MCP ì„œë²„: {connected_servers if connected_servers else 'ì—†ìŒ'}")
                    logger.error(f"[{self.name}] ğŸ“ Fallback (duckduckgo_search ë¼ì´ë¸ŒëŸ¬ë¦¬)ê°€ ì‘ë™í–ˆëŠ”ì§€ í™•ì¸ í•„ìš”")
                except Exception as e:
                    logger.debug(f"[{self.name}] MCP Hub ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                
                error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({len(failed_searches)}/{len(search_results_list)} ì‹¤íŒ¨)"
                raise RuntimeError(error_msg)
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•© (í•˜ë“œì½”ë”© ì œê±°, ë™ì  í†µí•©)
            all_search_data = []
            for sr in successful_results:
                result_data = sr['result'].get('data', {})
                if isinstance(result_data, dict):
                    items = result_data.get('results', result_data.get('items', []))
                    if isinstance(items, list):
                        all_search_data.extend(items)
                elif isinstance(result_data, list):
                    all_search_data.extend(result_data)
            
            # í†µí•©ëœ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            search_result = {
                'success': True,
                'data': {
                    'results': all_search_data,
                    'total_results': len(all_search_data),
                    'source': 'parallel_search'
                }
            }
            
            logger.info(f"[{self.name}] âœ… Integrated {len(all_search_data)} results from {len(successful_results)} successful searches")
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ 
            if self.context.shared_results_manager:
                shared_count = 0
                for sr in search_results_list:
                    if sr.get('success'):
                        task_id = f"search_{sr['index']}"
                        result_id = await self.context.shared_results_manager.share_result(
                            task_id=task_id,
                            agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                            result=sr['result'],
                            metadata={"query": sr['query'], "index": sr['index']},
                            confidence=1.0 if sr.get('success') else 0.0
                        )
                        shared_count += 1
                        logger.info(f"[{self.name}] ğŸ”— Shared search result for query: '{sr['query'][:50]}...' (result_id: {result_id[:8]}..., agent_id: {self.context.agent_id})")

                # ê³µìœ  í†µê³„ ë¡œê¹…
                total_results = len([sr for sr in search_results_list if sr.get('success')])
                logger.info(f"[{self.name}] ğŸ“¤ Shared {shared_count}/{total_results} successful search results with other agents")
                logger.info(f"[{self.name}] ğŸ¤ Agent communication: {shared_count} results shared via SharedResultsManager")
            
            logger.info(f"[{self.name}] Search completed: success={search_result.get('success')}, total_results={search_result.get('data', {}).get('total_results', 0)}")
            logger.info(f"[{self.name}] Search result type: {type(search_result)}, keys: {list(search_result.keys()) if isinstance(search_result, dict) else 'N/A'}")
            
            if search_result.get('success') and search_result.get('data'):
                data = search_result.get('data', {})
                logger.info(f"[{self.name}] Data type: {type(data)}, keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
                
                # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± - ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                search_results = []
                if isinstance(data, dict):
                    # í‘œì¤€ í˜•ì‹: {"query": "...", "results": [...], "total_results": N, "source": "..."}
                    search_results = data.get('results', [])
                    logger.info(f"[{self.name}] Found 'results' key: {len(search_results)} items")
                    
                    if not search_results:
                        # ë‹¤ë¥¸ í‚¤ ì‹œë„
                        search_results = data.get('items', data.get('data', []))
                        logger.info(f"[{self.name}] Tried 'items' or 'data' keys: {len(search_results)} items")
                    
                    # data ìì²´ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì¤‘ì²©ëœ ê²½ìš°)
                    if not search_results and isinstance(data, dict):
                        # dataì˜ ê°’ ì¤‘ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                        for key, value in data.items():
                            if isinstance(value, list) and len(value) > 0:
                                # ì²« ë²ˆì§¸ í•­ëª©ì´ dictì¸ì§€ í™•ì¸
                                if value and isinstance(value[0], dict):
                                    search_results = value
                                    logger.info(f"[{self.name}] Found list in key '{key}': {len(search_results)} items")
                                    break
                elif isinstance(data, list):
                    search_results = data
                    logger.info(f"[{self.name}] Data is directly a list: {len(search_results)} items")
                
                logger.info(f"[{self.name}] âœ… Parsed {len(search_results)} search results")
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
                if search_results and len(search_results) > 0:
                    first_result = search_results[0]
                    logger.info(f"[{self.name}] First result type: {type(first_result)}, sample: {str(first_result)[:200]}")
                
                if search_results and len(search_results) > 0:
                    # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    unique_results = []
                    seen_urls = set()
                    filtered_count = 0
                    filtered_reasons = []
                    
                    # ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°’ ë¡œê·¸ ì¶œë ¥ (query ë³€ìˆ˜ëŠ” ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬)
                    actual_query = query if isinstance(query, str) else str(query)
                    logger.info(f"[{self.name}] Processing {len(search_results)} results for query: '{actual_query}'")
                    
                    for i, result in enumerate(search_results, 1):
                        # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                        if isinstance(result, dict):
                            title = result.get('title', result.get('name', result.get('Title', 'No title')))
                            snippet = result.get('snippet', result.get('content', result.get('summary', result.get('description', result.get('abstract', '')))))
                            url = result.get('url', result.get('link', result.get('href', result.get('URL', ''))))
                            
                            # snippetì— ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì—¬ëŸ¬ ê²°ê³¼ê°€ ë“¤ì–´ìˆëŠ” ê²½ìš° íŒŒì‹±
                            if snippet and ("Found" in snippet or "search results" in snippet.lower() or "\n1." in snippet):
                                logger.info(f"[{self.name}] Detected markdown format in snippet, parsing...")
                                import re
                                parsed_results = []
                                lines = snippet.split('\n')
                                current_result = None
                                
                                for line in lines:
                                    original_line = line
                                    line = line.strip()
                                    if not line:
                                        continue
                                    
                                    # íŒ¨í„´ 1: ë§ˆí¬ë‹¤ìš´ ë§í¬ "1. [Title](URL)"
                                    link_match = re.match(r'^\d+\.\s*\[([^\]]+)\]\(([^\)]+)\)', line)
                                    # íŒ¨í„´ 2: ë²ˆí˜¸ì™€ ì œëª©ë§Œ "1. [Title]" ë˜ëŠ” "1. Title"
                                    title_match = re.match(r'^\d+\.\s*(?:\[([^\]]+)\]|(.+?))(?:\s*$|:)', line)
                                    # íŒ¨í„´ 3: URL ì¤„ "   URL: https://..."
                                    url_match = re.search(r'URL:\s*(https?://[^\s]+)', line, re.IGNORECASE)
                                    # íŒ¨í„´ 4: Summary ì¤„ "   Summary: ..."
                                    summary_match = re.search(r'Summary:\s*(.+)$', line, re.IGNORECASE)
                                    
                                    if link_match:
                                        # ì´ì „ ê²°ê³¼ ì €ì¥
                                        if current_result and current_result.get('title'):
                                            parsed_results.append(current_result)
                                        
                                        title_parsed = link_match.group(1)
                                        url_parsed = link_match.group(2)
                                        current_result = {
                                            "title": title_parsed,
                                            "url": url_parsed,
                                            "snippet": ""
                                        }
                                    elif title_match and not current_result:
                                        # ë²ˆí˜¸ì™€ ì œëª©ë§Œ ìˆëŠ” ê²½ìš° (ë‹¤ìŒ ì¤„ì— URLì´ ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
                                        title_parsed = title_match.group(1) or title_match.group(2)
                                        if title_parsed:
                                            current_result = {
                                                "title": title_parsed.strip(),
                                                "url": "",
                                                "snippet": ""
                                            }
                                    elif url_match:
                                        # URLì´ ë³„ë„ ì¤„ì— ìˆëŠ” ê²½ìš°
                                        if current_result:
                                            current_result["url"] = url_match.group(1)
                                        else:
                                            # URLë§Œ ìˆê³  ì œëª©ì´ ì—†ëŠ” ê²½ìš° (ì´ì „ ê²°ê³¼ì— ì¶”ê°€)
                                            if parsed_results:
                                                parsed_results[-1]["url"] = url_match.group(1)
                                    elif summary_match and current_result:
                                        # Summary ì¤„
                                        current_result["snippet"] = summary_match.group(1).strip()
                                    elif current_result and line and not any([
                                        line.startswith('URL:'), 
                                        line.startswith('Summary:'),
                                        line.startswith('Found'),
                                        'search results' in line.lower()
                                    ]):
                                        # ì„¤ëª… í…ìŠ¤íŠ¸ (ë“¤ì—¬ì“°ê¸°ëœ ê²½ìš°)
                                        if original_line.startswith('   ') or original_line.startswith('\t'):
                                            if current_result["snippet"]:
                                                current_result["snippet"] += " " + line
                                            else:
                                                current_result["snippet"] = line
                                
                                # ë§ˆì§€ë§‰ ê²°ê³¼ ì¶”ê°€
                                if current_result and current_result.get('title'):
                                    parsed_results.append(current_result)
                                
                                if parsed_results:
                                    logger.info(f"[{self.name}] Parsed {len(parsed_results)} results from markdown snippet")
                                    # íŒŒì‹±ëœ ê²°ê³¼ë“¤ì„ unique_resultsì— ì¶”ê°€
                                    for parsed_result in parsed_results:
                                        parsed_url = parsed_result.get('url', '')
                                        parsed_title = parsed_result.get('title', '')
                                        parsed_snippet = parsed_result.get('snippet', '')
                                        
                                        if parsed_url and parsed_url in seen_urls:
                                            logger.debug(f"[{self.name}] Duplicate URL skipped in parsed results: {parsed_url[:50]}")
                                            continue
                                        if parsed_url:
                                            seen_urls.add(parsed_url)
                                        
                                        # ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê²°ê³¼ë„ í•„í„°ë§ ì ìš©
                                        invalid_indicators = [
                                            "no results were found", "bot detection",
                                            "no results", "not found", "try again",
                                            "unable to", "error occurred", "no matches"
                                        ]
                                        parsed_snippet_lower = parsed_snippet.lower() if parsed_snippet else ""
                                        matched_indicators = [ind for ind in invalid_indicators if ind in parsed_snippet_lower]
                                        
                                        if matched_indicators:
                                            filtered_count += 1
                                            reason = f"Matched indicators: {', '.join(matched_indicators)}"
                                            filtered_reasons.append({
                                                "result_index": f"{i}(parsed)",
                                                "title": parsed_title[:80],
                                                "reason": reason,
                                                "snippet_preview": parsed_snippet[:200] if parsed_snippet else "(empty)"
                                            })
                                            logger.warning(f"[{self.name}] âš ï¸ Filtering invalid parsed result: '{parsed_title[:60]}...' - Reason: {reason}")
                                            continue
                                        
                                        unique_results.append({
                                            "index": len(unique_results) + 1,
                                            "title": parsed_title,
                                            "snippet": parsed_snippet[:500],
                                            "url": parsed_url,
                                            "source": "search"
                                        })
                                        logger.info(f"[{self.name}] Parsed result: {parsed_title[:50]}... (URL: {parsed_url[:50] if parsed_url else 'N/A'}...)")
                                    
                                    # ì›ë³¸ ê²°ê³¼ëŠ” ê±´ë„ˆë›°ê¸°
                                    continue
                            
                            logger.debug(f"[{self.name}] Result {i}: title={title[:50] if title else 'N/A'}, url={url[:50] if url else 'N/A'}")
                        elif isinstance(result, str):
                            # ë¬¸ìì—´ í˜•ì‹ì¸ ê²½ìš° íŒŒì‹± ì‹œë„ (ë§ˆí¬ë‹¤ìš´ ë§í¬ í˜•ì‹)
                            import re
                            link_match = re.match(r'^\d+\.\s*\[([^\]]+)\]\(([^\)]+)\)', result.strip())
                            if link_match:
                                title = link_match.group(1)
                                url = link_match.group(2)
                                snippet = ""
                                logger.info(f"[{self.name}] Parsed string result {i} as markdown: {title[:50]}")
                            else:
                                logger.warning(f"[{self.name}] Result {i} is string but not markdown format, skipping: {result[:100]}")
                                continue
                        else:
                            logger.warning(f"[{self.name}] Unknown result format for result {i}: {type(result)}, value: {str(result)[:100]}")
                            continue
                        
                        # URL ì¤‘ë³µ ì œê±°
                        if url and url in seen_urls:
                            logger.debug(f"[{self.name}] Duplicate URL skipped: {url}")
                            continue
                        if url:
                            seen_urls.add(url)
                        
                        # ë””ë²„ê¹…: ì›ë³¸ ë°ì´í„° ë¡œê¹…
                        logger.debug(f"[{self.name}] Result {i} ì›ë³¸ ë°ì´í„° - title: '{title[:80]}', snippet: '{snippet[:150] if snippet else '(empty)'}', url: '{url[:80] if url else '(empty)'}'")
                        
                        # snippet ë‚´ìš©ìœ¼ë¡œ ìœ íš¨í•˜ì§€ ì•Šì€ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§
                        invalid_indicators = [
                            "no results were found", "bot detection",
                            "no results", "not found", "try again",
                            "unable to", "error occurred", "no matches"
                        ]
                        snippet_lower = snippet.lower() if snippet else ""
                        matched_indicators = [ind for ind in invalid_indicators if ind in snippet_lower]
                        
                        if matched_indicators:
                            filtered_count += 1
                            reason = f"Matched indicators: {', '.join(matched_indicators)}"
                            filtered_reasons.append({
                                "result_index": i,
                                "title": title[:80],
                                "reason": reason,
                                "snippet_preview": snippet[:200] if snippet else "(empty)"
                            })
                            logger.warning(f"[{self.name}] âš ï¸ Filtering invalid search result {i}: '{title[:60]}...' - Reason: {reason}")
                            logger.debug(f"[{self.name}]   Filtered snippet preview: '{snippet[:200] if snippet else '(empty)'}'")
                            continue
                        
                        # êµ¬ì¡°í™”ëœ ê²°ê³¼ ì €ì¥
                        result_dict = {
                            "index": len(unique_results) + 1,
                            "title": title,
                            "snippet": snippet[:500] if snippet else "",
                            "url": url,
                            "source": "search"
                        }
                        unique_results.append(result_dict)
                        
                        logger.info(f"[{self.name}] Result {i}: {title[:50]}... (URL: {url[:50] if url else 'N/A'}...)")
                    
                    # í•„í„°ë§ í†µê³„ ë¡œê¹…
                    total_processed = len(search_results)
                    valid_results = len(unique_results)
                    logger.info(f"[{self.name}] ğŸ“Š í•„í„°ë§ í†µê³„: ì´ {total_processed}ê°œ ì¤‘ {filtered_count}ê°œ í•„í„°ë§ë¨, {valid_results}ê°œ ìœ íš¨í•œ ê²°ê³¼")
                    
                    if filtered_count > 0:
                        logger.warning(f"[{self.name}] âš ï¸ í•„í„°ë§ëœ ê²°ê³¼ ìƒì„¸:")
                        for fr in filtered_reasons[:5]:  # ìµœëŒ€ 5ê°œë§Œ ìƒì„¸ ë¡œê¹…
                            logger.warning(f"[{self.name}]   - ê²°ê³¼ {fr['result_index']}: '{fr['title']}' - {fr['reason']}")
                            logger.warning(f"[{self.name}]     Snippet: '{fr['snippet_preview']}'")
                        if len(filtered_reasons) > 5:
                            logger.warning(f"[{self.name}]   ... ì™¸ {len(filtered_reasons) - 5}ê°œ ê²°ê³¼ë„ í•„í„°ë§ë¨")
                    
                    # ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    if unique_results:
                        results = unique_results
                        logger.info(f"[{self.name}] âœ… Collected {len(results)} unique results")
                        
                        # ìµœì†Œ 5ê°œ ì´ìƒì˜ ê³ ìœ í•œ ì¶œì²˜ ë³´ì¥
                        MIN_UNIQUE_SOURCES = 5
                        unique_urls = set()
                        for result in results:
                            url = result.get('url', '')
                            if url:
                                # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
                                try:
                                    from urllib.parse import urlparse
                                    parsed = urlparse(url)
                                    domain = f"{parsed.scheme}://{parsed.netloc}"
                                    unique_urls.add(domain)
                                except:
                                    unique_urls.add(url)
                        
                        logger.info(f"[{self.name}] ğŸ“Š Unique sources found: {len(unique_urls)} (minimum required: {MIN_UNIQUE_SOURCES})")
                        
                        # ì¶œì²˜ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
                        if len(unique_urls) < MIN_UNIQUE_SOURCES:
                            logger.warning(f"[{self.name}] âš ï¸ Only {len(unique_urls)} unique sources found, need at least {MIN_UNIQUE_SOURCES}. Performing additional searches...")
                            
                            # ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë‹¤ì–‘í•œ ê´€ì )
                            additional_queries = []
                            base_query = query
                            
                            # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ íŒ¨í„´ ì‹œë„
                            additional_patterns = [
                                f"{base_query} ë‰´ìŠ¤",
                                f"{base_query} ë¦¬í¬íŠ¸",
                                f"{base_query} ì¡°ì‚¬",
                                f"{base_query} í†µê³„",
                                f"{base_query} ìë£Œ"
                            ]
                            
                            # ì´ë¯¸ ì‚¬ìš©í•œ ì¿¼ë¦¬ ì œì™¸
                            used_queries = set(search_queries)
                            for pattern in additional_patterns:
                                if pattern not in used_queries and len(additional_queries) < 3:
                                    additional_queries.append(pattern)
                            
                            if additional_queries:
                                logger.info(f"[{self.name}] ğŸ” Executing {len(additional_queries)} additional searches for more sources...")
                                
                                # ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰
                                additional_search_tasks = [execute_single_search(q, len(search_queries) + i) for i, q in enumerate(additional_queries)]
                                additional_results_list = await asyncio.gather(*additional_search_tasks)
                                
                                # ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
                                additional_unique_results = []
                                additional_seen_urls = seen_urls.copy()
                                
                                for sr in additional_results_list:
                                    if sr.get('success') and sr.get('result', {}).get('data'):
                                        result_data = sr['result'].get('data', {})
                                        if isinstance(result_data, dict):
                                            items = result_data.get('results', result_data.get('items', []))
                                            if isinstance(items, list):
                                                for item in items:
                                                    if isinstance(item, dict):
                                                        url = item.get('url', item.get('link', ''))
                                                        if url and url not in additional_seen_urls:
                                                            title = item.get('title', item.get('name', ''))
                                                            snippet = item.get('snippet', item.get('content', ''))
                                                            if title and len(title.strip()) >= 3:
                                                                additional_unique_results.append({
                                                                    "index": len(results) + len(additional_unique_results) + 1,
                                                                    "title": title,
                                                                    "snippet": snippet[:500] if snippet else '',
                                                                    "url": url,
                                                                    "source": "additional_search"
                                                                })
                                                                additional_seen_urls.add(url)
                                        
                                        # ë„ë©”ì¸ ì¶”ì¶œí•˜ì—¬ ê³ ìœ  ì¶œì²˜ í™•ì¸
                                        for item in additional_unique_results:
                                            url = item.get('url', '')
                                            if url:
                                                try:
                                                    from urllib.parse import urlparse
                                                    parsed = urlparse(url)
                                                    domain = f"{parsed.scheme}://{parsed.netloc}"
                                                    unique_urls.add(domain)
                                                except:
                                                    unique_urls.add(url)
                                        
                                        # ì¶©ë¶„í•œ ì¶œì²˜ë¥¼ ì–»ìœ¼ë©´ ì¤‘ë‹¨
                                        if len(unique_urls) >= MIN_UNIQUE_SOURCES:
                                            break
                                
                                if additional_unique_results:
                                    results.extend(additional_unique_results)
                                    logger.info(f"[{self.name}] âœ… Added {len(additional_unique_results)} additional results from {len(additional_queries)} searches")
                                    logger.info(f"[{self.name}] ğŸ“Š Total unique sources: {len(unique_urls)} (target: {MIN_UNIQUE_SOURCES})")
                                else:
                                    logger.warning(f"[{self.name}] âš ï¸ Additional searches did not yield new unique sources")
                            else:
                                logger.warning(f"[{self.name}] âš ï¸ No additional query patterns available")
                        else:
                            logger.info(f"[{self.name}] âœ… Sufficient unique sources found: {len(unique_urls)} >= {MIN_UNIQUE_SOURCES}")
                        
                        # ìµœì¢… ê²°ê³¼ ìš”ì•½
                        final_unique_sources = set()
                        for result in results:
                            url = result.get('url', '')
                            if url:
                                try:
                                    from urllib.parse import urlparse
                                    parsed = urlparse(url)
                                    domain = f"{parsed.scheme}://{parsed.netloc}"
                                    final_unique_sources.add(domain)
                                except:
                                    final_unique_sources.add(url)
                        
                        logger.info(f"[{self.name}] ğŸ“Š Final collection: {len(results)} results from {len(final_unique_sources)} unique sources")
                        if len(final_unique_sources) < MIN_UNIQUE_SOURCES:
                            logger.warning(f"[{self.name}] âš ï¸ Warning: Only {len(final_unique_sources)} unique sources collected (target: {MIN_UNIQUE_SOURCES})")
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ê²€í†  ë° ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš© í¬ë¡¤ë§
                        logger.info(f"[{self.name}] ğŸ” Reviewing search results and fetching full web content...")
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ê²€í†  ë° ì‹¤ì œ ì›¹ í˜ì´ì§€ í¬ë¡¤ë§
                        enriched_results = []
                        for result in results:
                            url = result.get('url', '')
                            if not url:
                                enriched_results.append(result)
                                continue
                            
                            try:
                                # ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                                logger.info(f"[{self.name}] ğŸ“¥ Fetching full content from: {url[:80]}...")
                                fetch_result = await execute_tool("fetch", {"url": url})
                                
                                if fetch_result.get('success') and fetch_result.get('data'):
                                    content = fetch_result.get('data', {}).get('content', '')
                                    if content:
                                        # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
                                        import re
                                        from bs4 import BeautifulSoup
                                        
                                        try:
                                            soup = BeautifulSoup(content, 'html.parser')
                                            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, í—¤ë”, í‘¸í„° ì œê±°
                                            for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):
                                                element.decompose()
                                            
                                            # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ
                                            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|article|post|main', re.I))
                                            if main_content:
                                                full_text = main_content.get_text(separator='\n', strip=True)
                                            else:
                                                full_text = soup.get_text(separator='\n', strip=True)
                                            
                                            # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë„ˆë¬´ ê¸´ ê³µë°± ì œê±°)
                                            full_text = re.sub(r'\n{3,}', '\n\n', full_text)
                                            full_text = re.sub(r' {3,}', ' ', full_text)
                                            
                                            # ìµœëŒ€ ê¸¸ì´ ì œí•œ (50000ì)
                                            if len(full_text) > 50000:
                                                full_text = full_text[:50000] + "... [truncated]"
                                            
                                            result['full_content'] = full_text
                                            result['content_length'] = len(full_text)
                                            
                                            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                                            date_patterns = [
                                                r'(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})',  # YYYY-MM-DD
                                                r'(\d{1,2})[.\-/](\d{1,2})[.\-/](\d{4})',  # MM-DD-YYYY
                                                r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',  # í•œêµ­ì–´ í˜•ì‹
                                            ]
                                            
                                            date_found = None
                                            for pattern in date_patterns:
                                                matches = re.findall(pattern, full_text[:5000])  # ì²˜ìŒ 5000ìë§Œ ê²€ìƒ‰
                                                if matches:
                                                    try:
                                                        from datetime import datetime
                                                        match = matches[-1]  # ê°€ì¥ ìµœê·¼ ë‚ ì§œ
                                                        if len(match) == 3:
                                                            if 'ë…„' in full_text[:5000]:
                                                                # í•œêµ­ì–´ í˜•ì‹
                                                                date_str = f"{match[0]}-{match[1].zfill(2)}-{match[2].zfill(2)}"
                                                            elif len(match[0]) == 4:
                                                                # YYYY-MM-DD
                                                                date_str = f"{match[0]}-{match[1].zfill(2)}-{match[2].zfill(2)}"
                                                            else:
                                                                # MM-DD-YYYY
                                                                date_str = f"{match[2]}-{match[0].zfill(2)}-{match[1].zfill(2)}"
                                                            date_found = datetime.strptime(date_str, "%Y-%m-%d")
                                                            break
                                                    except:
                                                        continue
                                            
                                            if date_found:
                                                result['published_date'] = date_found.isoformat()
                                                logger.info(f"[{self.name}] ğŸ“… Found date: {date_found.strftime('%Y-%m-%d')} for {url[:50]}...")
                                            else:
                                                # ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì • (ìµœì‹  ì •ë³´ ìš°ì„ )
                                                from datetime import datetime
                                                result['published_date'] = datetime.now().isoformat()
                                                logger.info(f"[{self.name}] âš ï¸ No date found, using current time for {url[:50]}...")
                                            
                                            logger.info(f"[{self.name}] âœ… Fetched {len(full_text)} characters from {url[:50]}...")
                                        except Exception as e:
                                            logger.warning(f"[{self.name}] âš ï¸ Failed to parse HTML from {url[:50]}...: {e}")
                                            # íŒŒì‹± ì‹¤íŒ¨í•´ë„ ì›ë³¸ ê²°ê³¼ëŠ” ìœ ì§€
                                            result['full_content'] = content[:50000] if len(content) > 50000 else content
                                            result['content_length'] = len(result['full_content'])
                                    else:
                                        logger.warning(f"[{self.name}] âš ï¸ No content fetched from {url[:50]}...")
                                else:
                                    logger.warning(f"[{self.name}] âš ï¸ Failed to fetch content from {url[:50]}...: {fetch_result.get('error', 'Unknown error')}")
                            except Exception as e:
                                logger.error(f"[{self.name}] âŒ Error fetching content from {url[:50]}...: {e}")
                            
                            enriched_results.append(result)
                        
                        # ìµœì‹  ì •ë³´ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
                        from datetime import datetime
                        enriched_results.sort(key=lambda x: (
                            datetime.fromisoformat(x.get('published_date', datetime.now().isoformat())) if x.get('published_date') else datetime.min,
                            x.get('content_length', 0)
                        ), reverse=True)
                        
                        logger.info(f"[{self.name}] âœ… Enriched {len(enriched_results)} results with full web content")
                        results = enriched_results
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ê²€í†  (LLMìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í‰ê°€)
                        logger.info(f"[{self.name}] ğŸ” Reviewing search results for relevance and recency...")
                        try:
                            from src.core.llm_manager import execute_llm_task, TaskType
                            
                            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë° í‰ê°€
                            review_prompt = f"""ë‹¤ìŒì€ '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ê²°ê³¼ë¥¼ ê²€í† í•˜ì—¬:
1. ì‚¬ìš©ì ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„± í‰ê°€
2. ì •ë³´ì˜ ìµœì‹ ì„± í™•ì¸ (ë‚ ì§œ ì •ë³´ í¬í•¨)
3. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì¸ì§€ í™•ì¸
4. ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš©ì´ ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸

ê²€ìƒ‰ ê²°ê³¼:
{chr(10).join([f"{i+1}. {r.get('title', 'N/A')} - {r.get('url', 'N/A')} - ë‚ ì§œ: {r.get('published_date', 'N/A')} - ë‚´ìš© ê¸¸ì´: {r.get('content_length', 0)}ì" for i, r in enumerate(results[:10])])}

ê° ê²°ê³¼ì— ëŒ€í•´:
- ê´€ë ¨ì„± ì ìˆ˜ (0-10)
- ìµœì‹ ì„± í‰ê°€ (ìµœì‹ /ë³´í†µ/ì˜¤ë˜ë¨)
- ì‹ ë¢°ë„ í‰ê°€ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)
- ì¶”ì²œ ì—¬ë¶€ (ì¶”ì²œ/ë³´í†µ/ë¹„ì¶”ì²œ)

í˜•ì‹: JSON ë°°ì—´ë¡œ ë°˜í™˜
[
  {{
    "index": 1,
    "relevance_score": 8,
    "recency": "ìµœì‹ ",
    "reliability": "ë†’ìŒ",
    "recommend": "ì¶”ì²œ",
    "reason": "ìµœì‹  ì •ë³´ì´ë©° ì¿¼ë¦¬ì™€ ì§ì ‘ ê´€ë ¨"
  }},
  ...
]
"""
                            
                            review_result = await execute_llm_task(
                                prompt=review_prompt,
                                task_type=TaskType.ANALYSIS,
                                model_name=None,
                                system_message="You are an expert information analyst who evaluates search results for relevance, recency, and reliability."
                            )
                            
                            # LLM ê²°ê³¼ íŒŒì‹±
                            import json
                            review_text = review_result.content or ""
                            try:
                                # JSON ì¶”ì¶œ
                                json_match = re.search(r'\[.*\]', review_text, re.DOTALL)
                                if json_match:
                                    review_data = json.loads(json_match.group())
                                    
                                    # ê²€í†  ê²°ê³¼ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                                    for review_item in review_data:
                                        idx = review_item.get('index', 0) - 1
                                        if 0 <= idx < len(results):
                                            results[idx]['review'] = {
                                                'relevance_score': review_item.get('relevance_score', 5),
                                                'recency': review_item.get('recency', 'ë³´í†µ'),
                                                'reliability': review_item.get('reliability', 'ë³´í†µ'),
                                                'recommend': review_item.get('recommend', 'ë³´í†µ'),
                                                'reason': review_item.get('reason', '')
                                            }
                                    
                                    # ì¶”ì²œ ê²°ê³¼ë§Œ í•„í„°ë§ (ì„ íƒì )
                                    recommended_results = [r for r in results if r.get('review', {}).get('recommend') == 'ì¶”ì²œ']
                                    if recommended_results:
                                        logger.info(f"[{self.name}] âœ… Found {len(recommended_results)} highly recommended results")
                                        # ì¶”ì²œ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜, ìµœì†Œ 5ê°œëŠ” ìœ ì§€
                                        if len(recommended_results) >= 5:
                                            results = recommended_results
                                        else:
                                            # ì¶”ì²œ ê²°ê³¼ + ì¼ë°˜ ê²°ê³¼ í˜¼í•©
                                            results = recommended_results + [r for r in results if r not in recommended_results][:5-len(recommended_results)]
                                    
                                    logger.info(f"[{self.name}] âœ… Reviewed {len(review_data)} search results")
                            except Exception as e:
                                logger.warning(f"[{self.name}] âš ï¸ Failed to parse review result: {e}")
                        except Exception as e:
                            logger.warning(f"[{self.name}] âš ï¸ Failed to review search results: {e}")
                    else:
                        # ëª¨ë“  ê²°ê³¼ê°€ í•„í„°ë§ëœ ê²½ìš° ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€
                        error_details = []
                        error_details.append(f"ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100]}'")
                        error_details.append(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {total_processed}ê°œ")
                        error_details.append(f"í•„í„°ë§ëœ ê²°ê³¼: {filtered_count}ê°œ")
                        error_details.append(f"ìœ íš¨í•œ ê²°ê³¼: 0ê°œ")
                        
                        if filtered_reasons:
                            error_details.append("\ní•„í„°ë§ëœ ê²°ê³¼ ìƒì„¸:")
                            for fr in filtered_reasons[:3]:  # ìµœëŒ€ 3ê°œë§Œ ì—ëŸ¬ ë©”ì‹œì§€ì— í¬í•¨
                                error_details.append(f"  - ê²°ê³¼ {fr['result_index']}: '{fr['title']}' - {fr['reason']}")
                        
                        error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.\n" + "\n".join(error_details)
                        logger.error(f"[{self.name}] âŒ {error_msg}")
                        raise RuntimeError(error_msg)
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ - ì‹¤íŒ¨ ì²˜ë¦¬
                    logger.error(f"[{self.name}] âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    logger.error(f"[{self.name}]   ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100]}'")
                    logger.error(f"[{self.name}]   ê²€ìƒ‰ ë„êµ¬: {search_result.get('source', 'unknown')}")
                    logger.error(f"[{self.name}]   ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€: {search_result.get('success', False)}")
                    if search_result.get('error'):
                        logger.error(f"[{self.name}]   ê²€ìƒ‰ ì—ëŸ¬: {search_result.get('error')}")
                    error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: '{query[:100]}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    logger.error(f"[{self.name}] âŒ {error_msg}")
                    raise RuntimeError(error_msg)
            else:
                # ê²€ìƒ‰ ì‹¤íŒ¨ - ì—ëŸ¬ ë°˜í™˜
                logger.error(f"[{self.name}] âŒ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨")
                logger.error(f"[{self.name}]   ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100]}'")
                logger.error(f"[{self.name}]   ê²€ìƒ‰ ë„êµ¬: {search_result.get('source', 'unknown')}")
                logger.error(f"[{self.name}]   ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€: {search_result.get('success', False)}")
                logger.error(f"[{self.name}]   ì—ëŸ¬ ë©”ì‹œì§€: {search_result.get('error', 'Unknown error')}")
                if search_result.get('data'):
                    logger.debug(f"[{self.name}]   ì‘ë‹µ ë°ì´í„° íƒ€ì…: {type(search_result.get('data'))}")
                    logger.debug(f"[{self.name}]   ì‘ë‹µ ë°ì´í„° ìƒ˜í”Œ: {str(search_result.get('data'))[:200]}")
                error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. {search_result.get('error', 'Unknown error')}"
                logger.error(f"[{self.name}] âŒ {error_msg}")
                raise RuntimeError(error_msg)
                
        except Exception as e:
            # ì‹¤ì œ ì˜¤ë¥˜ ë°œìƒ - ì‹¤íŒ¨ ì²˜ë¦¬
            import traceback
            error_type = type(e).__name__
            error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            logger.error(f"[{self.name}] âŒ ì˜ˆì™¸ ë°œìƒ: {error_type}")
            logger.error(f"[{self.name}]   ì—ëŸ¬ ë©”ì‹œì§€: {error_msg}")
            logger.error(f"[{self.name}]   ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100] if 'query' in locals() else 'N/A'}'")
            logger.debug(f"[{self.name}]   Traceback:\n{traceback.format_exc()}")
            
            # ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡
            state['research_results'] = []
            state['current_agent'] = self.name
            state['error'] = error_msg
            state['research_failed'] = True
            
            # ë©”ëª¨ë¦¬ì— ì‹¤íŒ¨ ì •ë³´ ê¸°ë¡
            memory.write(
                key=f"execution_error_{state['session_id']}",
                value=error_msg,
                scope=MemoryScope.SESSION,
                session_id=state['session_id'],
                agent_id=self.name
            )
            
            # ì‹¤íŒ¨ ìƒíƒœ ë°˜í™˜ (ë”ë¯¸ ë°ì´í„° ì—†ì´)
            return state
        
        # Council í™œì„±í™” í™•ì¸ ë° ì ìš© (ì¤‘ìš”í•œ ì •ë³´ ìˆ˜ì§‘ ì‹œ)
        use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
        if use_council is None:
            # ìë™ í™œì„±í™” íŒë‹¨
            from src.core.council_activator import get_council_activator
            activator = get_council_activator()
            
            # ì¤‘ìš”í•œ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•œì§€ íŒë‹¨
            context = {
                'results_count': len(results),
                'has_controversial_topic': any(
                    keyword in state['user_query'].lower() 
                    for keyword in ['debate', 'controversy', 'disagreement', 'ë…¼ìŸ', 'ì˜ê²¬']
                ),
                'high_stakes': any(
                    keyword in state['user_query'].lower()
                    for keyword in ['critical', 'important', 'decision', 'ì¤‘ìš”í•œ', 'ê²°ì •']
                )
            }
            
            activation_decision = activator.should_activate(
                process_type='execution',
                query=state['user_query'],
                context=context
            )
            use_council = activation_decision.should_activate
            if use_council:
                logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
        
        # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if use_council and results:
            try:
                from src.core.llm_council import run_full_council
                logger.info(f"[{self.name}] ğŸ›ï¸ Running Council verification for research results...")
                
                # ê²°ê³¼ ìš”ì•½ ìƒì„±
                results_summary = "\n\n".join([
                    f"Result {i+1}:\nTitle: {r.get('title', 'N/A')}\nURL: {r.get('url', 'N/A')}\nSnippet: {r.get('snippet', 'N/A')[:200]}"
                    for i, r in enumerate(results[:10])  # ìµœëŒ€ 10ê°œë§Œ ê²€í† 
                ])
                
                council_query = f"""Verify the accuracy and reliability of the following research results. Identify any inconsistencies, missing information, or potential issues.

Research Query: {state['user_query']}

Research Results:
{results_summary}

Provide a verification report with:
1. Accuracy assessment
2. Missing information
3. Recommendations for improvement"""
                
                stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                    council_query
                )
                
                # Council ê²€ì¦ ê²°ê³¼ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                verification_report = stage3_result.get('response', '')
                logger.info(f"[{self.name}] âœ… Council verification completed.")
                logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                
                # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                if 'council_metadata' not in state:
                    state['council_metadata'] = {}
                state['council_metadata']['execution'] = {
                    'stage1_results': stage1_results,
                    'stage2_results': stage2_results,
                    'stage3_result': stage3_result,
                    'metadata': metadata,
                    'verification_report': verification_report
                }
                
                # ê²€ì¦ ë¦¬í¬íŠ¸ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                results.append({
                    'title': 'Council Verification Report',
                    'url': '',
                    'snippet': verification_report,
                    'source': 'council',
                    'council_verified': True
                })
            except Exception as e:
                logger.warning(f"[{self.name}] Council verification failed: {e}. Using original results.")
                # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²°ê³¼ ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        
        # ì„±ê³µì ìœ¼ë¡œ ê²°ê³¼ ìˆ˜ì§‘ëœ ê²½ìš°
        state['research_results'] = results  # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ (ë®ì–´ì“°ê¸°)
        state['current_agent'] = self.name
        state['research_failed'] = False
        
        logger.info(f"[{self.name}] âœ… Research execution completed: {len(results)} results")
        
        # Write to shared memory (êµ¬ì¡°í™”ëœ í˜•ì‹)
        memory.write(
            key=f"research_results_{state['session_id']}",
            value=results,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Results saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class VerifierAgent:
    """Verifier agent - verifies research results (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "verifier"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("evaluator")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a verification agent."
    
    async def execute(self, state: AgentState) -> AgentState:
        """Verify research results with LLM-based verification."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting verification")
        logger.info(f"=" * 80)
        
        # ì—°êµ¬ ì‹¤íŒ¨ í™•ì¸
        if state.get('research_failed'):
            logger.error(f"[{self.name}] âŒ Research execution failed, skipping verification")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['current_agent'] = self.name
            return state
        
        memory = self.context.shared_memory
        
        # Read results from state or shared memory
        results = state.get('research_results', [])
        if not results:
            results = memory.read(
                key=f"research_results_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        # SharedResultsManagerì—ì„œ ë‹¤ë¥¸ Executorì˜ ê²°ê³¼ë„ ê°€ì ¸ì˜¤ê¸°
        if self.context.shared_results_manager:
            shared_results = await self.context.shared_results_manager.get_shared_results(
                exclude_agent_id=self.name
            )
            logger.info(f"[{self.name}] ğŸ” Found {len(shared_results)} shared results from other agents")

            # ê³µìœ ëœ ê²°ê³¼ë¥¼ resultsì— ì¶”ê°€
            shared_data_count = 0
            for shared_result in shared_results:
                if isinstance(shared_result.result, dict) and shared_result.result.get('data'):
                    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
                    data = shared_result.result.get('data', {})
                    if isinstance(data, dict):
                        shared_search_results = data.get('results', data.get('items', []))
                        if isinstance(shared_search_results, list):
                            results.extend(shared_search_results)
                            shared_data_count += len(shared_search_results)
                    elif isinstance(data, list):
                        results.extend(data)
                        shared_data_count += len(data)

            logger.info(f"[{self.name}] ğŸ“¥ Retrieved {shared_data_count} additional results from {len(shared_results)} shared agent results")
            logger.info(f"[{self.name}] ğŸ¤ Agent communication: Retrieved results from agents: {[r.agent_id for r in shared_results]}")
        
        logger.info(f"[{self.name}] Found {len(results)} results to verify (including shared results)")
        
        if not results or len(results) == 0:
            # ê²€ì¦í•  ê²°ê³¼ê°€ ì—†ëŠ” ì´ìœ  ìƒì„¸ ë¶„ì„
            logger.error(f"[{self.name}] âŒ ê²€ì¦í•  ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # stateì—ì„œ ê²°ê³¼ ì¶”ì 
            execution_results = state.get('execution_results', [])
            compression_results = state.get('compression_results', [])
            shared_results = state.get('shared_results', [])
            
            logger.error(f"[{self.name}] ğŸ“‹ ê²°ê³¼ ì¶”ì :")
            logger.error(f"[{self.name}]   - execution_results: {len(execution_results) if isinstance(execution_results, list) else 0}ê°œ")
            logger.error(f"[{self.name}]   - compression_results: {len(compression_results) if isinstance(compression_results, list) else 0}ê°œ")
            logger.error(f"[{self.name}]   - shared_results: {len(shared_results) if isinstance(shared_results, list) else 0}ê°œ")
            logger.error(f"[{self.name}]   - ê²€ì¦ì— ì „ë‹¬ëœ results: {len(results) if isinstance(results, list) else 0}ê°œ")
            
            # execution_results ìƒì„¸ ë¶„ì„
            if execution_results:
                successful_executions = [er for er in execution_results if er.get('success', False)]
                failed_executions = [er for er in execution_results if not er.get('success', False)]
                logger.error(f"[{self.name}]   - ì„±ê³µí•œ ì‹¤í–‰: {len(successful_executions)}ê°œ")
                logger.error(f"[{self.name}]   - ì‹¤íŒ¨í•œ ì‹¤í–‰: {len(failed_executions)}ê°œ")
                
                if failed_executions:
                    logger.error(f"[{self.name}]   ğŸ“ ì‹¤íŒ¨í•œ ì‹¤í–‰ ìƒì„¸:")
                    for i, fe in enumerate(failed_executions[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                        error = fe.get('error', 'Unknown error')
                        logger.error(f"[{self.name}]     {i}. {str(error)[:100]}")
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            search_results_found = False
            for er in execution_results if isinstance(execution_results, list) else []:
                if isinstance(er, dict):
                    data = er.get('data', {})
                    if isinstance(data, dict):
                        results_data = data.get('results', data.get('items', []))
                        if results_data and len(results_data) > 0:
                            search_results_found = True
                            logger.error(f"[{self.name}]   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆì§€ë§Œ ê²€ì¦ ë‹¨ê³„ì— ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                            break
            
            if not search_results_found:
                logger.error(f"[{self.name}]   âš ï¸ ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ExecutorAgentì˜ ê²€ìƒ‰ ì‹¤íŒ¨ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            error_msg = "ê²€ì¦ ì‹¤íŒ¨: ê²€ì¦í•  ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"[{self.name}] âŒ {error_msg}")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['error'] = error_msg
            state['current_agent'] = self.name
            return state
        
        # LLMì„ ì‚¬ìš©í•œ ì‹¤ì œ ê²€ì¦
        from src.core.llm_manager import execute_llm_task, TaskType
        
        verified = []
        rejected_reasons = []  # ê²€ì¦ ì‹¤íŒ¨ ì›ì¸ ì¶”ì 
        skipped_count = 0
        verification_errors = []
        
        user_query = state.get('user_query', '')
        logger.info(f"[{self.name}] ğŸ” Starting verification of {len(results)} results for query: '{user_query}'")
        
        for i, result in enumerate(results, 1):
            if isinstance(result, dict):
                # ë‹¤ì–‘í•œ í‚¤ì—ì„œ title, snippet, url ì¶”ì¶œ ì‹œë„
                title = result.get('title') or result.get('name') or result.get('Title') or result.get('headline') or ''
                snippet = result.get('snippet') or result.get('content') or result.get('summary') or result.get('description') or result.get('abstract') or ''
                url = result.get('url') or result.get('link') or result.get('href') or result.get('URL') or ''
                
                # titleì´ ë¹„ì–´ìˆê±°ë‚˜ "Search Results" ê°™ì€ ë©”íƒ€ë°ì´í„°ì¸ ê²½ìš° ìŠ¤í‚µ
                if not title or len(title.strip()) < 3:
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: empty or invalid title")
                    continue
                
                # "Search Results", "Results", "Error" ê°™ì€ ë©”íƒ€ë°ì´í„° ì œì™¸
                title_lower = title.lower().strip()
                if title_lower in ['search results', 'results', 'error', 'no results', 'no title']:
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: metadata title '{title}'")
                    continue
                
                # snippetì´ ë¹„ì–´ìˆê³  urlë„ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                if not snippet and not url:
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: no content or URL")
                    continue

                # snippet ë‚´ìš©ìœ¼ë¡œ ìœ íš¨í•˜ì§€ ì•Šì€ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§
                invalid_indicators = [
                    "no results were found", "bot detection",
                    "no results", "not found", "try again",
                    "unable to", "error occurred", "no matches"
                ]
                snippet_lower = snippet.lower() if snippet else ""
                if any(indicator in snippet_lower for indicator in invalid_indicators):
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: invalid snippet content (contains error message)")
                    continue
                
                # full_content ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ snippet ì‚¬ìš©
                full_content = result.get('full_content', '')
                verification_content = full_content[:2000] if full_content else (snippet[:800] if snippet else 'ë‚´ìš© ì—†ìŒ')
                
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                published_date = result.get('published_date', '')
                date_info = ""
                if published_date:
                    try:
                        from datetime import datetime
                        date_obj = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        date_info = f"\n- ë°œí–‰ì¼: {date_obj.strftime('%Y-%m-%d')}"
                    except:
                        date_info = f"\n- ë°œí–‰ì¼: {published_date[:10]}"
                
                # LLMìœ¼ë¡œ ê²€ì¦
                verification_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ì„¸ìš” (ìµœì‹  ì •ë³´ ìš°ì„ ):

ì œëª©: {title}
ë‚´ìš©: {verification_content}
URL: {url if url else 'URL ì—†ìŒ'}{date_info}

ì›ë˜ ì¿¼ë¦¬: {user_query}

ì´ ê²°ê³¼ê°€ ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©° ìµœì‹  ì •ë³´ì¸ì§€ ê²€ì¦í•˜ì„¸ìš”.
- ì¿¼ë¦¬ì˜ ì£¼ì œì™€ ê´€ë ¨ì´ ìˆê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ë©´ "VERIFIED"ë¡œ ì‘ë‹µ
- ì¿¼ë¦¬ì™€ ì „í˜€ ë¬´ê´€í•˜ê±°ë‚˜ ì‹ ë¢°í•  ìˆ˜ ì—†ìœ¼ë©´ "REJECTED"ë¡œ ì‘ë‹µ
- ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆê±°ë‚˜ ê°„ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆì–´ë„ "VERIFIED"ë¡œ ì‘ë‹µ ê°€ëŠ¥
- **ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”** (ë‚ ì§œê°€ ìµœê·¼ì´ë©´ ë” ë†’ì€ ì ìˆ˜)

âš ï¸ ì¤‘ìš”: ë„ˆë¬´ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì§€ ë§ê³ , ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆë‹¤ê³  íŒë‹¨ë˜ë©´ "VERIFIED"ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹: "VERIFIED" ë˜ëŠ” "REJECTED"ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ í•œ ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
                
                try:
                    logger.info(f"[{self.name}] ğŸ” Verifying result {i}/{len(results)}: '{title[:60]}...'")
                    verification_result = await execute_llm_task(
                        prompt=verification_prompt,
                        task_type=TaskType.VERIFICATION,
                        model_name=None,
                        system_message="You are a verification agent. Verify if search results are relevant and reliable. Be reasonable - if the result is even partially related to the query, verify it."
                    )
                    
                    verification_text = verification_result.content or "UNKNOWN"
                    # ê²€ì¦ ë¡œì§ ê°œì„ : ëª…ì‹œì ìœ¼ë¡œ VERIFIEDê°€ ìˆê±°ë‚˜ REJECTEDê°€ ì—†ìœ¼ë©´ ê²€ì¦ë¨
                    verification_upper = verification_text.upper().strip()
                    is_verified = "VERIFIED" in verification_upper and "REJECTED" not in verification_upper
                    
                    logger.info(f"[{self.name}] ğŸ“‹ Verification result {i}: '{verification_text[:150]}' -> is_verified={is_verified}")
                    
                    if is_verified:
                        verified_result = {
                            "index": i,
                            "title": title,
                            "snippet": snippet,
                            "url": url,
                            "status": "verified",
                            "verification_note": verification_text[:200]
                        }
                        # full_contentì™€ published_date í¬í•¨
                        if full_content:
                            verified_result['full_content'] = full_content
                        if published_date:
                            verified_result['published_date'] = published_date
                        verified.append(verified_result)
                        logger.info(f"[{self.name}] âœ… Result {i} verified: '{title[:50]}...' (reason: {verification_text[:80]})")
                    else:
                        rejected_reasons.append({
                            "index": i,
                            "title": title[:80],
                            "reason": verification_text[:200],
                            "url": url[:100] if url else "N/A"
                        })
                        logger.info(f"[{self.name}] âš ï¸ Result {i} rejected: '{title[:50]}...' (reason: {verification_text[:100]})")
                        continue
                except Exception as e:
                    error_str = str(e).lower()
                    verification_errors.append({
                        "index": i,
                        "title": title[:80],
                        "error": str(e)[:200]
                    })
                    # Rate limitì´ë‚˜ ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ (í’ˆì§ˆ ì €í•˜ ë°©ì§€)
                    if "rate limit" in error_str or "429" in error_str or "all fallback models failed" in error_str or "no available models" in error_str:
                        logger.warning(f"[{self.name}] âš ï¸ Verification failed for result {i}: {e} (rate limit/all models failed), excluding from results")
                        continue  # í’ˆì§ˆ ì €í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ì œì™¸
                    else:
                        logger.warning(f"[{self.name}] âš ï¸ Verification failed for result {i}: {e}, including anyway")
                        # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨ (ë‹¨, rate limitì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                        if title and (snippet or url):
                            verified.append({
                                "index": i,
                                "title": title,
                                "snippet": snippet,
                                "url": url,
                                "status": "partial",
                                "verification_note": f"Verification failed: {str(e)[:100]}"
                            })
            else:
                skipped_count += 1
                logger.warning(f"[{self.name}] âš ï¸ Unknown result format: {type(result)}, value: {str(result)[:100]}")
                continue
        
        # ê²€ì¦ í†µê³„ ë° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        logger.info(f"[{self.name}] ğŸ“Š Verification Statistics:")
        logger.info(f"[{self.name}]   - Total results: {len(results)}")
        logger.info(f"[{self.name}]   - Verified: {len(verified)}")
        logger.info(f"[{self.name}]   - Rejected: {len(rejected_reasons)}")
        logger.info(f"[{self.name}]   - Skipped: {skipped_count}")
        logger.info(f"[{self.name}]   - Verification errors: {len(verification_errors)}")
        
        if rejected_reasons:
            logger.warning(f"[{self.name}] ğŸ” Rejected Results Analysis:")
            for rejected in rejected_reasons[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                logger.warning(f"[{self.name}]   - Result {rejected['index']}: '{rejected['title']}'")
                logger.warning(f"[{self.name}]     Reason: {rejected['reason']}")
                logger.warning(f"[{self.name}]     URL: {rejected['url']}")
        
        if verification_errors:
            logger.error(f"[{self.name}] âŒ Verification Errors:")
            for error_info in verification_errors[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                logger.error(f"[{self.name}]   - Result {error_info['index']}: '{error_info['title']}'")
                logger.error(f"[{self.name}]     Error: {error_info['error']}")
        
        # ê²€ì¦ëœ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ì›ë³¸ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ëŠ” fallback
        if not verified and len(results) > 0:
            logger.warning(f"[{self.name}] âš ï¸ No results verified! Using original results as fallback...")
            logger.warning(f"[{self.name}] ğŸ” This may indicate:")
            logger.warning(f"[{self.name}]   1. Search queries are not matching the user query")
            logger.warning(f"[{self.name}]   2. Verification criteria are too strict")
            logger.warning(f"[{self.name}]   3. Search results are genuinely irrelevant")
            
            # ì›ë³¸ ê²°ê³¼ë¥¼ ê²€ì¦ëœ ê²°ê³¼ë¡œ ì‚¬ìš© (ì‹ ë¢°ë„ ë‚®ê²Œ)
            for i, result in enumerate(results[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ
                if isinstance(result, dict):
                    title = result.get('title') or result.get('name') or ''
                    snippet = result.get('snippet') or result.get('content') or ''
                    url = result.get('url') or result.get('link') or ''
                    
                    if title and len(title.strip()) >= 3:
                        verified.append({
                            "index": i,
                            "title": title,
                            "snippet": snippet[:500] if snippet else '',
                            "url": url,
                            "status": "fallback",
                            "verification_note": "No verified results found, using original search results as fallback"
                        })
                        logger.warning(f"[{self.name}] âš ï¸ Added fallback result {i}: '{title[:50]}...'")
            
            logger.warning(f"[{self.name}] âš ï¸ Using {len(verified)} fallback results (low confidence)")
        
        logger.info(f"[{self.name}] âœ… Verification completed: {len(verified)}/{len(results)} results verified (including fallback)")
        
        # ê²€ì¦ ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ 
        if self.context.shared_results_manager:
            shared_verification_count = 0
            for verified_result in verified:
                task_id = f"verification_{verified_result.get('index', 0)}"
                result_id = await self.context.shared_results_manager.share_result(
                    task_id=task_id,
                    agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                    result=verified_result,
                    metadata={"status": verified_result.get('status', 'unknown')},
                    confidence=1.0 if verified_result.get('status') == 'verified' else 0.5
                )
                shared_verification_count += 1
                logger.info(f"[{self.name}] ğŸ”— Shared verification result {verified_result.get('index', 0)} (result_id: {result_id[:8]}..., status: {verified_result.get('status', 'unknown')})")

            logger.info(f"[{self.name}] ğŸ“¤ Shared {shared_verification_count} verification results with other agents")

            # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê²€ì¦ ê²°ê³¼ì™€ í† ë¡  (ê²€ì¦ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš°)
            if self.context.discussion_manager and len(verified) > 0:
                other_verified = await self.context.shared_results_manager.get_shared_results(
                    agent_id=None,  # ëª¨ë“  ì—ì´ì „íŠ¸
                    exclude_agent_id=self.context.agent_id  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                )

                # ê²€ì¦ëœ ê²°ê³¼ë§Œ í•„í„°ë§
                other_verified_results = [r for r in other_verified if isinstance(r.result, dict) and r.result.get('status') == 'verified']

                if other_verified_results:
                    logger.info(f"[{self.name}] ğŸ’¬ Found {len(other_verified_results)} verified results from other agents for discussion")

                    # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ 
                    first_verified = verified[0]
                    result_id = f"verification_{first_verified.get('index', 0)}"
                    logger.info(f"[{self.name}] ğŸ’¬ Starting discussion on verification result {first_verified.get('index', 0)} with {len(other_verified_results[:3])} other agents")

                    discussion = await self.context.discussion_manager.agent_discuss_result(
                        result_id=result_id,
                        agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                        other_agent_results=other_verified_results[:3]  # ìµœëŒ€ 3ê°œ
                    )
                    if discussion:
                        logger.info(f"[{self.name}] ğŸ’¬ Discussion completed: {discussion[:150]}... (agent_id: {self.context.agent_id})")
                        logger.info(f"[{self.name}] ğŸ¤ Agent discussion: Analyzed verification consistency with {len(other_verified_results[:3])} peer agents")
                    else:
                        logger.info(f"[{self.name}] ğŸ’¬ No discussion generated for verification result")
                else:
                    logger.info(f"[{self.name}] ğŸ’¬ No other verified results found for discussion")
            else:
                logger.info(f"[{self.name}] Agent discussion disabled or no verified results to discuss")
        
        # Council í™œì„±í™” í™•ì¸ ë° ì ìš© (ì‚¬ì‹¤ í™•ì¸ì´ ì¤‘ìš”í•œ ê²½ìš° - ê¸°ë³¸ í™œì„±í™”)
        use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
        if use_council is None:
            # ìë™ í™œì„±í™” íŒë‹¨ (ê¸°ë³¸ í™œì„±í™”)
            from src.core.council_activator import get_council_activator
            activator = get_council_activator()
            
            context = {
                'low_confidence_sources': len([r for r in verified if r.get('confidence', 1.0) < 0.7]),
                'verification_count': len(verified)
            }
            
            activation_decision = activator.should_activate(
                process_type='verification',
                query=state['user_query'],
                context=context
            )
            use_council = activation_decision.should_activate
            if use_council:
                logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
        
        # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if use_council and verified:
            try:
                from src.core.llm_council import run_full_council
                logger.info(f"[{self.name}] ğŸ›ï¸ Running Council review for verification results...")
                
                # ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±
                verification_summary = "\n\n".join([
                    f"Result {i+1}:\nTitle: {r.get('title', 'N/A')}\nStatus: {r.get('status', 'N/A')}\nConfidence: {r.get('confidence', 0.0):.2f}\nNote: {r.get('verification_note', 'N/A')[:100]}"
                    for i, r in enumerate(verified[:10])  # ìµœëŒ€ 10ê°œë§Œ ê²€í† 
                ])
                
                council_query = f"""Review the verification results and assess their reliability. Check for consistency and identify any potential issues.

Research Query: {state['user_query']}

Verification Results:
{verification_summary}

Provide a review with:
1. Overall verification quality assessment
2. Consistency check across results
3. Recommendations for improvement"""
                
                stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                    council_query
                )
                
                # Council ê²€í†  ê²°ê³¼
                review_report = stage3_result.get('response', '')
                logger.info(f"[{self.name}] âœ… Council review completed.")
                logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                
                # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                if 'council_metadata' not in state:
                    state['council_metadata'] = {}
                state['council_metadata']['verification'] = {
                    'stage1_results': stage1_results,
                    'stage2_results': stage2_results,
                    'stage3_result': stage3_result,
                    'metadata': metadata,
                    'review_report': review_report
                }
            except Exception as e:
                logger.warning(f"[{self.name}] Council review failed: {e}. Using original verification results.")
                # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²€ì¦ ê²°ê³¼ ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        
        state['verified_results'] = verified
        state['current_agent'] = self.name
        state['verification_failed'] = False if verified else True
        
        # Write to shared memory
        memory.write(
            key=f"verified_{state['session_id']}",
            value=verified,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Verified results saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class GeneratorAgent:
    """Generator agent - creates final report (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "generator"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("synthesizer")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a report generation agent."
    
    async def execute(self, state: AgentState) -> AgentState:
        """Generate final report."""
        logger.info(f"[{self.name}] Generating final report...")
        
        # ì—°êµ¬ ë˜ëŠ” ê²€ì¦ ì‹¤íŒ¨ í™•ì¸ - Fallback ì œê±°, ëª…í™•í•œ ì—ëŸ¬ë§Œ ë°˜í™˜
        if state.get('research_failed') or state.get('verification_failed'):
            error_msg = state.get('error')
            if not error_msg:
                if state.get('verification_failed'):
                    error_msg = "ê²€ì¦ ì‹¤íŒ¨: ê²€ì¦ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
                elif state.get('research_failed'):
                    error_msg = "ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨"
                else:
                    error_msg = "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"

            # ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            logger.error(f"[{self.name}] âŒ Research or verification failed: {error_msg}")
            logger.error(f"[{self.name}] ğŸ” Debugging Information:")
            logger.error(f"[{self.name}]   - Research failed: {state.get('research_failed', False)}")
            logger.error(f"[{self.name}]   - Verification failed: {state.get('verification_failed', False)}")
            logger.error(f"[{self.name}]   - User query: '{state.get('user_query', 'N/A')}'")
            
            # ê²€ì¦ ê²°ê³¼ í™•ì¸
            verified_results = state.get('verified_results', [])
            research_results = state.get('research_results', [])
            logger.error(f"[{self.name}]   - Verified results count: {len(verified_results) if verified_results else 0}")
            logger.error(f"[{self.name}]   - Research results count: {len(research_results) if research_results else 0}")
            
            # SharedResultsManagerì—ì„œ ê²°ê³¼ í™•ì¸
            if self.context.shared_results_manager:
                try:
                    shared_results = await self.context.shared_results_manager.get_shared_results(
                        agent_id=None
                    )
                    logger.error(f"[{self.name}]   - Shared results count: {len(shared_results) if shared_results else 0}")
                except Exception as e:
                    logger.error(f"[{self.name}]   - Failed to get shared results: {e}")
            
            # ê²€ì¦ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
            if state.get('verification_failed'):
                logger.error(f"[{self.name}] ğŸ” Verification Failure Analysis:")
                logger.error(f"[{self.name}]   - Possible causes:")
                logger.error(f"[{self.name}]     1. Search queries did not match user query")
                logger.error(f"[{self.name}]     2. Verification criteria were too strict")
                logger.error(f"[{self.name}]     3. Search results were genuinely irrelevant")
                logger.error(f"[{self.name}]     4. LLM verification service issues")
                
                # ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¼ë¶€ í‘œì‹œ
                if research_results and len(research_results) > 0:
                    logger.error(f"[{self.name}]   - Sample research results (first 3):")
                    for i, result in enumerate(research_results[:3], 1):
                        if isinstance(result, dict):
                            title = result.get('title', result.get('name', 'N/A'))[:60]
                            logger.error(f"[{self.name}]     {i}. {title}")
            
            state['final_report'] = None
            state['current_agent'] = self.name
            state['report_failed'] = True
            state['error'] = error_msg
            return state
        
        memory = self.context.shared_memory
        
        # Read verified results from state or shared memory
        verified_results = state.get('verified_results', [])
        if not verified_results:
            verified_results = memory.read(
                key=f"verified_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        # SharedResultsManagerì—ì„œ ëª¨ë“  ê³µìœ ëœ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        if self.context.shared_results_manager:
            all_shared_results = await self.context.shared_results_manager.get_shared_results()
            logger.info(f"[{self.name}] ğŸ” Found {len(all_shared_results)} total shared results from all agents")

            # ê³µìœ  ê²°ê³¼ í†µê³„
            verification_results = [r for r in all_shared_results if isinstance(r.result, dict) and r.result.get('status') == 'verified']
            search_results = [r for r in all_shared_results if not isinstance(r.result, dict) or r.result.get('status') != 'verified']

            logger.info(f"[{self.name}] ğŸ“Š Shared results breakdown: {len(verification_results)} verified, {len(search_results)} search results")

            # ê²€ì¦ëœ ê²°ê³¼ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ê°€
            added_from_shared = 0
            for shared_result in all_shared_results:
                if isinstance(shared_result.result, dict):
                    # ê²€ì¦ëœ ê²°ê³¼ì¸ ê²½ìš°
                    if shared_result.result.get('status') == 'verified':
                        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
                        existing_urls = {r.get('url', '') for r in verified_results if isinstance(r, dict)}
                        result_url = shared_result.result.get('url', '')
                        if result_url and result_url not in existing_urls:
                            verified_results.append(shared_result.result)
                            added_from_shared += 1
                            logger.info(f"[{self.name}] â• Added shared verified result from agent {shared_result.agent_id}: {shared_result.result.get('title', '')[:50]}...")

            logger.info(f"[{self.name}] ğŸ“¥ Added {added_from_shared} verified results from shared agent communications")
            logger.info(f"[{self.name}] ğŸ¤ Agent communication: Incorporated results from agents: {list(set(r.agent_id for r in all_shared_results))}")
        
        logger.info(f"[{self.name}] Found {len(verified_results)} verified results for report generation (including shared results)")
        
        if not verified_results or len(verified_results) == 0:
            # Fallback ì œê±° - ëª…í™•í•œ ì—ëŸ¬ë§Œ ë°˜í™˜
            error_msg = "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"[{self.name}] âŒ {error_msg}")
            state['final_report'] = None
            state['current_agent'] = self.name
            state['report_failed'] = True
            state['error'] = error_msg
            return state
        
        # ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° LLMìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±
        logger.info(f"[{self.name}] Generating report with LLM from {len(verified_results)} verified results...")
        
        # ê²€ì¦ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (full_content ìš°ì„  ì‚¬ìš©)
        verified_text = ""
        for i, result in enumerate(verified_results, 1):
            if isinstance(result, dict):
                title = result.get('title', '')
                url = result.get('url', '')
                
                # full_contentê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ snippet ì‚¬ìš©
                content = result.get('full_content', '')
                if not content:
                    content = result.get('snippet', '')
                
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                published_date = result.get('published_date', '')
                date_str = ""
                if published_date:
                    try:
                        from datetime import datetime
                        date_obj = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        date_str = f" (ë°œí–‰ì¼: {date_obj.strftime('%Y-%m-%d')})"
                    except:
                        date_str = f" (ë°œí–‰ì¼: {published_date[:10]})"
                
                # ê²€í†  ì •ë³´ ì¶”ê°€
                review = result.get('review', {})
                review_str = ""
                if review:
                    relevance = review.get('relevance_score', 'N/A')
                    recency = review.get('recency', 'N/A')
                    reliability = review.get('reliability', 'N/A')
                    review_str = f" [ê´€ë ¨ì„±: {relevance}/10, ìµœì‹ ì„±: {recency}, ì‹ ë¢°ë„: {reliability}]"
                
                verified_text += f"\n--- ì¶œì²˜ {i}: {title}{date_str}{review_str} ---\n"
                verified_text += f"URL: {url}\n"
                verified_text += f"ë‚´ìš©:\n{content[:10000] if len(content) > 10000 else content}\n"  # ìµœëŒ€ 10000ì
            else:
                verified_text += f"\n--- ì¶œì²˜ {i} ---\n{str(result)}\n"
        
        # í˜„ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
        from datetime import datetime
        current_time = datetime.now()
        current_date_str = current_time.strftime('%Yë…„ %mì›” %dì¼')
        current_datetime_str = current_time.strftime('%Y-%m-%d %H:%M:%S')
        
        # LLMìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ìƒì„±
        from src.core.llm_manager import execute_llm_task, TaskType
        
        # ì‚¬ìš©ì ìš”ì²­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬ - LLMì´ í˜•ì‹ì„ ê²°ì •í•˜ë„ë¡
        generation_prompt = f"""ì‚¬ìš©ì ìš”ì²­: {state['user_query']}

ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼ (ì‹¤ì œ ì›¹ í˜ì´ì§€ ì „ì²´ ë‚´ìš© í¬í•¨):
{verified_text}

âš ï¸ ì¤‘ìš” ì§€ì¹¨:
1. **ìµœì‹  ì •ë³´ ìš°ì„ **: ë‚ ì§œê°€ í‘œì‹œëœ ì¶œì²˜ ì¤‘ ê°€ì¥ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
2. **ì „ì²´ ë‚´ìš© í™œìš©**: ê° ì¶œì²˜ì˜ ì „ì²´ ë‚´ìš©(full_content)ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
3. **ë‹¤ì–‘í•œ ì¶œì²˜ ì¢…í•©**: ì—¬ëŸ¬ ì¶œì²˜ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê· í˜• ì¡íŒ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.
4. **í˜„ì¬ ì‹œê°„ ê¸°ì¤€**: ë³´ê³ ì„œ ì‘ì„±ì¼ì€ {current_date_str} ({current_datetime_str})ë¡œ ì„¤ì •í•˜ì„¸ìš”.
5. **ìµœì‹  ë™í–¥ ë°˜ì˜**: ìµœì‹  ë‰´ìŠ¤ë‚˜ ë™í–¥ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³ , ìš”ì²­í•œ í˜•ì‹ì— ë§ê²Œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ë³´ê³ ì„œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ (ì‘ì„±ì¼: {current_date_str} í¬í•¨)
- ì½”ë“œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ
- ë¬¸ì„œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ

ìš”ì²­ëœ í˜•ì‹ì— ë§ê²Œ ì™„ì „í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        try:
            report_result = await execute_llm_task(
                prompt=generation_prompt,
                task_type=TaskType.GENERATION,
                model_name=None,
                system_message=None
            )
            
            report = report_result.content or f"# Report: {state['user_query']}\n\nNo report generated."
            
            # Safety filter ì°¨ë‹¨ í™•ì¸ - Fallback ì œê±°, ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
            if "blocked by safety" in report.lower() or "content blocked" in report.lower() or len(report) < 100:
                error_msg = "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: Safety filterì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
                logger.error(f"[{self.name}] âŒ {error_msg}")
                state['final_report'] = None
                state['report_failed'] = True
                state['error'] = error_msg
                state['current_agent'] = self.name
                return state
            else:
                logger.info(f"[{self.name}] âœ… Report generated: {len(report)} characters")
            
            # Council í™œì„±í™” í™•ì¸ ë° ì ìš© (ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹œ - ê¸°ë³¸ í™œì„±í™”)
            use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
            if use_council is None:
                # ìë™ í™œì„±í™” íŒë‹¨ (ê¸°ë³¸ í™œì„±í™”)
                from src.core.council_activator import get_council_activator
                activator = get_council_activator()
                
                activation_decision = activator.should_activate(
                    process_type='synthesis',
                    query=state['user_query'],
                    context={'important_conclusion': True}  # ìµœì¢… ë³´ê³ ì„œëŠ” í•­ìƒ ì¤‘ìš”í•œ ê²°ë¡ 
                )
                use_council = activation_decision.should_activate
                if use_council:
                    logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
            
            # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
            if use_council:
                try:
                    from src.core.llm_council import run_full_council
                    logger.info(f"[{self.name}] ğŸ›ï¸ Running Council review for final report...")
                    
                    # ë³´ê³ ì„œ ìƒ˜í”Œ (ìµœëŒ€ 2000ì)
                    report_sample = report[:2000]
                    
                    council_query = f"""Review the final report and assess its completeness and accuracy. Check for any missing information or potential improvements.

Research Query: {state['user_query']}

Final Report Sample:
{report_sample}

Provide a review with:
1. Completeness assessment
2. Accuracy check
3. Recommendations for improvement"""
                    
                    stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                        council_query
                    )
                    
                    # Council ê²€í†  ê²°ê³¼
                    review_report = stage3_result.get('response', '')
                    logger.info(f"[{self.name}] âœ… Council review completed.")
                    logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                    
                    # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                    if 'council_metadata' not in state:
                        state['council_metadata'] = {}
                    state['council_metadata']['synthesis'] = {
                        'stage1_results': stage1_results,
                        'stage2_results': stage2_results,
                        'stage3_result': stage3_result,
                        'metadata': metadata,
                        'review_report': review_report
                    }
                    
                    # Council ê²€í†  ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì¶”ê°€ (ì„ íƒì )
                    if review_report:
                        report += f"\n\n--- Council Review ---\n{review_report}"
                except Exception as e:
                    logger.warning(f"[{self.name}] Council review failed: {e}. Using original report.")
                    # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³´ê³ ì„œ ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Report generation failed: {e}")
            # Fallback ì œê±° - ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
            error_msg = f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state['final_report'] = None
            state['report_failed'] = True
            state['error'] = error_msg
            state['current_agent'] = self.name
            return state
        
        state['final_report'] = report
        state['current_agent'] = self.name
        state['report_failed'] = False
        
        # Write to shared memory
        memory.write(
            key=f"report_{state['session_id']}",
            value=report,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] âœ… Report saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


###################
# Orchestrator
###################

class AgentOrchestrator:
    """Orchestrator for multi-agent workflow."""
    
    def __init__(self, config: Any = None):
        """Initialize orchestrator."""
        self.config = config
        self.shared_memory = get_shared_memory()
        self.skill_manager = get_skill_manager()
        self.agent_config = get_agent_config()
        self.graph = None
        # GraphëŠ” ì²« ì‹¤í–‰ ì‹œ ì¿¼ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë¹Œë“œ

        # SharedResultsManagerì™€ AgentDiscussionManagerëŠ” execute ì‹œì ì— ì´ˆê¸°í™”
        # (objective_idê°€ í•„ìš”í•˜ë¯€ë¡œ)
        self.shared_results_manager: Optional[SharedResultsManager] = None
        self.discussion_manager: Optional[AgentDiscussionManager] = None

        # MCP ë„êµ¬ ìë™ ë°œê²¬ ë° ì„ íƒ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.mcp_servers = self._initialize_mcp_servers()
        self.tool_loader = MCPToolLoader(FastMCPMulti(self.mcp_servers))
        self.tool_selector = AgentToolSelector()

        logger.info("AgentOrchestrator initialized with MCP tool auto-discovery")

    def _initialize_mcp_servers(self) -> dict[str, Any]:
        """í™˜ê²½ ë³€ìˆ˜ ë° êµ¬ì„±ì—ì„œ MCP ì„œë²„ ì„¤ì •ì„ ì´ˆê¸°í™”.
        
        Returns:
            mcp_config.json ì›ë³¸ í˜•ì‹ì˜ dict (FastMCPê°€ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹)
        """
        servers: dict[str, Any] = {}
        
        try:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
            current_file = Path(__file__)
            project_root = current_file.parent.parent.parent
            
            # configs í´ë”ì—ì„œ ë¡œë“œ ì‹œë„ (ìš°ì„ )
            config_file = project_root / "configs" / "mcp_config.json"
            if not config_file.exists():
                # í•˜ìœ„ í˜¸í™˜ì„±: ë£¨íŠ¸ì—ì„œë„ ì‹œë„
                config_file = project_root / "mcp_config.json"
            
            if config_file.exists():
                with open(config_file, 'r') as f:
                    config_data = json.load(f)
                    raw_configs = config_data.get("mcpServers", {})
                    
                    # í™˜ê²½ë³€ìˆ˜ ì¹˜í™˜
                    resolved_configs = self._resolve_env_vars_in_value(raw_configs)
                    
                    # FastMCPê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì •ë¦¬
                    # - stdio ì„œë²„: command, args, env, cwdë§Œ ìœ ì§€
                    # - HTTP ì„œë²„: type í•„ë“œ ì œê±°, httpUrl ë˜ëŠ” urlë§Œ ìœ ì§€
                    for server_name, server_config in resolved_configs.items():
                        cleaned_config = {}
                        
                        # stdio ì„œë²„ì¸ ê²½ìš°
                        if "command" in server_config:
                            cleaned_config["command"] = server_config["command"]
                            if "args" in server_config:
                                cleaned_config["args"] = server_config["args"]
                            if "env" in server_config and server_config["env"]:
                                cleaned_config["env"] = server_config["env"]
                            if "cwd" in server_config and server_config["cwd"]:
                                cleaned_config["cwd"] = server_config["cwd"]
                        # HTTP ì„œë²„ì¸ ê²½ìš°
                        elif "httpUrl" in server_config or "url" in server_config:
                            # FastMCPëŠ” url í•„ë“œë¥¼ ê¸°ëŒ€í•¨ (httpUrlì„ urlë¡œ ë³€í™˜)
                            if "httpUrl" in server_config:
                                cleaned_config["url"] = server_config["httpUrl"]
                            elif "url" in server_config:
                                cleaned_config["url"] = server_config["url"]
                            if "headers" in server_config and server_config["headers"]:
                                cleaned_config["headers"] = server_config["headers"]
                            if "params" in server_config and server_config["params"]:
                                cleaned_config["params"] = server_config["params"]
                        
                        if cleaned_config:
                            servers[server_name] = cleaned_config
                    
                    logger.info(f"âœ… Loaded {len(servers)} MCP servers from config: {list(servers.keys())}")
            else:
                logger.warning(f"MCP config file not found at {config_file}")
                
        except Exception as e:
            logger.warning(f"Failed to load MCP server configs: {e}")

        logger.info(f"Initialized {len(servers)} MCP servers for auto-discovery")
        return servers
    
    def _resolve_env_vars_in_value(self, value: Any) -> Any:
        """
        ì¬ê·€ì ìœ¼ë¡œ ê°ì²´ ë‚´ì˜ í™˜ê²½ë³€ìˆ˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜.
        ${VAR_NAME} ë˜ëŠ” $VAR_NAME í˜•ì‹ ì§€ì›.
        """
        if isinstance(value, str):
            # ${VAR_NAME} ë˜ëŠ” $VAR_NAME íŒ¨í„´ ì°¾ê¸°
            pattern = r'\$\{([^}]+)\}|\$(\w+)'
            
            def replace_env_var(match):
                var_name = match.group(1) or match.group(2)
                env_value = os.getenv(var_name)
                if env_value is not None:
                    return env_value
                # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€ (ë˜ëŠ” ê²½ê³ )
                logger.warning(f"Environment variable '{var_name}' not found, keeping placeholder")
                return match.group(0)
            
            result = re.sub(pattern, replace_env_var, value)
            return result
        elif isinstance(value, dict):
            return {k: self._resolve_env_vars_in_value(v) for k, v in value.items()}
        elif isinstance(value, list):
            return [self._resolve_env_vars_in_value(item) for item in value]
        else:
            return value

    async def _assign_tools_to_agents(self, session_id: str) -> None:
        """ëª¨ë“  ì—ì´ì „íŠ¸ì— ìë™ìœ¼ë¡œ MCP ë„êµ¬ í• ë‹¹."""
        try:
            # MCP ë„êµ¬ ìë™ ë°œê²¬
            discovered_tools = await self.tool_loader.get_all_tools()
            tool_infos = await self.tool_loader.list_tool_info()

            logger.info(f"Discovered {len(discovered_tools)} MCP tools from {len(self.mcp_servers)} servers")

            # ê° ì—ì´ì „íŠ¸ë³„ ë„êµ¬ ì„ íƒ ë° í• ë‹¹
            assignments = self.tool_selector.select_tools_for_all_agents(
                discovered_tools, tool_infos
            )

            # ê° ì—ì´ì „íŠ¸ì— ë„êµ¬ í• ë‹¹
            for agent_type, assignment in assignments.items():
                agent = getattr(self, agent_type.value, None)
                if agent:
                    agent.available_tools = assignment.tools
                    agent.tool_infos = assignment.tool_infos
                    logger.info(f"Assigned {len(assignment.tools)} tools to {agent_type.value} agent")

                    # ë„êµ¬ í• ë‹¹ ìš”ì•½ ë¡œê¹…
                    summary = self.tool_selector.get_agent_tool_summary(assignment)
                    logger.info(f"Tool assignment summary for {agent_type.value}: {summary}")

        except Exception as e:
            logger.warning(f"Failed to assign MCP tools to agents: {e}")
            # ë„êµ¬ í• ë‹¹ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    def _build_graph(self, user_query: Optional[str] = None, session_id: Optional[str] = None) -> None:
        """Build LangGraph workflow with Skills auto-selection."""
        
        # Create context for all agents
        context = AgentContext(
            agent_id="orchestrator",
            session_id=session_id or "default",
            shared_memory=self.shared_memory,
            config=self.config,
            shared_results_manager=self.shared_results_manager,
            discussion_manager=self.discussion_manager
        )
        
        # Skills ìë™ ì„ íƒ (ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´)
        selected_skills = {}
        if user_query:
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(user_query)
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # Initialize agents with Skills
        self.planner = PlannerAgent(context, selected_skills.get("research_planner"))
        self.executor = ExecutorAgent(context, selected_skills.get("research_executor"))
        self.verifier = VerifierAgent(context, selected_skills.get("evaluator"))
        self.generator = GeneratorAgent(context, selected_skills.get("synthesizer"))

        # ê° ì—ì´ì „íŠ¸ì— MCP ë„êµ¬ ìë™ í• ë‹¹ (ë¹„ë™ê¸°)
        if session_id:
            asyncio.create_task(self._assign_tools_to_agents(session_id))

        # Build graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("executor", self._executor_node)  # Legacy
        workflow.add_node("parallel_executor", self._parallel_executor_node)  # New parallel executor
        workflow.add_node("verifier", self._verifier_node)  # Legacy
        workflow.add_node("parallel_verifier", self._parallel_verifier_node)  # New parallel verifier
        workflow.add_node("generator", self._generator_node)
        workflow.add_node("end", self._end_node)
        
        # Define edges - ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ ì‚¬ìš©
        workflow.set_entry_point("planner")
        workflow.add_edge("planner", "parallel_executor")  # ë³‘ë ¬ ì‹¤í–‰ ì‚¬ìš©
        workflow.add_edge("parallel_executor", "parallel_verifier")  # ë³‘ë ¬ ê²€ì¦ ì‚¬ìš©
        workflow.add_edge("parallel_verifier", "generator")
        workflow.add_edge("generator", "end")
        
        # Compile graph
        self.graph = workflow.compile()
        
        logger.info("LangGraph workflow built")
    
    async def _planner_node(self, state: AgentState) -> AgentState:
        """Planner node execution with tracking."""
        logger.info("=" * 80)
        logger.info("ğŸ”µ [WORKFLOW] â†’ Planner Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.PLANNING, {"message": "ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        result = await self.planner.execute(state)
        logger.info(f"ğŸ”µ [WORKFLOW] âœ“ Planner completed: {result.get('current_agent')}")
        return result
    
    async def _executor_node(self, state: AgentState) -> AgentState:
        """Executor node execution with tracking (legacy - for backward compatibility)."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¢ [WORKFLOW] â†’ Executor Node (legacy)")
        logger.info("=" * 80)
        result = await self.executor.execute(state)
        logger.info(f"ğŸŸ¢ [WORKFLOW] âœ“ Executor completed: {len(result.get('research_results', []))} results")
        return result
    
    async def _parallel_executor_node(self, state: AgentState) -> AgentState:
        """Parallel executor node - runs multiple ExecutorAgent instances simultaneously."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¢ [WORKFLOW] â†’ Parallel Executor Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.EXECUTING, {"message": "ì—°êµ¬ ì‹¤í–‰ ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        # ì‘ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        tasks = state.get('research_tasks', [])
        if not tasks:
            # ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
            memory = self.shared_memory
            tasks = memory.read(
                key=f"tasks_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        if not tasks:
            logger.warning("[WORKFLOW] No tasks found, falling back to single executor")
            return await self._executor_node(state)
        
        logger.info(f"[WORKFLOW] Executing {len(tasks)} tasks in parallel with {len(tasks)} ExecutorAgent instances")
        
        # ë™ì  ë™ì‹œì„± ê´€ë¦¬ í†µí•©
        from src.core.concurrency_manager import get_concurrency_manager
        concurrency_manager = get_concurrency_manager()
        max_concurrent = concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        max_concurrent = min(max_concurrent, len(tasks))  # ì‘ì—… ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡
        
        logger.info(f"[WORKFLOW] Using concurrency limit: {max_concurrent} (from concurrency_manager)")
        
        # Skills ìë™ ì„ íƒ
        selected_skills = {}
        if state.get('user_query'):
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(state['user_query'])
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # ì—¬ëŸ¬ ExecutorAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def execute_single_task(task: Dict[str, Any], task_index: int) -> AgentState:
            """ë‹¨ì¼ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ExecutorAgent."""
            agent_id = f"executor_{task_index}"
            context = AgentContext(
                agent_id=agent_id,
                session_id=state['session_id'],
                shared_memory=self.shared_memory,
                config=self.config,
                shared_results_manager=self.shared_results_manager,
                discussion_manager=self.discussion_manager
            )
            
            executor_agent = ExecutorAgent(context, selected_skills.get("research_executor"))
            
            try:
                logger.info(f"[WORKFLOW] ExecutorAgent {agent_id} starting task {task.get('task_id', 'unknown')}")
                result_state = await executor_agent.execute(state, assigned_task=task)
                logger.info(f"[WORKFLOW] ExecutorAgent {agent_id} completed: {len(result_state.get('research_results', []))} results")
                return result_state
            except Exception as e:
                logger.error(f"[WORKFLOW] ExecutorAgent {agent_id} failed: {e}")
                # ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸ì˜ ìƒíƒœ ë°˜í™˜
                failed_state = state.copy()
                failed_state['research_results'] = []
                failed_state['research_failed'] = True
                failed_state['error'] = f"Task {task.get('task_id', 'unknown')} failed: {str(e)}"
                failed_state['current_agent'] = agent_id
                return failed_state
        
        # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ë™ì  ë™ì‹œì„± ì œí•œ ì ìš©)
        if max_concurrent < len(tasks):
            # Semaphoreë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def execute_with_limit(task: Dict[str, Any], task_index: int) -> AgentState:
                async with semaphore:
                    return await execute_single_task(task, task_index)
            
            executor_tasks = [execute_with_limit(task, i) for i, task in enumerate(tasks)]
        else:
            # ë™ì‹œì„± ì œí•œì´ ì‘ì—… ìˆ˜ë³´ë‹¤ í¬ë©´ ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
            executor_tasks = [execute_single_task(task, i) for i, task in enumerate(tasks)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        executor_results = await asyncio.gather(*executor_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° í†µì‹  ìƒíƒœ í™•ì¸
        all_results = []
        all_failed = False
        errors = []
        communication_stats = {
            'agents_contributed': 0,
            'results_shared': 0,
            'communication_errors': 0
        }

        for i, result in enumerate(executor_results):
            if isinstance(result, Exception):
                logger.error(f"[WORKFLOW] ExecutorAgent {i} raised exception: {result}")
                all_failed = True
                errors.append(f"Task {tasks[i].get('task_id', 'unknown')}: {str(result)}")
                communication_stats['communication_errors'] += 1
            elif isinstance(result, dict):
                # ê²°ê³¼ ìˆ˜ì§‘
                task_results = result.get('research_results', [])
                if task_results:
                    all_results.extend(task_results)
                    communication_stats['agents_contributed'] += 1
                    logger.info(f"[WORKFLOW] ExecutorAgent {i} contributed {len(task_results)} results")

                # SharedResultsManager í†µì‹  ìƒíƒœ í™•ì¸
                if self.shared_results_manager:
                    agent_id = f"executor_{i}"
                    agent_results = await self.shared_results_manager.get_shared_results(agent_id=agent_id)
                    if agent_results:
                        communication_stats['results_shared'] += len(agent_results)
                        logger.info(f"[WORKFLOW] ğŸ¤ ExecutorAgent {agent_id} shared {len(agent_results)} results via SharedResultsManager")

                # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸
                if result.get('research_failed'):
                    all_failed = True
                    if result.get('error'):
                        errors.append(result['error'])
                        communication_stats['communication_errors'] += 1
        
        # í†µí•©ëœ ìƒíƒœ ìƒì„±
        final_state = state.copy()
        final_state['research_results'] = all_results
        final_state['research_failed'] = all_failed
        final_state['current_agent'] = "parallel_executor"
        
        if errors:
            final_state['error'] = "; ".join(errors)
        
        logger.info(f"[WORKFLOW] âœ… Parallel execution completed: {len(all_results)} total results from {len(tasks)} tasks")
        logger.info(f"[WORKFLOW] ğŸ¤ Agent communication summary: {communication_stats['agents_contributed']} agents contributed, {communication_stats['results_shared']} results shared")
        if communication_stats['communication_errors'] > 0:
            logger.warning(f"[WORKFLOW] âš ï¸ Communication errors: {communication_stats['communication_errors']}")
        logger.info(f"[WORKFLOW] Failed: {all_failed}")
        
        return final_state
    
    async def _verifier_node(self, state: AgentState) -> AgentState:
        """Verifier node execution with tracking (legacy - for backward compatibility)."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¡ [WORKFLOW] â†’ Verifier Node (legacy)")
        logger.info("=" * 80)
        result = await self.verifier.execute(state)
        logger.info(f"ğŸŸ¡ [WORKFLOW] âœ“ Verifier completed: {len(result.get('verified_results', []))} verified")
        return result
    
    async def _parallel_verifier_node(self, state: AgentState) -> AgentState:
        """Parallel verifier node - runs multiple VerifierAgent instances simultaneously."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¡ [WORKFLOW] â†’ Parallel Verifier Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.VERIFYING, {"message": "ê²°ê³¼ ê²€ì¦ ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        # ì—°êµ¬ ì‹¤íŒ¨ í™•ì¸
        if state.get('research_failed'):
            logger.error("[WORKFLOW] Research execution failed, skipping verification")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['current_agent'] = "parallel_verifier"
            return state
        
        # ê²€ì¦í•  ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        results = state.get('research_results', [])
        if not results:
            memory = self.shared_memory
            results = memory.read(
                key=f"research_results_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        if not results:
            logger.warning("[WORKFLOW] No results to verify, falling back to single verifier")
            return await self._verifier_node(state)
        
        # ê²°ê³¼ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ VerifierAgentì— í• ë‹¹
        num_verifiers = min(len(results), self.agent_config.max_concurrent_research_units or 3)
        chunk_size = max(1, len(results) // num_verifiers)
        result_chunks = [results[i:i + chunk_size] for i in range(0, len(results), chunk_size)]
        
        logger.info(f"[WORKFLOW] Verifying {len(results)} results with {len(result_chunks)} VerifierAgent instances")
        
        # ë™ì  ë™ì‹œì„± ê´€ë¦¬ í†µí•©
        from src.core.concurrency_manager import get_concurrency_manager
        concurrency_manager = get_concurrency_manager()
        max_concurrent = concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        max_concurrent = min(max_concurrent, len(result_chunks))
        
        logger.info(f"[WORKFLOW] Using concurrency limit: {max_concurrent} (from concurrency_manager)")
        
        # Skills ìë™ ì„ íƒ
        selected_skills = {}
        if state.get('user_query'):
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(state['user_query'])
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # ì—¬ëŸ¬ VerifierAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def verify_single_chunk(chunk: List[Dict[str, Any]], chunk_index: int) -> List[Dict[str, Any]]:
            """ë‹¨ì¼ ì²­í¬ë¥¼ ê²€ì¦í•˜ëŠ” VerifierAgent."""
            agent_id = f"verifier_{chunk_index}"
            logger.info(f"[WORKFLOW] ğŸ’¬ Creating VerifierAgent {agent_id} for {len(chunk)} results")
            context = AgentContext(
                agent_id=agent_id,
                session_id=state['session_id'],
                shared_memory=self.shared_memory,
                config=self.config,
                shared_results_manager=self.shared_results_manager,
                discussion_manager=self.discussion_manager
            )
            
            verifier_agent = VerifierAgent(context, selected_skills.get("evaluator"))
            
            # ì²­í¬ë§Œ í¬í•¨í•˜ëŠ” ì„ì‹œ state ìƒì„±
            chunk_state = state.copy()
            chunk_state['research_results'] = chunk
            
            try:
                logger.info(f"[WORKFLOW] VerifierAgent {agent_id} starting verification of {len(chunk)} results")
                result_state = await verifier_agent.execute(chunk_state)
                verified_chunk = result_state.get('verified_results', [])
                logger.info(f"[WORKFLOW] VerifierAgent {agent_id} completed: {len(verified_chunk)} verified")
                return verified_chunk
            except Exception as e:
                logger.error(f"[WORKFLOW] VerifierAgent {agent_id} failed: {e}")
                return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        # ëª¨ë“  ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ê²€ì¦ (ë™ì  ë™ì‹œì„± ì œí•œ ì ìš©)
        if max_concurrent < len(result_chunks):
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def verify_with_limit(chunk: List[Dict[str, Any]], chunk_index: int) -> List[Dict[str, Any]]:
                async with semaphore:
                    return await verify_single_chunk(chunk, chunk_index)
            
            verifier_tasks = [verify_with_limit(chunk, i) for i, chunk in enumerate(result_chunks)]
        else:
            verifier_tasks = [verify_single_chunk(chunk, i) for i, chunk in enumerate(result_chunks)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        verifier_results = await asyncio.gather(*verifier_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° í†µì‹  ìƒíƒœ í™•ì¸
        all_verified = []
        communication_stats = {
            'verifiers_contributed': 0,
            'verification_results_shared': 0,
            'discussion_participants': 0
        }

        for i, result in enumerate(verifier_results):
            if isinstance(result, Exception):
                logger.error(f"[WORKFLOW] VerifierAgent {i} raised exception: {result}")
            elif isinstance(result, list):
                all_verified.extend(result)
                communication_stats['verifiers_contributed'] += 1
                logger.info(f"[WORKFLOW] VerifierAgent {i} contributed {len(result)} verified results")

                # SharedResultsManager í†µì‹  ìƒíƒœ í™•ì¸
                if self.shared_results_manager:
                    agent_id = f"verifier_{i}"
                    agent_results = await self.shared_results_manager.get_shared_results(agent_id=agent_id)
                    verification_shared = [r for r in agent_results if isinstance(r.result, dict) and r.result.get('status') == 'verified']
                    if verification_shared:
                        communication_stats['verification_results_shared'] += len(verification_shared)
                        logger.info(f"[WORKFLOW] ğŸ¤ VerifierAgent {agent_id} shared {len(verification_shared)} verification results")

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_verified = []
        for verified_result in all_verified:
            if isinstance(verified_result, dict):
                url = verified_result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_verified.append(verified_result)
                elif not url:
                    unique_verified.append(verified_result)

        logger.info(f"[WORKFLOW] ğŸ“Š Verification deduplication: {len(all_verified)} â†’ {len(unique_verified)} unique results")

        # ì—¬ëŸ¬ VerifierAgent ê°„ í† ë¡  (ê²€ì¦ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš°)
        if self.discussion_manager and len(unique_verified) > 0:
            # ë‹¤ë¥¸ VerifierAgentì˜ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            if self.shared_results_manager:
                other_verified = await self.shared_results_manager.get_shared_results()
                other_verified_results = [r for r in other_verified if isinstance(r.result, dict) and r.result.get('status') == 'verified']

                if other_verified_results:
                    communication_stats['discussion_participants'] = len(set(r.agent_id for r in other_verified_results))
                    logger.info(f"[WORKFLOW] ğŸ’¬ Starting inter-verifier discussion with {len(other_verified_results)} results from {communication_stats['discussion_participants']} agents")

                    # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ 
                    first_verified = unique_verified[0]
                    result_id = f"verification_{first_verified.get('index', 0)}"
                    discussion = await self.discussion_manager.agent_discuss_result(
                        result_id=result_id,
                        agent_id="parallel_verifier",
                        other_agent_results=other_verified_results[:3]
                    )
                    if discussion:
                        logger.info(f"[WORKFLOW] ğŸ’¬ Inter-verifier discussion completed: {discussion[:150]}...")
                        logger.info(f"[WORKFLOW] ğŸ¤ Agent discussion: {communication_stats['discussion_participants']} verifiers participated in result validation")
                    else:
                        logger.info(f"[WORKFLOW] ğŸ’¬ No discussion generated between verifiers")
                else:
                    logger.info(f"[WORKFLOW] ğŸ’¬ No other verified results available for inter-verifier discussion")
        
        # í†µí•©ëœ ìƒíƒœ ìƒì„±
        final_state = state.copy()
        final_state['verified_results'] = unique_verified
        final_state['verification_failed'] = False if unique_verified else True
        final_state['current_agent'] = "parallel_verifier"
        
        logger.info(f"[WORKFLOW] âœ… Parallel verification completed: {len(unique_verified)} total verified results from {len(result_chunks)} verifiers")
        logger.info(f"[WORKFLOW] ğŸ¤ Agent communication summary: {communication_stats['verifiers_contributed']} verifiers contributed, {communication_stats['verification_results_shared']} verification results shared")
        if communication_stats['discussion_participants'] > 0:
            logger.info(f"[WORKFLOW] ğŸ’¬ Inter-verifier discussion: {communication_stats['discussion_participants']} agents participated")
        
        return final_state
    
    async def _generator_node(self, state: AgentState) -> AgentState:
        """Generator node execution with tracking."""
        logger.info("=" * 80)
        logger.info("ğŸŸ£ [WORKFLOW] â†’ Generator Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.GENERATING, {"message": "ë³´ê³ ì„œ ìƒì„± ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        result = await self.generator.execute(state)
        final_report = result.get('final_report') or ''
        report_length = len(final_report) if final_report else 0
        logger.info(f"ğŸŸ£ [WORKFLOW] âœ“ Generator completed: report_length={report_length}")
        return result
    
    async def _end_node(self, state: AgentState) -> AgentState:
        """End node - final state with summary."""
        logger.info("=" * 80)
        logger.info("âœ… [WORKFLOW] â†’ End Node - Workflow Completed")
        logger.info("=" * 80)
        logger.info(f"Session: {state.get('session_id')}")
        logger.info(f"Final Agent: {state.get('current_agent')}")
        logger.info(f"Research Results: {len(state.get('research_results', []))}")
        logger.info(f"Verified Results: {len(state.get('verified_results', []))}")
        logger.info(f"Report Generated: {bool(state.get('final_report'))}")
        logger.info(f"Failed: {state.get('research_failed') or state.get('verification_failed') or state.get('report_failed')}")
        logger.info("=" * 80)
        return state
    
    async def execute(self, user_query: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Execute multi-agent workflow with Skills auto-selection.
        
        Args:
            user_query: User's research query
            session_id: Session ID
            
        Returns:
            Final result from the workflow
        """
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"Starting workflow for query: {user_query}")
        
        # Objective ID ìƒì„± (ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ê³µìœ ìš©)
        objective_id = f"objective_{session_id}"
        
        # SharedResultsManagerì™€ AgentDiscussionManager ì´ˆê¸°í™” (ë³‘ë ¬ ì‹¤í–‰ í™œì„±í™” ì‹œ)
        if self.agent_config.enable_agent_communication:
            self.shared_results_manager = SharedResultsManager(objective_id=objective_id)
            self.discussion_manager = AgentDiscussionManager(
                objective_id=objective_id,
                shared_results_manager=self.shared_results_manager
            )
            logger.info("âœ… Agent result sharing and discussion enabled")
            logger.info(f"ğŸ¤ SharedResultsManager initialized for objective: {objective_id}")
            logger.info(f"ğŸ’¬ AgentDiscussionManager initialized with agent communication support")
        else:
            self.shared_results_manager = None
            self.discussion_manager = None
            logger.info("Agent communication disabled")
        
        # Graphê°€ ì—†ê±°ë‚˜ ì¿¼ë¦¬ ê¸°ë°˜ ì¬ë¹Œë“œê°€ í•„ìš”í•œ ê²½ìš° ë¹Œë“œ
        if self.graph is None:
            self._build_graph(user_query, session_id)
        
        # Initialize state
        initial_state = AgentState(
            messages=[],
            user_query=user_query,
            research_plan=None,
            research_tasks=[],
            research_results=[],
            verified_results=[],
            final_report=None,
            current_agent=None,
            iteration=0,
            session_id=session_id,
            research_failed=False,
            verification_failed=False,
            report_failed=False,
            error=None
        )
        
        # Execute workflow
        try:
            result = await self.graph.ainvoke(initial_state)
            logger.info("Workflow execution completed successfully")
            return result
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            raise
    
    async def stream(self, user_query: str, session_id: Optional[str] = None):
        """Stream workflow execution."""
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Initialize state
        initial_state = AgentState(
            messages=[],
            user_query=user_query,
            research_plan=None,
            research_tasks=[],
            research_results=[],
            verified_results=[],
            final_report=None,
            current_agent=None,
            iteration=0,
            session_id=session_id,
            research_failed=False,
            verification_failed=False,
            report_failed=False,
            error=None
        )
        
        # Stream execution
        async for event in self.graph.astream(initial_state):
            yield event


# Global orchestrator instance
_orchestrator: Optional[AgentOrchestrator] = None


def get_orchestrator(config: Any = None) -> AgentOrchestrator:
    """Get global orchestrator instance."""
    global _orchestrator
    
    if _orchestrator is None:
        _orchestrator = AgentOrchestrator(config=config)
    
    return _orchestrator


def init_orchestrator(config: Any = None) -> AgentOrchestrator:
    """Initialize orchestrator."""
    global _orchestrator
    
    _orchestrator = AgentOrchestrator(config=config)
    
    return _orchestrator

