"""
ğŸ—ï¸ AI Architect Agent Page

ì§„í™”í˜• AI ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° ìµœì í™”
"""

import streamlit as st
import asyncio
from mcp_agent.app import MCPApp
from mcp_agent.config import get_settings

from srcs.common.streamlit_utils import get_agent_state
from srcs.common.streamlit_log_handler import setup_streamlit_logging
from srcs.evolutionary_ai_architect.evolutionary_ai_architect_agent import EvolutionaryAIArchitectMCP
import json

app = MCPApp(
    name="ai_architect_app",
    settings=get_settings("configs/mcp_agent.config.yaml"),
)

def format_architect_output(result) -> str:
    """Format the rich result object from the agent into a markdown string."""
    if not result or not result.success:
        return "ì•„í‚¤í…ì²˜ ì„¤ê³„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    best_arch = result.best_architecture
    final_metrics = result.final_metrics

    output = [
        f"## ğŸ—ï¸ AI ì•„í‚¤í…ì²˜ ì§„í™” ì™„ë£Œ (ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ)",
        f"**ìµœì¢… ì„¸ëŒ€**: {result.generation_count}",
        "---",
        "### ğŸ† ìµœì  ì•„í‚¤í…ì²˜",
        f"- **ID**: `{best_arch.unique_id}`",
        f"- **ì í•©ë„ ì ìˆ˜**: {best_arch.fitness_score:.4f}",
        f"- **ë ˆì´ì–´ ìˆ˜**: {len(best_arch.layers)}",
        "#### ë ˆì´ì–´ êµ¬ì„±:",
    ]
    for i, layer in enumerate(best_arch.layers):
        output.append(f"  - Layer {i+1}: `{json.dumps(layer)}`")

    output.extend([
        "\n### ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­",
        f"- **ì •í™•ë„**: {final_metrics.accuracy:.4f}",
        f"- **í›ˆë ¨ ì‹œê°„**: {final_metrics.training_time:.2f}s",
        f"- **ì¶”ë¡  ì‹œê°„**: {final_metrics.inference_time:.3f}s",
        f"- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {final_metrics.memory_usage:.2f}MB",
        f"- **ì—ë„ˆì§€ íš¨ìœ¨**: {final_metrics.energy_efficiency:.3f}",
        "\n### ğŸ’¡ ìµœì í™” ì¶”ì²œ ì‚¬í•­",
    ])
    for recommendation in result.optimization_recommendations:
        output.append(f"- {recommendation}")

    return "\n".join(output)


async def main():
    await app.initialize()

    st.markdown("""
    <div style="
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        text-align: center;
        color: white;
        margin-bottom: 2rem;
    ">
        <h1>ğŸ—ï¸ AI Architect Agent</h1>
        <p style="font-size: 1.2rem; margin: 0;">
            ì§„í™”í˜• AI ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ
        </p>
    </div>
    """, unsafe_allow_html=True)

    # --- Real-time Log Display ---
    log_expander = st.expander("ì‹¤ì‹œê°„ ì‹¤í–‰ ë¡œê·¸", expanded=False)
    log_container = log_expander.empty()
    # Capture logs from the root mcp_agent logger and the agent's specific logger
    setup_streamlit_logging(["mcp_agent", "evolutionary_architect"], log_container)
    # --- End Log Display ---

    # Use the state management pattern
    # Note: EvolutionaryAIArchitectMCP is not a standard Agent subclass, so we handle it directly
    if 'architect_agent' not in st.session_state:
        st.session_state.architect_agent = EvolutionaryAIArchitectMCP(output_dir="ai_architect_reports")

    agent = st.session_state.architect_agent

    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ AI ì•„í‚¤í…ì²˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ë¬¸ì œ ìƒí™©ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
        ]

    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input("ì˜ˆ: 'ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê²½ëŸ‰ CNN ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ê³  ì‹¶ì–´'"):
        st.session_state["messages"].append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        with st.chat_message("assistant"):
            response_md = ""
            with st.spinner("ğŸ§¬ AI ì•„í‚¤í…ì²˜ë¥¼ ì§„í™”ì‹œí‚¤ëŠ” ì¤‘... (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                # Call the agent's main method
                # This agent is more complex and doesn't follow the simple run(prompt) pattern
                # We call its specific `evolve_architecture` method instead.
                result = await agent.evolve_architecture(
                    problem_description=prompt,
                    # We can add UI elements later to configure these if needed
                    max_generations=5,
                    population_size=10,
                )
                response_md = format_architect_output(result)

            st.markdown(response_md)

        st.session_state["messages"].append({"role": "assistant", "content": response_md})


if __name__ == "__main__":
    asyncio.run(main()) 