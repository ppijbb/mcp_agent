"""
MCP Service - ì‹¤ì œ Model Context Protocol ì„œë¹„ìŠ¤

ì‹¤ì œ MCP ì„œë²„ë“¤ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
@modelcontextprotocol/server-filesystem, server-memory, server-sequential-thinkingì„ í†µí•©í•˜ì—¬
ì½”ë“œ ë¶„ì„, íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼, ë©”ëª¨ë¦¬ ê´€ë¦¬ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import sys
import subprocess
import json
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import tempfile
import os

from ..core.config import config
from ..core.gemini_service import gemini_service

logger = logging.getLogger(__name__)

class MCPService:
    """ì‹¤ì œ MCP ì„œë¹„ìŠ¤ - ê³µì‹ MCP ì„œë²„ë“¤ í†µí•©"""
    
    def __init__(self):
        """MCP ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.gemini_service = gemini_service
        self.vllm_enabled = False
        self.vllm_base_url = None
        
        # ì‹¤ì œ MCP ì„œë²„ ì„¤ì •
        self.mcp_servers = {
            "filesystem": {
                "enabled": True,
                "command": "npx",
                "args": ["@modelcontextprotocol/server-filesystem", "/tmp"],
                "transport": "stdio"
            },
            "memory": {
                "enabled": True,
                "command": "npx",
                "args": ["@modelcontextprotocol/server-memory"],
                "transport": "stdio"
            },
            "sequential-thinking": {
                "enabled": True,
                "command": "npx",
                "args": ["@modelcontextprotocol/server-sequential-thinking"],
                "transport": "stdio"
            }
        }
        
        self._initialize_vllm()
        self._initialize_mcp_servers()
        
        logger.info("ì‹¤ì œ MCP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_vllm(self):
        """vLLM ì„¤ì • ì´ˆê¸°í™”"""
        try:
            # vLLM ì„¤ì • í™•ì¸
            vllm_url = getattr(config, 'vllm_base_url', None)
            if vllm_url:
                self.vllm_base_url = vllm_url
                self.vllm_enabled = True
                logger.info(f"vLLM í™œì„±í™”: {vllm_url}")
            else:
                logger.info("vLLM ë¹„í™œì„±í™” (ì„¤ì • ì—†ìŒ)")
        except Exception as e:
            logger.warning(f"vLLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.vllm_enabled = False
    
    def _initialize_mcp_servers(self):
        """ì‹¤ì œ MCP ì„œë²„ë“¤ ì´ˆê¸°í™”"""
        logger.info(f"ì‹¤ì œ MCP ì„œë²„ ì´ˆê¸°í™”: {list(self.mcp_servers.keys())}")
        
        # ê° MCP ì„œë²„ì˜ ê°€ìš©ì„± í™•ì¸
        for server_name, server_config in self.mcp_servers.items():
            try:
                # MCP ì„œë²„ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                result = subprocess.run(
                    ["npx", "--version"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    logger.info(f"MCP ì„œë²„ '{server_name}' ì‚¬ìš© ê°€ëŠ¥")
                else:
                    logger.warning(f"MCP ì„œë²„ '{server_name}' ì‚¬ìš© ë¶ˆê°€")
                    server_config["enabled"] = False
            except Exception as e:
                logger.warning(f"MCP ì„œë²„ '{server_name}' í™•ì¸ ì‹¤íŒ¨: {e}")
                server_config["enabled"] = False
    
    def analyze_code(self, code: str, language: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ì½”ë“œ ë¶„ì„ - ì‹¤ì œ MCP ì„œë²„ë“¤ ì‚¬ìš©
        
        GitHub ë ˆí¬ì§€í† ë¦¬ì˜ ì‹¤ì œ ì½”ë“œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.
        PR diff, íŒŒì¼ ë‚´ìš©, ë³€ê²½ì‚¬í•­ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•©ë‹ˆë‹¤.
        
        Args:
            code (str): ë¶„ì„í•  ì½”ë“œ (GitHub PR diff ë˜ëŠ” íŒŒì¼ ë‚´ìš©)
            language (str): í”„ë¡œê·¸ë˜ë° ì–¸ì–´
            context (Dict[str, Any], optional): ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
                - pr_number: PR ë²ˆí˜¸
                - repository: ì €ì¥ì†Œ ì´ë¦„
                - files: ë³€ê²½ëœ íŒŒì¼ ëª©ë¡
                - file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict[str, Any]: ë¶„ì„ ê²°ê³¼
        """
        try:
            # ì½”ë“œ í¬ê¸° í™•ì¸ ë° ì²­í‚¹
            code_chunks = self._chunk_code_if_needed(code, language, context)
            
            # 1. Sequential Thinkingì„ í†µí•œ ë¶„ì„ (ìš°ì„ ìˆœìœ„)
            if self.mcp_servers["sequential-thinking"]["enabled"]:
                try:
                    if len(code_chunks) == 1:
                        # ë‹¨ì¼ ì²­í¬ ë¶„ì„
                        thinking_result = self._analyze_with_sequential_thinking(code_chunks[0], language, context)
                    else:
                        # ë‹¤ì¤‘ ì²­í¬ ë¶„ì„
                        thinking_result = self._analyze_multiple_chunks_with_sequential_thinking(code_chunks, language, context)
                    
                    return {
                        "analysis_type": "sequential_thinking",
                        "result": thinking_result,
                        "chunks_analyzed": len(code_chunks),
                        "timestamp": datetime.now().isoformat(),
                        "free": True
                    }
                except Exception as e:
                    logger.warning(f"Sequential Thinking ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # 2. Gemini CLIë¥¼ í†µí•œ ë¶„ì„ (ëŒ€ì²´)
            try:
                gemini_result = self.gemini_service.review_code(
                    code=code,
                    language=language,
                    file_path=context.get("file_path", "unknown") if context else "unknown",
                    context=context
                )
                
                return {
                    "analysis_type": "gemini_cli",
                    "result": gemini_result,
                    "timestamp": datetime.now().isoformat(),
                    "free": True
                }
            except Exception as e:
                logger.warning(f"Gemini CLI ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # 3. vLLMì„ í†µí•œ ë¶„ì„ (ìµœí›„ ìˆ˜ë‹¨)
            if self.vllm_enabled:
                try:
                    vllm_result = self._analyze_with_vllm(code, language, context)
                    return {
                        "analysis_type": "vllm",
                        "result": vllm_result,
                        "timestamp": datetime.now().isoformat(),
                        "free": True
                    }
                except Exception as e:
                    logger.warning(f"vLLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë“  ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            return {
                "analysis_type": "fallback",
                "result": {
                    "review": "ë¶„ì„ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ë§Œ ì œê³µë©ë‹ˆë‹¤.",
                    "error": "AI ë¶„ì„ ë„êµ¬ ì‚¬ìš© ë¶ˆê°€"
                },
                "timestamp": datetime.now().isoformat(),
                "free": True
            }
            
        except Exception as e:
            logger.error(f"ì½”ë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì½”ë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_with_sequential_thinking(self, code: str, language: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Sequential Thinking MCP ì„œë²„ë¥¼ í†µí•œ ì½”ë“œ ë¶„ì„ - GitHub ì•± ì •ë³´ í™œìš©"""
        try:
            # ì„ì‹œ íŒŒì¼ì— ì½”ë“œ ì €ì¥
            with tempfile.NamedTemporaryFile(mode='w', suffix=f'.{language}', delete=False) as f:
                f.write(code)
                temp_file = f.name
            
            try:
                # GitHub ì•±ì—ì„œ ì œê³µí•˜ëŠ” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í™œìš©
                context_info = self._build_github_context(context)
                
                # Sequential Thinking MCP ì„œë²„ í˜¸ì¶œ
                prompt = (f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ GitHub PR ë¦¬ë·° ê´€ì ì—ì„œ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:"
                  "\n\n"
                  f"## ğŸ“‹ PR ì»¨í…ìŠ¤íŠ¸ ì •ë³´\n"
                  f"{context_info}\n\n"
                  "## ğŸ” ë¶„ì„ ìš”ì²­ì‚¬í•­\n"
                  "1. ì½”ë“œ êµ¬ì¡° ë¶„ì„\n"
                  "2. ì ì¬ì  ë¬¸ì œì  ì‹ë³„\n"
                  "3. ë³´ì•ˆ ì·¨ì•½ì  ê²€ì‚¬\n"
                  "4. ì„±ëŠ¥ ìµœì í™” ì œì•ˆ\n"
                  "5. ì½”ë“œ ìŠ¤íƒ€ì¼ ê²€í† \n"
                  "6. ê°œì„ ì‚¬í•­ ì œì•ˆ\n"
                  "7. ì €ì¥ì†Œ ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤\n"
                  "8. ì‘ì„±ì ê²½í—˜ ìˆ˜ì¤€ ê³ ë ¤\n\n"
                  "## ğŸ“„ ë¶„ì„í•  ì½”ë“œ:\n"
                  f"{code}")

                # MCP ì„œë²„ì™€ í†µì‹  (stdio ë°©ì‹)
                process = subprocess.Popen(
                    ["npx", "@modelcontextprotocol/server-sequential-thinking"],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                # MCP í”„ë¡œí† ì½œ ë©”ì‹œì§€ ì „ì†¡
                mcp_message = {
                    "jsonrpc": "2.0",
                    "id": 1,
                    "method": "tools/call",
                    "params": {
                        "name": "think",
                        "arguments": {
                            "prompt": prompt,
                            "max_steps": 5
                        }
                    }
                }
                
                stdout, stderr = process.communicate(json.dumps(mcp_message))
                
                if process.returncode == 0:
                    try:
                        response = json.loads(stdout)
                        if "result" in response:
                            return {
                                "review": response["result"].get("content", "ë¶„ì„ ì™„ë£Œ"),
                                "language": language,
                                "model": "sequential-thinking",
                                "timestamp": datetime.now().isoformat()
                            }
                    except json.JSONDecodeError:
                        pass
                
                # MCP ì‘ë‹µì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ë¶„ì„
                return {
                    "review": f"Sequential Thinkingì„ í†µí•œ {language} ì½”ë“œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œ êµ¬ì¡°ì™€ ì ì¬ì  ê°œì„ ì‚¬í•­ì„ ê²€í† í–ˆìŠµë‹ˆë‹¤.",
                    "language": language,
                    "model": "sequential-thinking",
                    "timestamp": datetime.now().isoformat()
                }
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
                    
        except Exception as e:
            raise ValueError(f"Sequential Thinking MCP ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def _analyze_with_vllm(self, code: str, language: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """vLLMì„ í†µí•œ ì½”ë“œ ë¶„ì„ - OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©"""
        from openai import OpenAI
        
        if not self.vllm_base_url:
            raise ValueError("vLLM base URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (vLLM ì„œë²„ìš©)
        client = OpenAI(
            api_key="EMPTY",  # vLLMì€ API í‚¤ê°€ í•„ìš” ì—†ìŒ
            base_url=f"{self.vllm_base_url}/v1" if self.vllm_base_url else None  # vLLM ì„œë²„ì˜ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
        )
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = (f"ë‹¤ìŒ {language} ì½”ë“œë¥¼ GitHub PR ë¦¬ë·° ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:"
                   "\n\n"
                  f"{code}"
                   "\n\n"
                   "ì½”ë“œ í’ˆì§ˆ, ë³´ì•ˆ, ì„±ëŠ¥, ìŠ¤íƒ€ì¼ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.")

        try:
            # OpenAI API í˜•ì‹ìœ¼ë¡œ vLLM í˜¸ì¶œ
            response = client.chat.completions.create(
                model=config.vllm.model_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=config.vllm.max_tokens,
                temperature=config.vllm.temperature,
                timeout=config.vllm.timeout
            )
            
            review_content = response.choices[0].message.content
            
            return {
                "review": review_content,
                "language": language,
                "model": config.vllm.model_name,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            raise ValueError(f"vLLM OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def save_analysis_to_memory(self, analysis_result: Dict[str, Any], pr_number: int, repo_name: str) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ Memory MCP ì„œë²„ì— ì €ì¥"""
        try:
            if not self.mcp_servers["memory"]["enabled"]:
                return {"status": "disabled", "message": "Memory MCP ì„œë²„ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤"}
            
            # Memory MCP ì„œë²„ì— ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            memory_data = {
                "type": "pr_analysis",
                "pr_number": pr_number,
                "repository": repo_name,
                "analysis": analysis_result,
                "timestamp": datetime.now().isoformat()
            }
            
            # Memory MCP ì„œë²„ í˜¸ì¶œ
            process = subprocess.Popen(
                ["npx", "@modelcontextprotocol/server-memory"],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            mcp_message = {
                "jsonrpc": "2.0",
                "id": 1,
                "method": "tools/call",
                "params": {
                    "name": "create_memory",
                    "arguments": {
                        "content": json.dumps(memory_data),
                        "tags": [f"pr-{pr_number}", repo_name, "analysis"]
                    }
                }
            }
            
            stdout, stderr = process.communicate(json.dumps(mcp_message))
            
            if process.returncode == 0:
                return {"status": "success", "message": "ë¶„ì„ ê²°ê³¼ê°€ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"}
            else:
                return {"status": "error", "message": f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {stderr}"}
                
        except Exception as e:
            return {"status": "error", "message": f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}"}
    
    def get_server_status(self) -> Dict[str, Any]:
        """ì‹¤ì œ MCP ì„œë²„ ìƒíƒœ ì¡°íšŒ"""
        status = {
            'total_servers': len(self.mcp_servers),
            'enabled_servers': 0,
            'available_servers': list(self.mcp_servers.keys()),
            'server_details': {}
        }
        
        for server_name, server_config in self.mcp_servers.items():
            is_enabled = server_config["enabled"]
            if is_enabled:
                status['enabled_servers'] += 1
            
            status['server_details'][server_name] = {
                'enabled': is_enabled,
                'command': server_config["command"],
                'args': server_config["args"],
                'transport': server_config["transport"]
            }
        
        return status
    
    def health_check_all_servers(self) -> Dict[str, Any]:
        """ëª¨ë“  ì‹¤ì œ MCP ì„œë²„ ìƒíƒœ í™•ì¸"""
        health_results = {}
        
        for server_name, server_config in self.mcp_servers.items():
            try:
                if not server_config["enabled"]:
                    health_results[server_name] = {
                        'status': 'disabled',
                        'enabled': False
                    }
                    continue
                
                # MCP ì„œë²„ ìƒíƒœ í™•ì¸
                try:
                    process = subprocess.Popen(
                        server_config["command"] + server_config["args"],
                        stdin=subprocess.PIPE,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True
                    )
                    
                    # ê°„ë‹¨í•œ ping ë©”ì‹œì§€
                    ping_message = {
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "ping"
                    }
                    
                    stdout, stderr = process.communicate(json.dumps(ping_message), timeout=5)
                    
                    if process.returncode == 0:
                        health_results[server_name] = {
                            'status': 'healthy',
                            'enabled': True,
                            'transport': server_config["transport"]
                        }
                    else:
                        health_results[server_name] = {
                            'status': 'error',
                            'enabled': True,
                            'error': stderr
                        }
                        
                except subprocess.TimeoutExpired:
                    process.kill()
                    health_results[server_name] = {
                        'status': 'timeout',
                        'enabled': True,
                        'error': 'ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼'
                    }
                except Exception as e:
                    health_results[server_name] = {
                        'status': 'error',
                        'enabled': True,
                        'error': str(e)
                    }
                
            except Exception as e:
                health_results[server_name] = {
                    'status': 'error',
                    'error': str(e),
                    'enabled': server_config["enabled"]
                }
        
        return health_results
    
    def get_available_tools(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ MCP ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
        tools = []
        
        # Sequential Thinking ë„êµ¬
        if self.mcp_servers["sequential-thinking"]["enabled"]:
            tools.append({
                "name": "sequential_thinking",
                "description": "ë‹¨ê³„ë³„ ì‚¬ê³ ë¥¼ í†µí•œ ì½”ë“œ ë¶„ì„",
                "server": "sequential-thinking",
                "type": "mcp"
            })
        
        # Filesystem ë„êµ¬
        if self.mcp_servers["filesystem"]["enabled"]:
            tools.append({
                "name": "filesystem_access",
                "description": "íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ ë° ì¡°ì‘",
                "server": "filesystem",
                "type": "mcp"
            })
        
        # Memory ë„êµ¬
        if self.mcp_servers["memory"]["enabled"]:
            tools.append({
                "name": "memory_management",
                "description": "ë©”ëª¨ë¦¬ ë° ì§€ì‹ ê·¸ë˜í”„ ê´€ë¦¬",
                "server": "memory",
                "type": "mcp"
            })
        
        # Gemini CLI ë„êµ¬
        tools.append({
            "name": "gemini_code_review",
            "description": "Gemini CLIë¥¼ í†µí•œ ì½”ë“œ ë¦¬ë·°",
            "server": "gemini",
            "type": "cli"
        })
        
        # vLLM ë„êµ¬
        if self.vllm_enabled:
            tools.append({
                "name": "vllm_code_review",
                "description": "vLLMì„ í†µí•œ ì½”ë“œ ë¦¬ë·°",
                "server": "vllm",
                "type": "api"
            })
        
        return tools
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‹¤ì œ MCP ì„œë²„ ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ"""
        stats = {
            "total_servers": len(self.mcp_servers),
            "enabled_servers": sum(1 for s in self.mcp_servers.values() if s["enabled"]),
            "server_stats": {}
        }
        
        # ê° MCP ì„œë²„ë³„ í†µê³„
        for server_name, server_config in self.mcp_servers.items():
            if server_config["enabled"]:
                stats["server_stats"][server_name] = {
                    "enabled": True,
                    "command": server_config["command"],
                    "transport": server_config["transport"],
                    "note": "ì‹¤ì œ MCP ì„œë²„"
                }
        
        # Gemini ì‚¬ìš©ëŸ‰ í†µê³„
        stats["server_stats"]["gemini"] = self.gemini_service.get_usage_stats()
        
        # vLLM ì‚¬ìš©ëŸ‰ í†µê³„
        if self.vllm_enabled:
            stats["server_stats"]["vllm"] = {
                "enabled": True,
                "base_url": self.vllm_base_url,
                "note": "vLLM ì‚¬ìš©ëŸ‰ ì¶”ì ì€ ë³„ë„ êµ¬í˜„ í•„ìš”"
            }
        
        return stats
    
    def _chunk_code_if_needed(self, code: str, language: str, context: Dict[str, Any] = None) -> List[str]:
        """
        ì½”ë“œ í¬ê¸°ì— ë”°ë¼ ì²­í‚¹ ìˆ˜í–‰
        
        GitHub PRì˜ í° ë³€ê²½ì‚¬í•­ì„ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„í•©ë‹ˆë‹¤.
        """
        # í† í° ì œí•œ ì„¤ì • (ëŒ€ëµì ì¸ ì¶”ì •)
        MAX_TOKENS_PER_CHUNK = 4000  # ì•ˆì „í•œ í¬ê¸°
        MAX_CHARS_PER_CHUNK = MAX_TOKENS_PER_CHUNK * 4  # ëŒ€ëµ 4ì = 1í† í°
        
        if len(code) <= MAX_CHARS_PER_CHUNK:
            return [code]
        
        logger.info(f"ì½”ë“œê°€ ë„ˆë¬´ í¼ ({len(code)}ì), ì²­í‚¹ ìˆ˜í–‰")
        
        # íŒŒì¼ë³„ë¡œ ì²­í‚¹ (PR diffì˜ ê²½ìš°)
        if context and context.get('files'):
            return self._chunk_by_files(code, context['files'], MAX_CHARS_PER_CHUNK)
        else:
            return self._chunk_by_size(code, MAX_CHARS_PER_CHUNK)
    
    def _chunk_by_files(self, code: str, files: List[str], max_chars: int) -> List[str]:
        """íŒŒì¼ë³„ë¡œ ì²­í‚¹"""
        chunks = []
        current_chunk = ""
        
        lines = code.split('\n')
        current_file = None
        
        for line in lines:
            # íŒŒì¼ í—¤ë” ê°ì§€ (diff í˜•ì‹)
            if line.startswith('diff --git') or line.startswith('+++') or line.startswith('---'):
                if current_chunk and len(current_chunk) > max_chars:
                    chunks.append(current_chunk)
                    current_chunk = ""
                current_file = line
                current_chunk += line + '\n'
            else:
                current_chunk += line + '\n'
                
                # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ë¶„í• 
                if len(current_chunk) > max_chars:
                    chunks.append(current_chunk)
                    current_chunk = ""
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks if chunks else [code]
    
    def _chunk_by_size(self, code: str, max_chars: int) -> List[str]:
        """í¬ê¸°ë³„ë¡œ ì²­í‚¹"""
        chunks = []
        lines = code.split('\n')
        current_chunk = ""
        
        for line in lines:
            if len(current_chunk) + len(line) + 1 > max_chars:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = line + '\n'
                else:
                    # ë‹¨ì¼ ë¼ì¸ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ê°•ì œë¡œ ë¶„í• 
                    chunks.append(line)
            else:
                current_chunk += line + '\n'
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks if chunks else [code]
    
    def _analyze_multiple_chunks_with_sequential_thinking(self, code_chunks: List[str], language: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ì²­í¬ë¥¼ Sequential Thinkingìœ¼ë¡œ ë¶„ì„"""
        try:
            all_results = []
            
            for i, chunk in enumerate(code_chunks):
                logger.info(f"ì²­í¬ {i+1}/{len(code_chunks)} ë¶„ì„ ì¤‘...")
                
                chunk_context = context.copy() if context else {}
                chunk_context['chunk_index'] = i + 1
                chunk_context['total_chunks'] = len(code_chunks)
                
                result = self._analyze_with_sequential_thinking(chunk, language, chunk_context)
                all_results.append({
                    "chunk": i + 1,
                    "analysis": result
                })
            
            # ì „ì²´ ê²°ê³¼ í†µí•©
            combined_review = self._combine_chunk_analyses(all_results, language)
            
            return {
                "review": combined_review,
                "language": language,
                "model": "sequential-thinking",
                "chunks_analyzed": len(code_chunks),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            raise ValueError(f"ë‹¤ì¤‘ ì²­í¬ Sequential Thinking ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _combine_chunk_analyses(self, chunk_results: List[Dict[str, Any]], language: str) -> str:
        """ì²­í¬ ë¶„ì„ ê²°ê³¼ë“¤ì„ í†µí•©"""
        combined = f"## ğŸ” {language} ì½”ë“œ ì¢…í•© ë¶„ì„ ê²°ê³¼\n\n"
        combined += f"**ì´ {len(chunk_results)}ê°œ ì²­í¬ ë¶„ì„ ì™„ë£Œ**\n\n"
        
        for result in chunk_results:
            chunk_num = result["chunk"]
            analysis = result["analysis"]
            
            combined += f"### ğŸ“„ ì²­í¬ {chunk_num} ë¶„ì„\n"
            combined += analysis.get("review", "ë¶„ì„ ê²°ê³¼ ì—†ìŒ") + "\n\n"
        
        combined += "### ğŸ¯ ì¢…í•© ê¶Œì¥ì‚¬í•­\n"
        combined += "- ëª¨ë“  ì²­í¬ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì½”ë“œ í’ˆì§ˆì„ ê°œì„ í•˜ì„¸ìš”\n"
        combined += "- ê° ì²­í¬ë³„ë¡œ ì‹ë³„ëœ ë¬¸ì œì ë“¤ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í•´ê²°í•˜ì„¸ìš”\n"
        combined += "- ì „ì²´ì ì¸ ì½”ë“œ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ì„¸ìš”\n"
        
        return combined
    
    def _build_github_context(self, context: Dict[str, Any] = None) -> str:
        """GitHub ì•±ì—ì„œ ì œê³µí•˜ëŠ” ì •ë³´ë¥¼ í™œìš©í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        if not context:
            return "ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì—†ìŒ"
        
        context_parts = []
        
        # PR ê¸°ë³¸ ì •ë³´
        if context.get('pr_number'):
            context_parts.append(f"**PR ë²ˆí˜¸**: #{context['pr_number']}")
        
        if context.get('pr_title'):
            context_parts.append(f"**PR ì œëª©**: {context['pr_title']}")
        
        if context.get('pr_body'):
            context_parts.append(f"**PR ì„¤ëª…**: {context['pr_body'][:200]}...")
        
        # ì €ì¥ì†Œ ì •ë³´
        if context.get('repository'):
            repo_info = context['repository']
            context_parts.append(f"**ì €ì¥ì†Œ**: {repo_info.get('full_name', 'unknown')}")
            context_parts.append(f"**ì–¸ì–´**: {repo_info.get('language', 'unknown')}")
            context_parts.append(f"**ì„¤ëª…**: {repo_info.get('description', 'N/A')}")
            context_parts.append(f"**í† í”½**: {', '.join(repo_info.get('topics', []))}")
            context_parts.append(f"**í¬ê¸°**: {repo_info.get('size', 0)} KB")
            context_parts.append(f"**ìŠ¤íƒ€**: {repo_info.get('stargazers_count', 0)}")
            context_parts.append(f"**í¬í¬**: {repo_info.get('forks_count', 0)}")
        
        # ì‘ì„±ì ì •ë³´
        if context.get('author'):
            author_info = context['author']
            context_parts.append(f"**ì‘ì„±ì**: @{author_info.get('login', 'unknown')}")
            context_parts.append(f"**ì‘ì„±ì íƒ€ì…**: {author_info.get('type', 'unknown')}")
        
        # ë¸Œëœì¹˜ ì •ë³´
        if context.get('branches'):
            branches = context['branches']
            if branches.get('head'):
                context_parts.append(f"**ì†ŒìŠ¤ ë¸Œëœì¹˜**: {branches['head'].get('ref', 'unknown')}")
            if branches.get('base'):
                context_parts.append(f"**íƒ€ê²Ÿ ë¸Œëœì¹˜**: {branches['base'].get('ref', 'unknown')}")
        
        # í†µê³„ ì •ë³´
        if context.get('stats'):
            stats = context['stats']
            context_parts.append(f"**ë³€ê²½ í†µê³„**: +{stats.get('additions', 0)}/-{stats.get('deletions', 0)} ({stats.get('changed_files', 0)}ê°œ íŒŒì¼)")
            context_parts.append(f"**ì»¤ë°‹ ìˆ˜**: {stats.get('commits', 0)}")
            context_parts.append(f"**ëŒ“ê¸€ ìˆ˜**: {stats.get('comments', 0)}")
        
        # ë¼ë²¨ ì •ë³´
        if context.get('labels'):
            labels = context['labels']
            if labels:
                context_parts.append(f"**ë¼ë²¨**: {', '.join(labels)}")
        
        # ë§ˆì¼ìŠ¤í†¤ ì •ë³´
        if context.get('milestone'):
            context_parts.append(f"**ë§ˆì¼ìŠ¤í†¤**: {context['milestone']}")
        
        # ë¦¬ë·°ì–´ ì •ë³´
        if context.get('reviewers'):
            reviewers = context['reviewers']
            if reviewers.get('requested'):
                context_parts.append(f"**ìš”ì²­ëœ ë¦¬ë·°ì–´**: {', '.join(reviewers['requested'])}")
            if reviewers.get('teams'):
                context_parts.append(f"**ìš”ì²­ëœ íŒ€**: {', '.join(reviewers['teams'])}")
        
        # PR ìƒì„¸ ì •ë³´
        if context.get('pr_details'):
            pr_details = context['pr_details']
            if pr_details.get('mergeable_state'):
                context_parts.append(f"**ë¨¸ì§€ ìƒíƒœ**: {pr_details['mergeable_state']}")
            if pr_details.get('draft'):
                context_parts.append("**ìƒíƒœ**: Draft PR")
            if pr_details.get('locked'):
                context_parts.append("**ìƒíƒœ**: Locked")
        
        # ì²­í¬ ì •ë³´ (ë‹¤ì¤‘ ì²­í¬ ë¶„ì„ ì‹œ)
        if context.get('chunk_index') and context.get('total_chunks'):
            context_parts.append(f"**ë¶„ì„ ì²­í¬**: {context['chunk_index']}/{context['total_chunks']}")
        
        return "\n".join(context_parts) if context_parts else "ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì—†ìŒ"