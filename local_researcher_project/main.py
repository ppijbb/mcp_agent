#!/usr/bin/env python3
"""
Autonomous Multi-Agent Research System - Main Entry Point
Implements 8 Core Innovations: Production-Grade Reliability, Universal MCP Hub, Streaming Pipeline

MCP agent ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ì˜ ììœ¨ ë¦¬ì„œì²˜ ì‹œìŠ¤í…œ.
ëª¨ë“  í•˜ë“œì½”ë”©, fallback, mock ì½”ë“œë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ MCP agentë¥¼ ì‚¬ìš©.

Usage:
    python main.py --request "ì—°êµ¬ ì£¼ì œ"                    # CLI ëª¨ë“œ
    python main.py --web                                    # ì›¹ ëª¨ë“œ
    python main.py --mcp-server                            # MCP ì„œë²„ ëª¨ë“œ
    python main.py --streaming                             # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
"""

import asyncio
import sys
import argparse
import subprocess
import signal
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import json
import os

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.researcher_config import load_config_from_env

# CRITICAL: Load configuration BEFORE importing any modules that depend on it
config = load_config_from_env()

# Use AutonomousOrchestrator for full research workflow
from src.core.autonomous_orchestrator import AutonomousOrchestrator
from src.monitoring.system_monitor import HealthMonitor

# Configure logging for production-grade reliability
# Advanced logging setup: setup logger manually to ensure logs directory exists and avoid issues with logging.basicConfig (per best practices)
log_dir = project_root / "logs"
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / "researcher.log"

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Remove existing handlers to avoid duplicate logs on reloads (e.g., under some app servers or noteboks)
if logger.hasHandlers():
    logger.handlers.clear()

# File handler
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)

# Console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)


class WebAppManager:
    """ì›¹ ì•± ê´€ë¦¬ì - Streaming Pipeline (Innovation 5) ì§€ì›"""
    
    def __init__(self):
        self.project_root = project_root
        self.health_monitor = HealthMonitor()
        
    def start_web_app(self):
        """ì›¹ ì•± ì‹œì‘ - Production-Grade Reliability ì ìš©"""
        try:
            streamlit_app_path = self.project_root / "src" / "web" / "streamlit_app.py"
            
            if not streamlit_app_path.exists():
                logger.error(f"Streamlit app not found at {streamlit_app_path}")
                return False
            
            # Get port from environment variable, default to 8501
            port = os.getenv("STREAMLIT_PORT", "8501")
            address = os.getenv("STREAMLIT_ADDRESS", "0.0.0.0")
            
            logger.info("ğŸŒ Starting Local Researcher Web Application with Streaming Pipeline...")
            logger.info(f"App will be available at: http://{address}:{port}")
            logger.info("Features: Real-time streaming, Progressive reporting, Incremental save")
            logger.info("Press Ctrl+C to stop the application")
            
            # Create logs directory if it doesn't exist
            logs_dir = self.project_root / "logs"
            logs_dir.mkdir(exist_ok=True)
            
            cmd = [
                sys.executable, "-m", "streamlit", "run",
                str(streamlit_app_path),
                "--server.port", port,
                "--server.address", address,
                "--browser.gatherUsageStats", "false",
                "--server.enableCORS", "false",
                "--server.enableXsrfProtection", "false"
            ]
            
            # Start health monitoring
            asyncio.create_task(self.health_monitor.start_monitoring())
            
            subprocess.run(cmd, cwd=str(self.project_root))
            return True
            
        except KeyboardInterrupt:
            logger.info("Application stopped by user")
            return True
        except Exception as e:
            logger.error(f"Error running web application: {e}")
            return False
    
    async def get_web_app_health(self) -> Dict[str, Any]:
        """Get web application health status."""
        port = int(os.getenv("STREAMLIT_PORT", "8501"))
        return {
            'status': 'running',
            'port': port,
            'streaming_enabled': True,
            'progressive_reporting': True,
            'incremental_save': True,
            'timestamp': datetime.now().isoformat()
        }


class AutonomousResearchSystem:
    """ììœ¨ ë¦¬ì„œì²˜ ì‹œìŠ¤í…œ - 8ê°€ì§€ í•µì‹¬ í˜ì‹  í†µí•© ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Load configurations from environment - ALL REQUIRED, NO DEFAULTS
        try:
            self.config = load_config_from_env()
            logger.info("âœ… Configuration loaded successfully from environment variables")
            
            # Validate ChromaDB availability (optional)
            try:
                import chromadb  # type: ignore
                logger.info("âœ… ChromaDB module available")
            except ImportError:
                logger.warning("âš ï¸ ChromaDB not installed - vector search will be disabled")
                logger.info("   Install with: pip install chromadb")
            
        except ValueError as e:
            logger.error(f"âŒ Configuration loading failed: {e}")
            logger.error("Please check your .env file and ensure all required variables are set")
            logger.info("\nRequired environment variables:")
            logger.info("  - LLM_MODEL: LLM model identifier (e.g., google/gemini-2.5-flash-lite)")
            logger.info("  - GOOGLE_API_KEY: Your Google or Vertex AI API key")
            logger.info("  - LLM_PROVIDER: Provider name (e.g., google)")
            raise
        
        # Initialize components with 8 innovations
        logger.info("ğŸ”§ Initializing system components...")
        try:
            # Use AutonomousOrchestrator for full research workflow with 8 innovations
            # Note: AutonomousOrchestrator loads config internally
            self.orchestrator = AutonomousOrchestrator()
            logger.info("âœ… Autonomous Orchestrator initialized")
        except Exception as e:
            logger.error(f"âŒ Orchestrator initialization failed: {e}")
            raise
        
        try:
            from src.core.mcp_integration import UniversalMCPHub
            self.mcp_hub = UniversalMCPHub()
            logger.info("âœ… MCP Hub initialized")
        except Exception as e:
            logger.error(f"âŒ MCP Hub initialization failed: {e}")
            raise
        
        try:
            self.web_manager = WebAppManager()
            logger.info("âœ… Web Manager initialized")
        except Exception as e:
            logger.error(f"âŒ Web Manager initialization failed: {e}")
            raise
        
        try:
            self.health_monitor = HealthMonitor()
            logger.info("âœ… Health Monitor initialized")
        except Exception as e:
            logger.error(f"âŒ Health Monitor initialization failed: {e}")
            raise
        
        # Initialize signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # Shutdown flag ì´ˆê¸°í™”
        self._shutdown_requested = False
        
        logger.info("âœ… AutonomousResearchSystem initialized successfully")
        
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully."""
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        import sys
        
        # Shutdown í”Œë˜ê·¸ ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
        if hasattr(self, '_shutdown_requested') and self._shutdown_requested:
            # ì´ë¯¸ ì¢…ë£Œ ì¤‘ì´ë©´ ì¬ì§„ì… ë°©ì§€ë§Œ ìˆ˜í–‰
            logger.warning("Shutdown already in progress; ignoring additional signal")
            return
        
        self._shutdown_requested = True
        
        # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  shutdown ì‘ì—… ìŠ¤ì¼€ì¤„ë§
        try:
            loop = asyncio.get_running_loop()
            # ì¤‘ë³µ ìƒì„± ë°©ì§€: ì´ë¯¸ ìŠ¤ì¼€ì¤„ëœ ì‘ì—…ì´ ìˆìœ¼ë©´ ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ
            if not hasattr(self, '_shutdown_task') or self._shutdown_task is None or self._shutdown_task.done():
                def _schedule():
                    self._shutdown_task = asyncio.create_task(self._graceful_shutdown())
                loop.call_soon_threadsafe(_schedule)
            else:
                logger.debug("Shutdown task already scheduled")
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ê°•ì œ ì¢…ë£Œ
            logger.warning("No event loop available, forcing exit")
            sys.exit(1)
    
    async def _graceful_shutdown(self):
        """Graceful shutdown with state persistence."""
        import sys
        import signal
        
        try:
            logger.info("Performing graceful shutdown...")
            
            # Health monitor ì •ì§€ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            try:
                await asyncio.wait_for(self.health_monitor.stop_monitoring(), timeout=5.0)
            except asyncio.TimeoutError:
                logger.warning("Health monitor stop timed out")
            except Exception as e:
                logger.debug(f"Error stopping health monitor: {e}")
            
            # MCP Hub cleanup (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            if self.config.mcp.enabled and self.mcp_hub:
                try:
                    # ì‹ ê·œ ì—°ê²° ì°¨ë‹¨
                    if hasattr(self.mcp_hub, 'start_shutdown'):
                        self.mcp_hub.start_shutdown()
                    # cleanupì€ CancelledErrorë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬´ì‹œ
                    try:
                        await asyncio.wait_for(self.mcp_hub.cleanup(), timeout=10.0)
                    except asyncio.CancelledError:
                        logger.debug("MCP Hub cleanup was cancelled (normal during shutdown)")
                    except asyncio.TimeoutError:
                        logger.warning("MCP Hub cleanup timed out")
                except asyncio.CancelledError:
                    logger.debug("MCP Hub cleanup setup was cancelled (normal during shutdown)")
                except Exception as e:
                    logger.warning(f"Error cleaning up MCP Hub: {e}")
            
            logger.info("âœ… Graceful shutdown completed")
            
        except Exception as e:
            logger.error(f"Error during graceful shutdown: {e}")
        finally:
            # ìµœì¢… ì¢…ë£Œ ì¤€ë¹„
            logger.info("Exiting...")
            # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒœìŠ¤í¬ëŠ” ê°œë³„ ë§¤ë‹ˆì €ê°€ ì •ë¦¬í•¨. ì¼ê´„ ì·¨ì†ŒëŠ” í•˜ì§€ ì•ŠìŒ
            
            # sys.exit(0)ì€ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - asyncio.run()ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            # ëŒ€ì‹  ë£¨í”„ì—ì„œ ë‚˜ê°€ë„ë¡ í•¨
    
    async def run_research(self, request: str, output_path: Optional[str] = None, 
                          streaming: bool = False, output_format: str = "json") -> Dict[str, Any]:
        """ì—°êµ¬ ì‹¤í–‰ - 8ê°€ì§€ í•µì‹¬ í˜ì‹  ì ìš©"""
        logger.info("ğŸ¤– Starting Autonomous Research System with 8 Core Innovations")
        logger.info("=" * 80)
        logger.info(f"Request: {request}")
        logger.info(f"Primary LLM: {self.config.llm.primary_model}")
        logger.info(f"Planning Model: {self.config.llm.planning_model}")
        logger.info(f"Reasoning Model: {self.config.llm.reasoning_model}")
        logger.info(f"Verification Model: {self.config.llm.verification_model}")
        logger.info(f"Self-planning: {self.config.agent.enable_self_planning}")
        logger.info(f"Agent Communication: {self.config.agent.enable_agent_communication}")
        logger.info(f"MCP Enabled: {self.config.mcp.enabled}")
        logger.info(f"Streaming Pipeline: {streaming}")
        logger.info(f"Adaptive Supervisor: {self.config.agent.max_concurrent_research_units}")
        logger.info(f"Hierarchical Compression: {self.config.compression.enabled}")
        logger.info(f"Continuous Verification: {self.config.verification.enabled}")
        logger.info(f"Adaptive Context Window: {self.config.context_window.enabled}")
        logger.info("=" * 80)
        
        try:
            # Start health monitoring
            await self.health_monitor.start_monitoring()
            
            # Initialize MCP client if enabled
            if self.config.mcp.enabled:
                try:
                    await self.mcp_hub.initialize_mcp()
                except asyncio.CancelledError:
                    # ì´ˆê¸°í™” ì¤‘ ì·¨ì†Œëœ ê²½ìš° - ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ ì¢…ë£Œ
                    logger.warning("MCP initialization was cancelled")
                    raise
            
            # Run research with production-grade reliability
            if streaming:
                result = await self._run_streaming_research(request)
            else:
                # Use AutonomousOrchestrator with full research workflow
                result = await self.orchestrator.run_research(user_request=request, context={})
                
                # Extract content from synthesis
                final_synthesis = result.get("synthesis_results", {}).get("content", "")
                if not final_synthesis:
                    final_synthesis = result.get("content", "")
                if not final_synthesis:
                    # Fallback: use execution results
                    execution_results = result.get("detailed_results", {}).get("execution_results", [])
                    if execution_results:
                        final_synthesis = "\n\n".join([
                            str(r.get("result", r)) for r in execution_results[:5]
                        ])
                
                # Convert to expected format
                result = {
                    "query": request,
                    "content": final_synthesis or "Research completed",
                    "timestamp": datetime.now().isoformat(),
                    "metadata": result.get("metadata", {
                        "model_used": "multi-agent",
                        "execution_time": 0.0,
                        "cost": 0.0,
                        "confidence": result.get("metadata", {}).get("confidence", 0.9)
                    }),
                    "synthesis_results": result.get("synthesis_results", {
                        "content": final_synthesis
                    }),
                    "sources": self._extract_sources(result),
                    "innovation_stats": result.get("innovation_stats", {}),
                    "system_health": result.get("system_health", {"overall_status": "healthy"})
                }
            
            # Apply hierarchical compression if enabled
            # Commented out to avoid serialization errors
            # if self.config.compression.enabled:
            #     result = await self._apply_hierarchical_compression(result)
            
            # Save results with incremental save
            if output_path:
                await self._save_results_incrementally(result, output_path, output_format)
                logger.info(f"ğŸ“„ Results saved to: {output_path}")
            else:
                # Generate default output path if not provided
                output_dir = project_root / "output"
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                default_output = output_dir / f"research_{timestamp}.{output_format}"
                await self._save_results_incrementally(result, str(default_output), output_format)
                logger.info(f"ğŸ“„ Results saved to default location: {default_output}")
                self._display_results(result)
            
            # Get final health status
            health_status = self.health_monitor.get_system_health()
            result['system_health'] = health_status
            
            logger.info("âœ… Research completed successfully with 8 Core Innovations")
            return result
            
        except Exception as e:
            logger.error(f"Research failed: {e}")
            # Get error health status
            error_health = self.health_monitor.get_system_health()
            logger.error(f"System health at failure: {error_health}")
            raise
    
    async def _run_streaming_research(self, request: str) -> Dict[str, Any]:
        """Run research with streaming pipeline (Innovation 5)."""
        logger.info("ğŸŒŠ Starting streaming research pipeline...")
        
        # Create streaming callback
        async def streaming_callback(partial_result: Dict[str, Any]):
            logger.info(f"ğŸ“Š Streaming partial result: {partial_result.get('type', 'unknown')}")
            # In a real implementation, this would send to web interface
            print(f"ğŸ“Š Partial Result: {partial_result.get('summary', 'Processing...')}")
        
        # Use standard run_research but with streaming callback simulation
        result = await self.orchestrator.run_research(user_request=request, context={})
        
        # Extract and format result
        final_synthesis = result.get("synthesis_results", {}).get("content", "")
        if not final_synthesis:
            final_synthesis = result.get("content", "")
        
        formatted_result = {
            "query": request,
            "content": final_synthesis or "Research completed",
            "timestamp": datetime.now().isoformat(),
            "metadata": result.get("metadata", {}),
            "synthesis_results": result.get("synthesis_results", {}),
            "sources": self._extract_sources(result),
            "innovation_stats": result.get("innovation_stats", {}),
            "system_health": result.get("system_health", {})
        }
        
        logger.info("âœ… Streaming research completed")
        return formatted_result
    
    def _extract_sources(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract sources from research results."""
        sources = []
        
        # Try to extract from execution results
        execution_results = result.get("detailed_results", {}).get("execution_results", [])
        for exec_result in execution_results:
            if isinstance(exec_result, dict):
                # Look for search results
                if "results" in exec_result:
                    sources.extend(exec_result["results"])
                elif "sources" in exec_result:
                    sources.extend(exec_result["sources"])
                elif "url" in exec_result:
                    sources.append(exec_result)
        
        # Deduplicate by URL
        seen_urls = set()
        unique_sources = []
        for source in sources:
            url = source.get("url") or source.get("link")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append({
                    "title": source.get("title", source.get("name", "")),
                    "url": url,
                    "snippet": source.get("snippet", source.get("summary", ""))
                })
        
        return unique_sources[:20]  # Limit to 20 sources
    
    async def _apply_hierarchical_compression(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Apply hierarchical compression (Innovation 2)."""
        if not self.config.compression.enabled:
            return result
        
        logger.info("ğŸ—œï¸ Applying hierarchical compression...")
        
        # Import compression module
        from src.core.compression import compress_data
        
        # Compress large text fields
        if 'synthesis_results' in result:
            compressed_synthesis = await compress_data(
                result['synthesis_results']
            )
            result['synthesis_results_compressed'] = compressed_synthesis
        
        logger.info("âœ… Hierarchical compression applied")
        return result
    
    async def _save_results_incrementally(self, result: Dict[str, Any], output_path: str, output_format: str = "json"):
        """Save results with incremental save (Innovation 5)."""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Save incrementally based on format
        temp_file = output_file.with_suffix('.tmp')
        
        if output_format.lower() == "json":
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
        elif output_format.lower() == "yaml":
            import yaml
            with open(temp_file, 'w', encoding='utf-8') as f:
                yaml.dump(result, f, default_flow_style=False, allow_unicode=True)
        elif output_format.lower() == "txt":
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(f"Research Results\n")
                f.write(f"===============\n\n")
                f.write(f"Query: {result.get('query', 'N/A')}\n")
                f.write(f"Timestamp: {result.get('timestamp', 'N/A')}\n\n")
                if 'content' in result:
                    f.write(f"Content:\n{result['content']}\n\n")
                elif 'synthesis_results' in result:
                    synthesis = result['synthesis_results']
                    if isinstance(synthesis, dict) and 'content' in synthesis:
                        f.write(f"Content:\n{synthesis['content']}\n\n")
                if 'sources' in result:
                    f.write(f"Sources:\n")
                    for i, source in enumerate(result['sources'], 1):
                        f.write(f"{i}. {source.get('title', 'N/A')} - {source.get('url', 'N/A')}\n")
        else:
            raise ValueError(f"Unsupported output format: {output_format}")
        
        # Atomic move
        temp_file.replace(output_file)
        
        logger.info(f"âœ… Results saved incrementally to: {output_file} (format: {output_format})")
    
    def _display_results(self, result: Dict[str, Any]):
        """Display results with enhanced formatting."""
        print("\nğŸ“‹ Research Results with 8 Core Innovations:")
        print("=" * 80)
        
        # Display main research content
        if 'content' in result and result['content']:
            print("\nğŸ“ Research Content:")
            print("-" * 60)
            print(result['content'])
            print("-" * 60)
        elif 'synthesis_results' in result:
            synthesis = result['synthesis_results']
            if isinstance(synthesis, dict) and 'content' in synthesis:
                print("\nğŸ“ Research Content:")
                print("-" * 60)
                print(synthesis['content'])
                print("-" * 60)
            else:
                print(f"\nğŸ“ Synthesis: {synthesis}")
        else:
            print("\nâŒ No research content found in results")
        
        # Display research metadata
        if 'metadata' in result:
            metadata = result['metadata']
            print(f"\nğŸ“Š Research Metadata:")
            print(f"  â€¢ Model Used: {metadata.get('model_used', 'N/A')}")
            print(f"  â€¢ Execution Time: {metadata.get('execution_time', 'N/A'):.2f}s")
            print(f"  â€¢ Cost: ${metadata.get('cost', 0):.4f}")
            print(f"  â€¢ Confidence: {metadata.get('confidence', 'N/A')}")
        
        # Display synthesis results
        if 'synthesis_results' in result and isinstance(result['synthesis_results'], dict):
            synthesis = result['synthesis_results']
            if 'synthesis_results' in synthesis:
                print(f"\nğŸ“ Synthesis: {synthesis.get('synthesis_results', 'N/A')}")
        
        # Display innovation stats
        if 'innovation_stats' in result:
            stats = result['innovation_stats']
            print(f"\nğŸš€ Innovation Statistics:")
            print(f"  â€¢ Adaptive Supervisor: {stats.get('adaptive_supervisor', 'N/A')}")
            print(f"  â€¢ Hierarchical Compression: {stats.get('hierarchical_compression', 'N/A')}")
            print(f"  â€¢ Multi-Model Orchestration: {stats.get('multi_model_orchestration', 'N/A')}")
            print(f"  â€¢ Continuous Verification: {stats.get('continuous_verification', 'N/A')}")
            print(f"  â€¢ Streaming Pipeline: {stats.get('streaming_pipeline', 'N/A')}")
            print(f"  â€¢ Universal MCP Hub: {stats.get('universal_mcp_hub', 'N/A')}")
            print(f"  â€¢ Adaptive Context Window: {stats.get('adaptive_context_window', 'N/A')}")
            print(f"  â€¢ Production-Grade Reliability: {stats.get('production_grade_reliability', 'N/A')}")
        
        # Display system health
        if 'system_health' in result:
            health = result['system_health']
            print(f"\nğŸ¥ System Health: {health.get('overall_status', 'Unknown')}")
            print(f"  â€¢ Health Score: {health.get('health_score', 'N/A')}")
            print(f"  â€¢ Monitoring Active: {health.get('monitoring_active', 'N/A')}")
        
        print("=" * 80)
    
    async def run_mcp_server(self):
        """MCP ì„œë²„ ì‹¤í–‰"""
        await self.mcp_hub.initialize_mcp()
    
    async def run_mcp_client(self):
        """MCP í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰"""
        await self.mcp_hub.initialize_mcp()
    
    def run_web_app(self):
        """ì›¹ ì•± ì‹¤í–‰"""
        return self.web_manager.start_web_app()
    
    async def run_health_check(self):
        """Run comprehensive health check for all system components."""
        logger.info("ğŸ¥ Running comprehensive health check...")
        
        # Check MCP tools health
        if self.config.mcp.enabled:
            # MCP Hub health check
            logger.info("MCP Hub initialized and ready")
        
        # Check system health
        system_health = self.health_monitor.get_system_health()
        logger.info(f"System Health: {system_health.get('overall_status', 'Unknown')}")
        
        # Check web app health
        web_health = await self.web_manager.get_web_app_health()
        logger.info(f"Web App Health: {web_health.get('status', 'Unknown')}")
        
        logger.info("âœ… Health check completed")
    
    async def check_mcp_servers(self):
        """MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸."""
        logger.info("ğŸ“Š Checking MCP server connections...")
        
        if not self.config.mcp.enabled:
            logger.warning("MCP is disabled")
            return
        
        try:
            # MCP Hub ì´ˆê¸°í™” í™•ì¸ - ì´ë¯¸ ì—°ê²°ëœ ì„œë²„ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if self.mcp_hub.mcp_sessions:
                logger.info(f"Found {len(self.mcp_hub.mcp_sessions)} existing MCP server connections")
            else:
                logger.info("No existing connections. Will attempt quick connection tests for each server...")
            
            # ì„œë²„ ìƒíƒœ í™•ì¸ (ê° ì„œë²„ì— ëŒ€í•´ ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì—°ê²° ì‹œë„)
            logger.info("Checking MCP server connection status...")
            server_status = await self.mcp_hub.check_mcp_servers()
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "=" * 80)
            print("ğŸ“Š MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸")
            print("=" * 80)
            print(f"ì „ì²´ ì„œë²„ ìˆ˜: {server_status['total_servers']}")
            print(f"ì—°ê²°ëœ ì„œë²„: {server_status['connected_servers']}")
            print(f"ì—°ê²°ë¥ : {server_status['summary']['connection_rate']}")
            print(f"ì „ì²´ ì‚¬ìš© ê°€ëŠ¥í•œ Tool ìˆ˜: {server_status['summary']['total_tools_available']}")
            print("\n")
            
            for server_name, info in server_status["servers"].items():
                status_icon = "âœ…" if info["connected"] else "âŒ"
                print(f"{status_icon} ì„œë²„: {server_name}")
                print(f"   íƒ€ì…: {info['type']}")
                
                if info["type"] == "http":
                    print(f"   URL: {info.get('url', 'unknown')}")
                else:
                    cmd = info.get('command', 'unknown')
                    args_preview = ' '.join(info.get('args', [])[:3])
                    print(f"   ëª…ë ¹ì–´: {cmd} {args_preview}...")
                
                print(f"   ì—°ê²° ìƒíƒœ: {'ì—°ê²°ë¨' if info['connected'] else 'ì—°ê²° ì•ˆ ë¨'}")
                print(f"   ì œê³µ Tool ìˆ˜: {info['tools_count']}")
                
                if info["tools"]:
                    print(f"   Tool ëª©ë¡:")
                    for tool in info["tools"][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                        registered_name = f"{server_name}::{tool}"
                        print(f"     - {registered_name}")
                    if len(info["tools"]) > 5:
                        print(f"     ... ë° {len(info['tools']) - 5}ê°œ ë”")
                
                if info.get("error"):
                    print(f"   âš ï¸ ì˜¤ë¥˜: {info['error']}")
                print()
            
            print("=" * 80)
            
            # ìš”ì•½ ì •ë³´ ë¡œê¹…
            logger.info(f"MCP ì„œë²„ í™•ì¸ ì™„ë£Œ: {server_status['summary']['connection_rate']} ì—°ê²°")
            
        except Exception as e:
            logger.error(f"MCP ì„œë²„ í™•ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())


async def main():
    """Main function - 8ê°€ì§€ í•µì‹¬ í˜ì‹  í†µí•© ì‹¤í–‰ ì§„ì…ì """
    # Python ì¢…ë£Œ ì‹œ ë°œìƒí•˜ëŠ” async generator ì •ë¦¬ ì˜¤ë¥˜ ë¬´ì‹œ
    def ignore_async_gen_errors(loop, context):
        """anyio cancel scope ë° async generator ì¢…ë£Œ ì˜¤ë¥˜ ë¬´ì‹œ"""
        exception = context.get('exception')
        if exception:
            error_msg = str(exception)
            # anyio cancel scope ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
            if isinstance(exception, RuntimeError) and ("cancel scope" in error_msg.lower() or "different task" in error_msg.lower()):
                return  # ë¬´ì‹œ
            # async generator ì¢…ë£Œ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
            if isinstance(exception, GeneratorExit) or "async_generator" in error_msg.lower():
                return  # ë¬´ì‹œ
        # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ê¸°ë³¸ handlerë¡œ ì „ë‹¬
        loop.set_exception_handler(None)
        loop.call_exception_handler(context)
        loop.set_exception_handler(ignore_async_gen_errors)
    
    # asyncio exception handler ì„¤ì •
    loop = asyncio.get_running_loop()
    loop.set_exception_handler(ignore_async_gen_errors)
    
    parser = argparse.ArgumentParser(
        description="Autonomous Multi-Agent Research System with 8 Core Innovations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py --request "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ ì „ë§"
  python main.py --request "ì—°êµ¬ ì£¼ì œ" --output results/report.json
  python main.py --request "ì—°êµ¬ ì£¼ì œ" --streaming
  python main.py --web
  python main.py --mcp-server
  python main.py --mcp-client
  python main.py --health-check
        """
    )
    
    # Mode selection
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument("--request", help="Research request (CLI mode)")
    mode_group.add_argument("--web", action="store_true", help="Start web application with streaming")
    mode_group.add_argument("--mcp-server", action="store_true", help="Start MCP server with Universal MCP Hub")
    mode_group.add_argument("--mcp-client", action="store_true", help="Start MCP client with Smart Tool Selection")
    mode_group.add_argument("--health-check", action="store_true", help="Check system health and MCP tools")
    mode_group.add_argument("--check-mcp-servers", action="store_true", help="Check all MCP server connections and list tools")
    
    # Optional arguments
    parser.add_argument("--output", help="Output file path for research results")
    parser.add_argument("--config", help="Configuration file path")
    parser.add_argument("--format", choices=["json", "yaml", "txt"], default="json", help="Output format for research results")
    parser.add_argument("--streaming", action="store_true", help="Enable streaming pipeline for real-time results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Create logs directory
    logs_dir = project_root / "logs"
    logs_dir.mkdir(exist_ok=True)
    
    # Initialize system
    system = AutonomousResearchSystem()
    
    try:
        if args.request:
            # CLI Research Mode with 8 innovations
            await system.run_research(
                args.request, 
                args.output, 
                streaming=args.streaming,
                output_format=args.format
            )
            
        elif args.web:
            # Web Application Mode with Streaming Pipeline
            system.run_web_app()
            
        elif args.mcp_server:
            # MCP Server Mode with Universal MCP Hub - ì‹¤ì œ ì—°ê²° ìˆ˜í–‰
            logger.info("Initializing MCP servers...")
            try:
                await system.mcp_hub.initialize_mcp()
                logger.info("âœ… MCP servers initialized")
                
                # ì—°ê²°ëœ ì„œë²„ ìƒíƒœ ì¶œë ¥
                if system.mcp_hub.mcp_sessions:
                    print("\n" + "=" * 80)
                    print("âœ… MCP ì„œë²„ ì—°ê²° ì™„ë£Œ")
                    print("=" * 80)
                    for server_name in system.mcp_hub.mcp_sessions.keys():
                        tools_count = len(system.mcp_hub.mcp_tools_map.get(server_name, {}))
                        print(f"âœ… {server_name}: {tools_count} tools available")
                    print("=" * 80)
                    print("\nMCP Hub is running. Press Ctrl+C to stop.")
                    
                    # ê³„ì† ì‹¤í–‰ ëŒ€ê¸°
                    try:
                        await asyncio.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸° (ë˜ëŠ” Ctrl+Cë¡œ ì¢…ë£Œ)
                    except KeyboardInterrupt:
                        logger.info("Shutting down MCP Hub...")
                else:
                    logger.warning("âš ï¸ No MCP servers connected")
                    sys.exit(1)
            except Exception as e:
                logger.error(f"Failed to initialize MCP servers: {e}")
                sys.exit(1)
            
        elif args.mcp_client:
            # MCP Client Mode with Smart Tool Selection
            success = await system.run_mcp_client()
            if not success:
                sys.exit(1)
            
        elif args.health_check:
            # Health Check Mode
            await system.run_health_check()
        
        elif args.check_mcp_servers:
            # MCP ì„œë²„ í™•ì¸ ëª¨ë“œ
            await system.check_mcp_servers()
            
    except KeyboardInterrupt:
        logger.info("Operation cancelled by user (KeyboardInterrupt)")
        system._shutdown_requested = True
        try:
            await system._graceful_shutdown()
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
        # sys.exit(0) ì œê±° - asyncio.run()ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    except asyncio.CancelledError:
        # ì·¨ì†Œëœ ê²½ìš° ì •ë¦¬ í›„ ì¢…ë£Œ
        logger.info("Operation cancelled")
        system._shutdown_requested = True
        try:
            await system._graceful_shutdown()
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
        # asyncio.CancelledErrorëŠ” ë‹¤ì‹œ raiseí•˜ì—¬ ì •ìƒì ì¸ ì·¨ì†Œ íë¦„ ìœ ì§€
        raise
    except Exception as e:
        logger.error(f"Error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        system._shutdown_requested = True
        try:
            await system._graceful_shutdown()
        except Exception as e2:
            logger.error(f"Error during shutdown: {e2}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ ì½”ë“œ 1ë¡œ ì¢…ë£Œ
        sys.exit(1)
    finally:
        # ìµœì¢… ì •ë¦¬ ë³´ì¥
        if hasattr(system, 'mcp_hub') and system.mcp_hub and hasattr(system.mcp_hub, 'mcp_sessions'):
            try:
                if system.mcp_hub.mcp_sessions:
                    logger.info("Final cleanup of MCP connections...")
                    await system.mcp_hub.cleanup()
            except Exception as e:
                logger.debug(f"Error in final cleanup: {e}")


if __name__ == "__main__":
    asyncio.run(main())