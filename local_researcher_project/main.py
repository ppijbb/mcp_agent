#!/usr/bin/env python3
"""
Autonomous Multi-Agent Research System - Main Entry Point
Implements 8 Core Innovations: Production-Grade Reliability, Universal MCP Hub, Streaming Pipeline

MCP agent ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ì˜ ììœ¨ ë¦¬ì„œì²˜ ì‹œìŠ¤í…œ.
ëª¨ë“  í•˜ë“œì½”ë”©, fallback, mock ì½”ë“œë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ MCP agentë¥¼ ì‚¬ìš©.

Usage:
    python main.py --request "ì—°êµ¬ ì£¼ì œ"                    # CLI ëª¨ë“œ
    python main.py --web                                    # ì›¹ ëª¨ë“œ
    python main.py --mcp-server                            # MCP ì„œë²„ ëª¨ë“œ
    python main.py --streaming                             # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
"""

import asyncio
import sys
import argparse
import subprocess
import signal
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import json
import os

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.researcher_config import load_config_from_env

# CRITICAL: Load configuration BEFORE importing any modules that depend on it
config = load_config_from_env()

# Use new AgentOrchestrator for multi-agent orchestration
from src.core.agent_orchestrator import AgentOrchestrator as NewAgentOrchestrator
from src.core.autonomous_orchestrator import AutonomousOrchestrator
from src.monitoring.system_monitor import HealthMonitor

# Configure logging for production-grade reliability
# Advanced logging setup: setup logger manually to ensure logs directory exists and avoid issues with logging.basicConfig (per best practices)
log_dir = project_root / "logs"
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / "researcher.log"

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Remove existing handlers to avoid duplicate logs on reloads (e.g., under some app servers or noteboks)
if logger.hasHandlers():
    logger.handlers.clear()

# File handler
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)

# Console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)


class WebAppManager:
    """ì›¹ ì•± ê´€ë¦¬ì - Streaming Pipeline (Innovation 5) ì§€ì›"""
    
    def __init__(self):
        self.project_root = project_root
        self.health_monitor = HealthMonitor()
        
    def start_web_app(self):
        """ì›¹ ì•± ì‹œì‘ - Production-Grade Reliability ì ìš©"""
        try:
            streamlit_app_path = self.project_root / "src" / "web" / "streamlit_app.py"
            
            if not streamlit_app_path.exists():
                logger.error(f"Streamlit app not found at {streamlit_app_path}")
                return False
            
            # Get port from environment variable, default to 8501
            port = os.getenv("STREAMLIT_PORT", "8501")
            address = os.getenv("STREAMLIT_ADDRESS", "0.0.0.0")
            
            logger.info("ğŸŒ Starting Local Researcher Web Application with Streaming Pipeline...")
            logger.info(f"App will be available at: http://{address}:{port}")
            logger.info("Features: Real-time streaming, Progressive reporting, Incremental save")
            logger.info("Press Ctrl+C to stop the application")
            
            # Create logs directory if it doesn't exist
            logs_dir = self.project_root / "logs"
            logs_dir.mkdir(exist_ok=True)
            
            cmd = [
                sys.executable, "-m", "streamlit", "run",
                str(streamlit_app_path),
                "--server.port", port,
                "--server.address", address,
                "--browser.gatherUsageStats", "false",
                "--server.enableCORS", "false",
                "--server.enableXsrfProtection", "false"
            ]
            
            # Start health monitoring
            asyncio.create_task(self.health_monitor.start_monitoring())
            
            subprocess.run(cmd, cwd=str(self.project_root))
            return True
            
        except KeyboardInterrupt:
            logger.info("Application stopped by user")
            return True
        except Exception as e:
            logger.error(f"Error running web application: {e}")
            return False
    
    async def get_web_app_health(self) -> Dict[str, Any]:
        """Get web application health status."""
        port = int(os.getenv("STREAMLIT_PORT", "8501"))
        return {
            'status': 'running',
            'port': port,
            'streaming_enabled': True,
            'progressive_reporting': True,
            'incremental_save': True,
            'timestamp': datetime.now().isoformat()
        }


class AutonomousResearchSystem:
    """ììœ¨ ë¦¬ì„œì²˜ ì‹œìŠ¤í…œ - 8ê°€ì§€ í•µì‹¬ í˜ì‹  í†µí•© ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Load configurations from environment - ALL REQUIRED, NO DEFAULTS
        try:
            self.config = load_config_from_env()
            logger.info("âœ… Configuration loaded successfully from environment variables")
            
            # Validate ChromaDB availability (optional)
            try:
                import chromadb  # type: ignore
                logger.info("âœ… ChromaDB module available")
            except ImportError:
                logger.warning("âš ï¸ ChromaDB not installed - vector search will be disabled")
                logger.info("   Install with: pip install chromadb")
            
        except ValueError as e:
            logger.error(f"âŒ Configuration loading failed: {e}")
            logger.error("Please check your .env file and ensure all required variables are set")
            logger.info("\nRequired environment variables:")
            logger.info("  - LLM_MODEL: LLM model identifier (e.g., google/gemini-2.5-flash-lite)")
            logger.info("  - GOOGLE_API_KEY: Your Google or Vertex AI API key")
            logger.info("  - LLM_PROVIDER: Provider name (e.g., google)")
            raise
        
        # Initialize components with 8 innovations
        logger.info("ğŸ”§ Initializing system components...")
        try:
            # Use new multi-agent orchestrator (no fallback - fail clearly)
            self.orchestrator = NewAgentOrchestrator()
            logger.info("âœ… Multi-Agent Orchestrator initialized (no fallback mode)")
            logger.info("âœ… Autonomous Orchestrator initialized")
        except Exception as e:
            logger.error(f"âŒ Orchestrator initialization failed: {e}")
            raise
        
        try:
            from src.core.mcp_integration import UniversalMCPHub
            self.mcp_hub = UniversalMCPHub()
            logger.info("âœ… MCP Hub initialized")
        except Exception as e:
            logger.error(f"âŒ MCP Hub initialization failed: {e}")
            raise
        
        try:
            self.web_manager = WebAppManager()
            logger.info("âœ… Web Manager initialized")
        except Exception as e:
            logger.error(f"âŒ Web Manager initialization failed: {e}")
            raise
        
        try:
            self.health_monitor = HealthMonitor()
            logger.info("âœ… Health Monitor initialized")
        except Exception as e:
            logger.error(f"âŒ Health Monitor initialization failed: {e}")
            raise
        
        # Initialize signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # Shutdown flag ì´ˆê¸°í™”
        self._shutdown_requested = False
        
        logger.info("âœ… AutonomousResearchSystem initialized successfully")
        
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully."""
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        import sys
        
        # Shutdown í”Œë˜ê·¸ ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
        if hasattr(self, '_shutdown_requested') and self._shutdown_requested:
            # ì´ë¯¸ ì¢…ë£Œ ì¤‘ì´ë©´ ì¬ì§„ì… ë°©ì§€ë§Œ ìˆ˜í–‰
            logger.warning("Shutdown already in progress; ignoring additional signal")
            return
        
        self._shutdown_requested = True
        
        # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  shutdown ì‘ì—… ìŠ¤ì¼€ì¤„ë§
        try:
            loop = asyncio.get_running_loop()
            # ì¤‘ë³µ ìƒì„± ë°©ì§€: ì´ë¯¸ ìŠ¤ì¼€ì¤„ëœ ì‘ì—…ì´ ìˆìœ¼ë©´ ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ
            if not hasattr(self, '_shutdown_task') or self._shutdown_task is None or self._shutdown_task.done():
                def _schedule():
                    self._shutdown_task = asyncio.create_task(self._graceful_shutdown())
                loop.call_soon_threadsafe(_schedule)
            else:
                logger.debug("Shutdown task already scheduled")
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ê°•ì œ ì¢…ë£Œ
            logger.warning("No event loop available, forcing exit")
            sys.exit(1)
    
    async def _graceful_shutdown(self):
        """Graceful shutdown with state persistence."""
        import sys
        import signal
        
        try:
            logger.info("Performing graceful shutdown...")
            
            # Health monitor ì •ì§€ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            try:
                await asyncio.wait_for(self.health_monitor.stop_monitoring(), timeout=5.0)
            except asyncio.TimeoutError:
                logger.warning("Health monitor stop timed out")
            except Exception as e:
                logger.debug(f"Error stopping health monitor: {e}")
            
            # MCP Hub cleanup (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            if self.config.mcp.enabled and self.mcp_hub:
                try:
                    # ì‹ ê·œ ì—°ê²° ì°¨ë‹¨
                    if hasattr(self.mcp_hub, 'start_shutdown'):
                        self.mcp_hub.start_shutdown()
                    # cleanupì€ CancelledErrorë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬´ì‹œ
                    try:
                        await asyncio.wait_for(self.mcp_hub.cleanup(), timeout=10.0)
                    except asyncio.CancelledError:
                        logger.debug("MCP Hub cleanup was cancelled (normal during shutdown)")
                    except asyncio.TimeoutError:
                        logger.warning("MCP Hub cleanup timed out")
                except asyncio.CancelledError:
                    logger.debug("MCP Hub cleanup setup was cancelled (normal during shutdown)")
                except Exception as e:
                    logger.warning(f"Error cleaning up MCP Hub: {e}")
            
            logger.info("âœ… Graceful shutdown completed")
            
        except Exception as e:
            logger.error(f"Error during graceful shutdown: {e}")
        finally:
            # ìµœì¢… ì¢…ë£Œ ì¤€ë¹„
            logger.info("Exiting...")
            # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒœìŠ¤í¬ëŠ” ê°œë³„ ë§¤ë‹ˆì €ê°€ ì •ë¦¬í•¨. ì¼ê´„ ì·¨ì†ŒëŠ” í•˜ì§€ ì•ŠìŒ
            
            # sys.exit(0)ì€ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - asyncio.run()ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            # ëŒ€ì‹  ë£¨í”„ì—ì„œ ë‚˜ê°€ë„ë¡ í•¨
    
    def _detect_output_format_from_content(self, content: str, request: str) -> str:
        """ìƒì„±ëœ ë‚´ìš©ì„ ë³´ê³  íŒŒì¼ í˜•ì‹ ê²°ì • (ìµœì†Œí•œì˜ íŒ¨í„´ ë§¤ì¹­ë§Œ)."""
        if not content:
            return "md"
        
        # ì½”ë“œ ë¸”ë¡ ì œê±° í›„ ë‚´ìš© í™•ì¸
        content_clean = content.replace("```", "").strip()
        
        # Python ì½”ë“œ íŒ¨í„´
        if ("def " in content_clean[:1000] and "import " in content_clean[:1000]) or content.startswith("```python"):
            return "py"
        
        # Java ì½”ë“œ íŒ¨í„´
        if ("public class" in content_clean[:1000] or "public static void" in content_clean[:1000]) or content.startswith("```java"):
            return "java"
        
        # JavaScript ì½”ë“œ íŒ¨í„´
        if (("const " in content_clean[:1000] or "function " in content_clean[:1000]) and "console" in content_clean[:1000]) or content.startswith("```javascript"):
            return "js"
        
        # HTML íŒ¨í„´
        if content.strip().startswith("<!DOCTYPE") or content.strip().startswith("<html"):
            return "html"
        
        # ê¸°ë³¸: Markdown
        return "md"
    
    async def run_research(self, request: str, output_path: Optional[str] = None, 
                          streaming: bool = False, output_format: Optional[str] = None) -> Dict[str, Any]:
        """ì—°êµ¬ ì‹¤í–‰ - 8ê°€ì§€ í•µì‹¬ í˜ì‹  ì ìš©"""
        logger.info("ğŸ¤– Starting Autonomous Research System with 8 Core Innovations")
        logger.info("=" * 80)
        logger.info(f"Request: {request}")
        logger.info(f"Primary LLM: {self.config.llm.primary_model}")
        logger.info(f"Planning Model: {self.config.llm.planning_model}")
        logger.info(f"Reasoning Model: {self.config.llm.reasoning_model}")
        logger.info(f"Verification Model: {self.config.llm.verification_model}")
        logger.info(f"Self-planning: {self.config.agent.enable_self_planning}")
        logger.info(f"Agent Communication: {self.config.agent.enable_agent_communication}")
        logger.info(f"MCP Enabled: {self.config.mcp.enabled}")
        logger.info(f"Streaming Pipeline: {streaming}")
        logger.info(f"Adaptive Supervisor: {self.config.agent.max_concurrent_research_units}")
        logger.info(f"Hierarchical Compression: {self.config.compression.enabled}")
        logger.info(f"Continuous Verification: {self.config.verification.enabled}")
        logger.info(f"Adaptive Context Window: {self.config.context_window.enabled}")
        logger.info("=" * 80)
        
        try:
            # Start health monitoring
            await self.health_monitor.start_monitoring()
            
            # Initialize MCP client if enabled
            if self.config.mcp.enabled:
                try:
                    await self.mcp_hub.initialize_mcp()
                except asyncio.CancelledError:
                    # ì´ˆê¸°í™” ì¤‘ ì·¨ì†Œëœ ê²½ìš° - ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ ì¢…ë£Œ
                    logger.warning("MCP initialization was cancelled")
                    raise
            
            # Run research with production-grade reliability
            if streaming:
                result = await self._run_streaming_research(request)
            else:
                # Use new multi-agent orchestrator (no fallback - fail clearly)
                workflow_result = await self.orchestrator.execute(request)
                
                # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸ - fallback ì—†ì´ ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
                research_failed = workflow_result.get('research_failed', False)
                verification_failed = workflow_result.get('verification_failed', False)
                report_failed = workflow_result.get('report_failed', False)
                final_report = workflow_result.get("final_report", "")
                
                # ì‹¤íŒ¨ ë³´ê³ ì„œë„ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ (ë‚´ìš©ì´ "ì—°êµ¬ ì‹¤íŒ¨" ë˜ëŠ” "ì—°êµ¬ ì™„ë£Œ ë¶ˆê°€" í¬í•¨)
                is_failure_report = (
                    final_report and 
                    ("ì—°êµ¬ ì‹¤íŒ¨" in final_report or "ì—°êµ¬ ì™„ë£Œ ë¶ˆê°€" in final_report or "âŒ" in final_report)
                )
                
                if research_failed or verification_failed or report_failed or is_failure_report:
                    error_msg = workflow_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    if not error_msg or error_msg == 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜':
                        # ì‹¤íŒ¨ ë³´ê³ ì„œì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ì¶œ
                        if final_report and "ì˜¤ë¥˜ ë‚´ìš©" in final_report:
                            lines = final_report.split("\n")
                            for i, line in enumerate(lines):
                                if "ì˜¤ë¥˜ ë‚´ìš©" in line and i + 1 < len(lines):
                                    error_msg = lines[i + 1].strip()
                                    break
                        elif final_report and "âŒ" in final_report:
                            # ì‹¤íŒ¨ ë³´ê³ ì„œì—ì„œ ê°„ë‹¨íˆ ì¶”ì¶œ
                            if "ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨" in final_report:
                                error_msg = "ì—°êµ¬ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                            elif "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨" in final_report:
                                error_msg = "ë³´ê³ ì„œ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                            else:
                                error_msg = "ì—°êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
                    
                    failed_agent = workflow_result.get('current_agent', 'unknown')
                    session_id = workflow_result.get("session_id", 'N/A')
                    
                    logger.error(f"âŒ Research failed at {failed_agent}: {error_msg}")
                    
                    # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜ (ì‚¬ìš©ìê°€ ì¬ì‹œë„í•  ìˆ˜ ìˆë„ë¡)
                    result = {
                        "success": False,
                        "query": request,
                        "error": error_msg,
                        "failed_agent": failed_agent,
                        "session_id": session_id,
                        "content": final_report or f"ì—°êµ¬ ì‹¤íŒ¨: {error_msg}",
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {
                            "model_used": "multi-agent",
                            "execution_time": 0.0,
                            "cost": 0.0,
                            "confidence": 0.0,
                            "failed": True
                        },
                        "synthesis_results": {
                            "content": final_report or "",
                            "failed": True
                        },
                        "sources": [],
                        "innovation_stats": {"multi_agent_orchestration": "enabled"},
                        "system_health": {"overall_status": "unhealthy", "error": error_msg},
                        "retry_available": True
                    }
                    
                    logger.warning("âš ï¸ Research completed with errors - user can retry")
                else:
                    # ì„±ê³µ ê²°ê³¼
                    final_report = workflow_result.get("final_report", "")
                    if not final_report:
                        # ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                        result = {
                            "success": False,
                            "query": request,
                            "error": "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ìµœì¢… ë³´ê³ ì„œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                            "failed_agent": "generator",
                            "session_id": workflow_result.get("session_id", "N/A"),
                            "content": "ì—°êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                            "timestamp": datetime.now().isoformat(),
                            "metadata": {"failed": True, "confidence": 0.0},
                            "retry_available": True
                        }
                    else:
                        result = {
                            "success": True,
                            "query": request,
                            "content": final_report,
                            "timestamp": datetime.now().isoformat(),
                            "metadata": {
                                "model_used": "multi-agent",
                                "execution_time": 0.0,
                                "cost": 0.0,
                                "confidence": 0.9
                            },
                            "synthesis_results": {
                                "content": final_report
                            },
                            "sources": self._extract_sources_from_workflow(workflow_result),
                            "innovation_stats": {"multi_agent_orchestration": "enabled"},
                            "system_health": {"overall_status": "healthy"},
                            "session_id": workflow_result.get("session_id")
                        }
            
            # Apply hierarchical compression if enabled
            # Commented out to avoid serialization errors
            # if self.config.compression.enabled:
            #     result = await self._apply_hierarchical_compression(result)
            
            # Save results - LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            content = result.get('content', '') or result.get('synthesis_results', {}).get('content', '')
            
            if output_path:
                # ì‚¬ìš©ìê°€ ì§€ì •í•œ ê²½ë¡œ ì‚¬ìš©
                final_path = await self._save_content_as_file(content, output_path, result)
                logger.info(f"ğŸ“„ Results saved to: {final_path}")
            else:
                # ìƒì„±ëœ ë‚´ìš©ì„ ë³´ê³  í˜•ì‹ ê²°ì •
                detected_format = self._detect_output_format_from_content(content, request)
                if output_format is None:
                    output_format = detected_format
                
                output_dir = project_root / "output"
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # í™•ì¥ì ê²°ì • (ê°„ë‹¨í•˜ê²Œ)
                if output_format.startswith("."):
                    ext = output_format
                else:
                    ext_map = {"py": ".py", "java": ".java", "js": ".js", "html": ".html", "md": ".md", "pdf": ".pdf"}
                    ext = ext_map.get(output_format, ".md")
                default_output = output_dir / f"research_{timestamp}{ext}"
                
                final_path = await self._save_content_as_file(content, str(default_output), result)
                logger.info(f"ğŸ“„ Results saved to default location: {final_path} (format: {output_format})")
                self._display_results(result)
            
            # Get final health status
            health_status = self.health_monitor.get_system_health()
            result['system_health'] = health_status
            
            logger.info("âœ… Research completed successfully with 8 Core Innovations")
            return result
            
        except Exception as e:
            logger.error(f"Research failed with exception: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Get error health status
            error_health = self.health_monitor.get_system_health()
            logger.error(f"System health at failure: {error_health}")
            
            # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜ (ì˜ˆì™¸ ëŒ€ì‹ )
            result = {
                "success": False,
                "query": request,
                "error": f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}",
                "failed_agent": "system",
                "content": f"ì—°êµ¬ ì‹¤í–‰ ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "metadata": {
                    "model_used": "multi-agent",
                    "execution_time": 0.0,
                    "cost": 0.0,
                    "confidence": 0.0,
                    "failed": True
                },
                "synthesis_results": {"content": "", "failed": True},
                "sources": [],
                "innovation_stats": {},
                "system_health": error_health,
                "retry_available": True
            }
            
            # ì‹¤íŒ¨ ê²°ê³¼ë„ ì €ì¥
            if output_path:
                await self._save_results_incrementally(result, output_path, output_format)
            else:
                output_dir = project_root / "output"
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                default_output = output_dir / f"research_failed_{timestamp}.{output_format}"
                await self._save_results_incrementally(result, str(default_output), output_format)
                self._display_results(result)
            
            # ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ê²°ê³¼ ë°˜í™˜
            return result
    
    async def _run_streaming_research(self, request: str) -> Dict[str, Any]:
        """Run research with streaming pipeline (Innovation 5)."""
        logger.info("ğŸŒŠ Starting streaming research pipeline...")
        
        # Create streaming callback
        async def streaming_callback(partial_result: Dict[str, Any]):
            logger.info(f"ğŸ“Š Streaming partial result: {partial_result.get('type', 'unknown')}")
            # In a real implementation, this would send to web interface
            print(f"ğŸ“Š Partial Result: {partial_result.get('summary', 'Processing...')}")
        
        # Use standard run_research but with streaming callback simulation
        result = await self.orchestrator.run_research(user_request=request, context={})
        
        # Extract and format result
        final_synthesis = result.get("synthesis_results", {}).get("content", "")
        if not final_synthesis:
            final_synthesis = result.get("content", "")
        
        formatted_result = {
            "query": request,
            "content": final_synthesis or "Research completed",
            "timestamp": datetime.now().isoformat(),
            "metadata": result.get("metadata", {}),
            "synthesis_results": result.get("synthesis_results", {}),
            "sources": self._extract_sources(result),
            "innovation_stats": result.get("innovation_stats", {}),
            "system_health": result.get("system_health", {})
        }
        
        logger.info("âœ… Streaming research completed")
        return formatted_result
    
    def _extract_sources_from_workflow(self, workflow_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract sources from workflow results."""
        sources = []
        seen_urls = set()
        
        # Extract from verified results (ìš°ì„ )
        verified_results = workflow_result.get("verified_results", [])
        for result in verified_results:
            if isinstance(result, dict):
                url = result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    sources.append({
                        "title": result.get('title', ''),
                        "url": url,
                        "snippet": result.get('snippet', '')
                    })
        
        # Extract from research results (ë°±ì—…)
        research_results = workflow_result.get("research_results", [])
        for result in research_results:
            if isinstance(result, dict):
                url = result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    sources.append({
                        "title": result.get('title', ''),
                        "url": url,
                        "snippet": result.get('snippet', '')
                    })
            elif isinstance(result, str) and "Source:" in result:
                # Extract URL from result string (ë ˆê±°ì‹œ)
                parts = result.split("Source:")
                if len(parts) > 1:
                    url = parts[1].strip()
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        title = parts[0].split(":")[-1].strip() if ":" in parts[0] else ""
                        sources.append({
                            "title": title,
                            "url": url,
                            "snippet": ""
                        })
        
        return sources[:20]  # Limit to 20 sources
    
    def _extract_sources(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract sources from research results (legacy method)."""
        sources = []
        
        # Try to extract from execution results
        execution_results = result.get("detailed_results", {}).get("execution_results", [])
        for exec_result in execution_results:
            if isinstance(exec_result, dict):
                # Look for search results
                if "results" in exec_result:
                    sources.extend(exec_result["results"])
                elif "sources" in exec_result:
                    sources.extend(exec_result["sources"])
                elif "url" in exec_result:
                    sources.append(exec_result)
        
        # Deduplicate by URL
        seen_urls = set()
        unique_sources = []
        for source in sources:
            url = source.get("url") or source.get("link")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append({
                    "title": source.get("title", source.get("name", "")),
                    "url": url,
                    "snippet": source.get("snippet", source.get("summary", ""))
                })
        
        return unique_sources[:20]  # Limit to 20 sources
    
    async def _apply_hierarchical_compression(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Apply hierarchical compression (Innovation 2)."""
        if not self.config.compression.enabled:
            return result
        
        logger.info("ğŸ—œï¸ Applying hierarchical compression...")
        
        # Import compression module
        from src.core.compression import compress_data
        
        # Compress large text fields
        if 'synthesis_results' in result:
            compressed_synthesis = await compress_data(
                result['synthesis_results']
            )
            result['synthesis_results_compressed'] = compressed_synthesis
        
        logger.info("âœ… Hierarchical compression applied")
        return result
    
    async def _save_content_as_file(self, content: str, output_path: str, result: Dict[str, Any]) -> str:
        """LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ íŒŒì¼ë¡œ ì €ì¥ (í•˜ë“œì½”ë”© ì—†ì´)."""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì €ì¥ (ì¶”ê°€ í…œí”Œë¦¿ ì—†ì´)
        # ì†ŒìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ì¶”ê°€
        if result.get('sources'):
            sources_text = "\n\n## ì°¸ê³  ë¬¸í—Œ\n\n"
            for i, source in enumerate(result.get('sources', []), 1):
                sources_text += f"{i}. [{source.get('title', 'N/A')}]({source.get('url', '')})\n"
                if source.get('snippet'):
                    sources_text += f"   {source.get('snippet', '')[:200]}...\n"
                sources_text += "\n"
            content = content + sources_text
        
        output_file.write_text(content, encoding='utf-8')
        return str(output_file)
    
    async def _save_results_incrementally(self, result: Dict[str, Any], output_path: str, output_format: str = "json"):
        """Save results with incremental save (Innovation 5)."""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Save incrementally based on format
        temp_file = output_file.with_suffix('.tmp')
        
        if output_format.lower() == "json":
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
        elif output_format.lower() == "yaml":
            import yaml
            with open(temp_file, 'w', encoding='utf-8') as f:
                yaml.dump(result, f, default_flow_style=False, allow_unicode=True)
        elif output_format.lower() == "txt":
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(f"Research Results\n")
                f.write(f"===============\n\n")
                f.write(f"Query: {result.get('query', 'N/A')}\n")
                f.write(f"Timestamp: {result.get('timestamp', 'N/A')}\n\n")
                if 'content' in result:
                    f.write(f"Content:\n{result['content']}\n\n")
                elif 'synthesis_results' in result:
                    synthesis = result['synthesis_results']
                    if isinstance(synthesis, dict) and 'content' in synthesis:
                        f.write(f"Content:\n{synthesis['content']}\n\n")
                if 'sources' in result:
                    f.write(f"Sources:\n")
                    for i, source in enumerate(result['sources'], 1):
                        f.write(f"{i}. {source.get('title', 'N/A')} - {source.get('url', 'N/A')}\n")
        else:
            raise ValueError(f"Unsupported output format: {output_format}")
        
        # Atomic move
        temp_file.replace(output_file)
        
        logger.info(f"âœ… Results saved incrementally to: {output_file} (format: {output_format})")
    
    def _display_results(self, result: Dict[str, Any]):
        """Display results with enhanced formatting."""
        print("\nğŸ“‹ Research Results with 8 Core Innovations:")
        print("=" * 80)
        
        # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸ ë° í‘œì‹œ
        if not result.get('success', True):
            print("\nâŒ ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨")
            print("=" * 80)
            print(f"\nì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            print(f"ì‹¤íŒ¨ ë‹¨ê³„: {result.get('failed_agent', 'unknown')}")
            print(f"\nì„¸ì…˜ ID: {result.get('session_id', 'N/A')}")
            print("\nì¬ì‹œë„ ë°©ë²•:")
            print("1. ê°™ì€ ì¿¼ë¦¬ë¡œ ë‹¤ì‹œ ì‹œë„: python main.py --request 'YOUR_QUERY'")
            print("2. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„")
            print("3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
            print("=" * 80)
            return
        
        # Display main research content
        if 'content' in result and result['content']:
            print("\nğŸ“ Research Content:")
            print("-" * 60)
            print(result['content'])
            print("-" * 60)
        elif 'synthesis_results' in result:
            synthesis = result['synthesis_results']
            if isinstance(synthesis, dict) and 'content' in synthesis:
                print("\nğŸ“ Research Content:")
                print("-" * 60)
                print(synthesis['content'])
                print("-" * 60)
            else:
                print(f"\nğŸ“ Synthesis: {synthesis}")
        else:
            print("\nâŒ No research content found in results")
        
        # Display research metadata
        if 'metadata' in result:
            metadata = result['metadata']
            print(f"\nğŸ“Š Research Metadata:")
            print(f"  â€¢ Model Used: {metadata.get('model_used', 'N/A')}")
            print(f"  â€¢ Execution Time: {metadata.get('execution_time', 'N/A'):.2f}s")
            print(f"  â€¢ Cost: ${metadata.get('cost', 0):.4f}")
            print(f"  â€¢ Confidence: {metadata.get('confidence', 'N/A')}")
        
        # Display synthesis results
        if 'synthesis_results' in result and isinstance(result['synthesis_results'], dict):
            synthesis = result['synthesis_results']
            if 'synthesis_results' in synthesis:
                print(f"\nğŸ“ Synthesis: {synthesis.get('synthesis_results', 'N/A')}")
        
        # Display innovation stats
        if 'innovation_stats' in result:
            stats = result['innovation_stats']
            print(f"\nğŸš€ Innovation Statistics:")
            print(f"  â€¢ Adaptive Supervisor: {stats.get('adaptive_supervisor', 'N/A')}")
            print(f"  â€¢ Hierarchical Compression: {stats.get('hierarchical_compression', 'N/A')}")
            print(f"  â€¢ Multi-Model Orchestration: {stats.get('multi_model_orchestration', 'N/A')}")
            print(f"  â€¢ Continuous Verification: {stats.get('continuous_verification', 'N/A')}")
            print(f"  â€¢ Streaming Pipeline: {stats.get('streaming_pipeline', 'N/A')}")
            print(f"  â€¢ Universal MCP Hub: {stats.get('universal_mcp_hub', 'N/A')}")
            print(f"  â€¢ Adaptive Context Window: {stats.get('adaptive_context_window', 'N/A')}")
            print(f"  â€¢ Production-Grade Reliability: {stats.get('production_grade_reliability', 'N/A')}")
        
        # Display system health
        if 'system_health' in result:
            health = result['system_health']
            print(f"\nğŸ¥ System Health: {health.get('overall_status', 'Unknown')}")
            print(f"  â€¢ Health Score: {health.get('health_score', 'N/A')}")
            print(f"  â€¢ Monitoring Active: {health.get('monitoring_active', 'N/A')}")
        
        print("=" * 80)
    
    async def run_mcp_server(self):
        """MCP ì„œë²„ ì‹¤í–‰"""
        await self.mcp_hub.initialize_mcp()
    
    async def run_mcp_client(self):
        """MCP í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰"""
        await self.mcp_hub.initialize_mcp()
    
    def run_web_app(self):
        """ì›¹ ì•± ì‹¤í–‰"""
        return self.web_manager.start_web_app()
    
    async def run_health_check(self):
        """Run comprehensive health check for all system components."""
        logger.info("ğŸ¥ Running comprehensive health check...")
        
        # Check MCP tools health
        if self.config.mcp.enabled:
            # MCP Hub health check
            logger.info("MCP Hub initialized and ready")
        
        # Check system health
        system_health = self.health_monitor.get_system_health()
        logger.info(f"System Health: {system_health.get('overall_status', 'Unknown')}")
        
        # Check web app health
        web_health = await self.web_manager.get_web_app_health()
        logger.info(f"Web App Health: {web_health.get('status', 'Unknown')}")
        
        logger.info("âœ… Health check completed")
    
    async def check_mcp_servers(self):
        """MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸."""
        logger.info("ğŸ“Š Checking MCP server connections...")
        
        if not self.config.mcp.enabled:
            logger.warning("MCP is disabled")
            return
        
        try:
            # MCP Hub ì´ˆê¸°í™” í™•ì¸ - ì´ë¯¸ ì—°ê²°ëœ ì„œë²„ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if self.mcp_hub.mcp_sessions:
                logger.info(f"Found {len(self.mcp_hub.mcp_sessions)} existing MCP server connections")
            else:
                logger.info("No existing connections. Will attempt quick connection tests for each server...")
            
            # ì„œë²„ ìƒíƒœ í™•ì¸ (ê° ì„œë²„ì— ëŒ€í•´ ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì—°ê²° ì‹œë„)
            logger.info("Checking MCP server connection status...")
            server_status = await self.mcp_hub.check_mcp_servers()
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "=" * 80)
            print("ğŸ“Š MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸")
            print("=" * 80)
            print(f"ì „ì²´ ì„œë²„ ìˆ˜: {server_status['total_servers']}")
            print(f"ì—°ê²°ëœ ì„œë²„: {server_status['connected_servers']}")
            print(f"ì—°ê²°ë¥ : {server_status['summary']['connection_rate']}")
            print(f"ì „ì²´ ì‚¬ìš© ê°€ëŠ¥í•œ Tool ìˆ˜: {server_status['summary']['total_tools_available']}")
            print("\n")
            
            for server_name, info in server_status["servers"].items():
                status_icon = "âœ…" if info["connected"] else "âŒ"
                print(f"{status_icon} ì„œë²„: {server_name}")
                print(f"   íƒ€ì…: {info['type']}")
                
                if info["type"] == "http":
                    print(f"   URL: {info.get('url', 'unknown')}")
                else:
                    cmd = info.get('command', 'unknown')
                    args_preview = ' '.join(info.get('args', [])[:3])
                    print(f"   ëª…ë ¹ì–´: {cmd} {args_preview}...")
                
                print(f"   ì—°ê²° ìƒíƒœ: {'ì—°ê²°ë¨' if info['connected'] else 'ì—°ê²° ì•ˆ ë¨'}")
                print(f"   ì œê³µ Tool ìˆ˜: {info['tools_count']}")
                
                if info["tools"]:
                    print(f"   Tool ëª©ë¡:")
                    for tool in info["tools"][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                        registered_name = f"{server_name}::{tool}"
                        print(f"     - {registered_name}")
                    if len(info["tools"]) > 5:
                        print(f"     ... ë° {len(info['tools']) - 5}ê°œ ë”")
                
                if info.get("error"):
                    print(f"   âš ï¸ ì˜¤ë¥˜: {info['error']}")
                print()
            
            print("=" * 80)
            
            # ìš”ì•½ ì •ë³´ ë¡œê¹…
            logger.info(f"MCP ì„œë²„ í™•ì¸ ì™„ë£Œ: {server_status['summary']['connection_rate']} ì—°ê²°")
            
        except Exception as e:
            logger.error(f"MCP ì„œë²„ í™•ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())


async def main():
    """Main function - 8ê°€ì§€ í•µì‹¬ í˜ì‹  í†µí•© ì‹¤í–‰ ì§„ì…ì """
    # Python ì¢…ë£Œ ì‹œ ë°œìƒí•˜ëŠ” async generator ì •ë¦¬ ì˜¤ë¥˜ ë¬´ì‹œ
    def ignore_async_gen_errors(loop, context):
        """anyio cancel scope ë° async generator ì¢…ë£Œ ì˜¤ë¥˜ ë¬´ì‹œ"""
        exception = context.get('exception')
        if exception:
            error_msg = str(exception)
            # anyio cancel scope ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
            if isinstance(exception, RuntimeError) and ("cancel scope" in error_msg.lower() or "different task" in error_msg.lower()):
                return  # ë¬´ì‹œ
            # async generator ì¢…ë£Œ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
            if isinstance(exception, GeneratorExit) or "async_generator" in error_msg.lower():
                return  # ë¬´ì‹œ
        # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ê¸°ë³¸ handlerë¡œ ì „ë‹¬
        loop.set_exception_handler(None)
        loop.call_exception_handler(context)
        loop.set_exception_handler(ignore_async_gen_errors)
    
    # asyncio exception handler ì„¤ì •
    loop = asyncio.get_running_loop()
    loop.set_exception_handler(ignore_async_gen_errors)
    
    parser = argparse.ArgumentParser(
        description="Autonomous Multi-Agent Research System with 8 Core Innovations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py --request "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ ì „ë§"
  python main.py --request "ì—°êµ¬ ì£¼ì œ" --output results/report.json
  python main.py --request "ì—°êµ¬ ì£¼ì œ" --streaming
  python main.py --web
  python main.py --mcp-server
  python main.py --mcp-client
  python main.py --health-check
        """
    )
    
    # Mode selection
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument("--request", help="Research request (CLI mode)")
    mode_group.add_argument("--web", action="store_true", help="Start web application with streaming")
    mode_group.add_argument("--mcp-server", action="store_true", help="Start MCP server with Universal MCP Hub")
    mode_group.add_argument("--mcp-client", action="store_true", help="Start MCP client with Smart Tool Selection")
    mode_group.add_argument("--health-check", action="store_true", help="Check system health and MCP tools")
    mode_group.add_argument("--check-mcp-servers", action="store_true", help="Check all MCP server connections and list tools")
    
    # Optional arguments
    parser.add_argument("--output", help="Output file path for research results")
    parser.add_argument("--config", help="Configuration file path")
    parser.add_argument("--format", choices=["json", "yaml", "txt"], default="json", help="Output format for research results")
    parser.add_argument("--streaming", action="store_true", help="Enable streaming pipeline for real-time results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Create logs directory
    logs_dir = project_root / "logs"
    logs_dir.mkdir(exist_ok=True)

    # Initialize enhanced systems
    from src.utils.output_manager import UserCenteredOutputManager, set_output_manager
    from src.core.error_handler import ErrorHandler, set_error_handler
    from src.core.progress_tracker import ProgressTracker, set_progress_tracker

    # ì¶œë ¥ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    output_manager = UserCenteredOutputManager(
        output_level=UserCenteredOutputManager.OutputLevel.USER,
        output_format=UserCenteredOutputManager.OutputFormat.TEXT,
        enable_colors=True,
        stream_output=True,
        show_progress=True
    )
    set_output_manager(output_manager)

    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
    error_handler = ErrorHandler(
        log_errors=True,
        enable_recovery=True
    )
    set_error_handler(error_handler)

    # ì§„í–‰ ìƒí™© ì¶”ì ê¸° ì´ˆê¸°í™” (ì„¸ì…˜ë³„ë¡œ ìƒì„±)
    session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    progress_tracker = ProgressTracker(
        session_id=session_id,
        enable_real_time_updates=True,
        update_interval=1.0
    )
    set_progress_tracker(progress_tracker)

    # ì§„í–‰ ìƒí™© ì¶”ì ê¸° ì½œë°± ì„¤ì • (ì¶œë ¥ ë§¤ë‹ˆì €ì™€ ì—°ë™)
    async def progress_callback(workflow_progress):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹œ ì¶œë ¥ ë§¤ë‹ˆì €ì— í‘œì‹œ."""
        try:
            progress_pct = int(workflow_progress.overall_progress * 100)
            stage_name = workflow_progress.current_stage.value

            eta_str = ""
            if workflow_progress.estimated_completion:
                eta_seconds = max(0, int(workflow_progress.estimated_completion - time.time()))
                eta_str = f" (ì˜ˆìƒ {eta_seconds}ì´ˆ ë‚¨ìŒ)"

            message = f"ğŸ“Š ì§„í–‰ë¥ : {progress_pct}% - {stage_name.upper()}{eta_str}"

            # ì§„í–‰ë¥  ë°” ìŠ¤íƒ€ì¼ë¡œ í‘œì‹œ
            await output_manager.start_progress(
                stage_name,
                100,
                f"{progress_pct}% ì™„ë£Œ",
                workflow_progress.estimated_completion
            )
            await output_manager.update_progress(progress_pct)

        except Exception as e:
            logger.warning(f"Progress callback failed: {e}")

    progress_tracker.add_progress_callback(progress_callback)

    # Initialize system
    system = AutonomousResearchSystem()
    
    try:
        if args.request:
            # CLI Research Mode with 8 innovations
            logger.info("ğŸš€ Starting Local Researcher with enhanced systems...")

            # ì§„í–‰ ìƒí™© ì¶”ì  ì‹œì‘
            await progress_tracker.start_tracking()

            # ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì•Œë¦¼
            await output_manager.output(
                f"ğŸ”¬ ì—°êµ¬ ì£¼ì œ: {args.request}",
                level=output_manager.OutputLevel.USER
            )
            await output_manager.output(
                "ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶”ì  ë° í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.",
                level=output_manager.OutputLevel.SERVICE
            )

            # ì—°êµ¬ ì‹¤í–‰
            await system.run_research(
                args.request,
                args.output,
                streaming=args.streaming,
                output_format=args.format
            )
            
        elif args.web:
            # Web Application Mode with Streaming Pipeline
            system.run_web_app()
            
        elif args.mcp_server:
            # MCP Server Mode with Universal MCP Hub - ì‹¤ì œ ì—°ê²° ìˆ˜í–‰
            logger.info("Initializing MCP servers...")
            try:
                await system.mcp_hub.initialize_mcp()
                logger.info("âœ… MCP servers initialized")
                
                # ì—°ê²°ëœ ì„œë²„ ìƒíƒœ ì¶œë ¥
                if system.mcp_hub.mcp_sessions:
                    print("\n" + "=" * 80)
                    print("âœ… MCP ì„œë²„ ì—°ê²° ì™„ë£Œ")
                    print("=" * 80)
                    for server_name in system.mcp_hub.mcp_sessions.keys():
                        tools_count = len(system.mcp_hub.mcp_tools_map.get(server_name, {}))
                        print(f"âœ… {server_name}: {tools_count} tools available")
                    print("=" * 80)
                    print("\nMCP Hub is running. Press Ctrl+C to stop.")
                    
                    # ê³„ì† ì‹¤í–‰ ëŒ€ê¸°
                    try:
                        await asyncio.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸° (ë˜ëŠ” Ctrl+Cë¡œ ì¢…ë£Œ)
                    except KeyboardInterrupt:
                        logger.info("Shutting down MCP Hub...")
                else:
                    logger.warning("âš ï¸ No MCP servers connected")
                    sys.exit(1)
            except Exception as e:
                logger.error(f"Failed to initialize MCP servers: {e}")
                sys.exit(1)
            
        elif args.mcp_client:
            # MCP Client Mode with Smart Tool Selection
            success = await system.run_mcp_client()
            if not success:
                sys.exit(1)
            
        elif args.health_check:
            # Health Check Mode
            await system.run_health_check()
        
        elif args.check_mcp_servers:
            # MCP ì„œë²„ í™•ì¸ ëª¨ë“œ
            await system.check_mcp_servers()
            
    except KeyboardInterrupt:
        logger.info("Operation cancelled by user (KeyboardInterrupt)")
        system._shutdown_requested = True
        try:
            await system._graceful_shutdown()
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
        # sys.exit(0) ì œê±° - asyncio.run()ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    except asyncio.CancelledError:
        # ì·¨ì†Œëœ ê²½ìš° ì •ë¦¬ í›„ ì¢…ë£Œ
        logger.info("Operation cancelled")
        system._shutdown_requested = True
        try:
            await system._graceful_shutdown()
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
        # asyncio.CancelledErrorëŠ” ë‹¤ì‹œ raiseí•˜ì—¬ ì •ìƒì ì¸ ì·¨ì†Œ íë¦„ ìœ ì§€
        raise
    except Exception as e:
        # í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬
        from src.core.error_handler import ErrorContext, ErrorCategory
        import traceback

        error_context = ErrorContext(
            component="main",
            operation="run_research" if args.request else "system_operation",
            session_id=session_id
        )

        await error_handler.handle_error(
            e,
            category=ErrorCategory.UNKNOWN,
            severity=error_handler.ErrorSeverity.HIGH,
            context=error_context,
            custom_message=f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

        system._shutdown_requested = True
        try:
            await system._graceful_shutdown()
        except Exception as e2:
            logger.error(f"Error during shutdown: {e2}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ ì½”ë“œ 1ë¡œ ì¢…ë£Œ
        sys.exit(1)
    finally:
        # ì§„í–‰ ìƒí™© ì¶”ì  ì¤‘ì§€ ë° ìš”ì•½ ì¶œë ¥
        try:
            await progress_tracker.stop_tracking()

            if args.request:
                # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ìš”ì•½
                await output_manager.complete_progress(success=True)
                await output_manager.output_workflow_summary()

                # ì§„í–‰ ìƒí™© í†µê³„ ì¶œë ¥
                stats = progress_tracker.get_statistics()
                await output_manager.output(
                    f"ğŸ“ˆ ì„¸ì…˜ í†µê³„: {stats['total_agents_created']}ê°œ ì—ì´ì „íŠ¸ ìƒì„±, "
                    f"{stats['agents_completed']}ê°œ ì™„ë£Œ, {stats['agents_failed']}ê°œ ì‹¤íŒ¨",
                    level=output_manager.OutputLevel.SERVICE
                )

        except Exception as e:
            logger.warning(f"Failed to finalize progress tracking: {e}")

        # ìµœì¢… ì •ë¦¬ ë³´ì¥
        if hasattr(system, 'mcp_hub') and system.mcp_hub and hasattr(system.mcp_hub, 'mcp_sessions'):
            try:
                if system.mcp_hub.mcp_sessions:
                    logger.info("Final cleanup of MCP connections...")
                    await system.mcp_hub.cleanup()
            except Exception as e:
                logger.debug(f"Error in final cleanup: {e}")


if __name__ == "__main__":
    asyncio.run(main())