"""
Agent Orchestrator for Multi-Agent System

LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ
4ëŒ€ í•µì‹¬ ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨í•˜ì—¬ í˜‘ì—… ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Literal, Annotated
from datetime import datetime
from dataclasses import dataclass, field
import operator

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from src.core.shared_memory import get_shared_memory, MemoryScope
from src.core.skills_manager import get_skill_manager
from src.core.skills_selector import get_skill_selector, SkillMatch
from src.core.skills_loader import Skill
from src.core.agent_result_sharing import SharedResultsManager, AgentDiscussionManager
from src.core.researcher_config import get_agent_config

logger = logging.getLogger(__name__)

# Loggerê°€ handlerê°€ ì—†ìœ¼ë©´ root loggerì˜ handler ì‚¬ìš©
if not logger.handlers:
    logger.setLevel(logging.INFO)
    # Root loggerì˜ handler ì‚¬ìš© (main.pyì—ì„œ ì„¤ì •ëœ handler)
    parent_logger = logging.getLogger()
    if parent_logger.handlers:
        logger.handlers = parent_logger.handlers
        logger.propagate = True
    else:
        # Fallback: ê¸°ë³¸ handler ì„¤ì •
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)


###################
# State Definitions
###################

def override_reducer(current_value, new_value):
    """Reducer function that allows overriding values in state."""
    if isinstance(new_value, dict) and new_value.get("type") == "override":
        return new_value.get("value", new_value)
    else:
        return operator.add(current_value, new_value)


class AgentState(TypedDict):
    """Main agent state containing messages and research data."""
    
    messages: Annotated[list, add_messages]
    user_query: str
    research_plan: Optional[str]
    research_tasks: Annotated[list, override_reducer]  # List of research tasks for parallel execution
    research_results: Annotated[list, override_reducer]  # Changed: supports both dict and str
    verified_results: Annotated[list, override_reducer]  # Changed: supports both dict and str
    final_report: Optional[str]
    current_agent: Optional[str]
    iteration: int
    session_id: Optional[str]
    research_failed: bool
    verification_failed: bool
    report_failed: bool
    error: Optional[str]


###################
# Agent Definitions
###################

@dataclass
class AgentContext:
    """Agent execution context."""
    agent_id: str
    session_id: str
    shared_memory: Any
    config: Any = None
    shared_results_manager: Optional[SharedResultsManager] = None
    discussion_manager: Optional[AgentDiscussionManager] = None


class PlannerAgent:
    """Planner agent - creates research plans (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "planner"
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("research_planner")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a research planning agent."
    
    async def execute(self, state: AgentState) -> AgentState:
        """Execute planning task with Skills-based instruction and detailed logging."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting research planning")
        logger.info(f"Query: {state['user_query']}")
        logger.info(f"Session: {state['session_id']}")
        logger.info(f"=" * 80)
        
        # Read from shared memory
        memory = self.context.shared_memory
        previous_plans = memory.search(state['user_query'], limit=3)
        
        logger.info(f"[{self.name}] Previous plans found: {len(previous_plans) if previous_plans else 0}")
        
        # Skills-based instruction ì‚¬ìš©
        instruction = self.instruction if self.skill else "You are a research planning agent."
        
        logger.info(f"[{self.name}] Using skill: {self.skill is not None}")
        
        # LLM í˜¸ì¶œì€ llm_managerë¥¼ í†µí•´ Gemini ì§ê²° ì‚¬ìš©
        from src.core.llm_manager import execute_llm_task, TaskType
        
        # Use Skills instruction
        prompt = f"""{instruction}

Task: Create a detailed research plan for: {state['user_query']}

Based on previous research:
{previous_plans if previous_plans else "No previous research found"}

Create a comprehensive research plan with:
1. Research objectives
2. Key areas to investigate
3. Expected sources and methods
4. Success criteria

Keep it concise and actionable (max 300 words)."""

        logger.info(f"[{self.name}] Calling LLM for planning...")
        # Gemini ì‹¤í–‰
        model_result = await execute_llm_task(
            prompt=prompt,
            task_type=TaskType.PLANNING,
            model_name=None,
            system_message=None
        )
        plan = model_result.content or 'No plan generated'
        
        logger.info(f"[{self.name}] âœ… Plan generated: {len(plan)} characters")
        logger.info(f"[{self.name}] Plan preview: {plan[:200]}...")
        
        state['research_plan'] = plan
        
        # ì‘ì—… ë¶„í• : ì—°êµ¬ ê³„íšì„ ì—¬ëŸ¬ ë…ë¦½ì ì¸ ì‘ì—…ìœ¼ë¡œ ë¶„í• 
        logger.info(f"[{self.name}] Splitting research plan into parallel tasks...")
        
        task_split_prompt = f"""ì—°êµ¬ ê³„íš:
{plan}

ì›ë˜ ì§ˆë¬¸: {state['user_query']}

ìœ„ ì—°êµ¬ ê³„íšì„ ë¶„ì„í•˜ì—¬ ì—¬ëŸ¬ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì—°êµ¬ ì‘ì—…ìœ¼ë¡œ ë¶„í• í•˜ì„¸ìš”.
ê° ì‘ì—…ì€ ë³„ë„ì˜ ì—°êµ¬ì(ExecutorAgent)ê°€ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "tasks": [
    {{
      "task_id": "task_1",
      "description": "ì‘ì—… ì„¤ëª…",
      "search_queries": ["ê²€ìƒ‰ ì¿¼ë¦¬ 1", "ê²€ìƒ‰ ì¿¼ë¦¬ 2"],
      "priority": 1,
      "estimated_time": "medium",
      "dependencies": []
    }},
    ...
  ]
}}

ê° ì‘ì—…ì€:
- ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•¨
- ëª…í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í¬í•¨í•´ì•¼ í•¨
- ìš°ì„ ìˆœìœ„ì™€ ì˜ˆìƒ ì‹œê°„ì„ í¬í•¨í•´ì•¼ í•¨
- ì˜ì¡´ì„±ì´ ì—†ì–´ì•¼ í•¨ (ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•´)

ì‘ì—… ìˆ˜: 3-5ê°œ ê¶Œì¥"""

        try:
            task_split_result = await execute_llm_task(
                prompt=task_split_prompt,
                task_type=TaskType.PLANNING,
                model_name=None,
                system_message="You are a task decomposition agent. Split research plans into independent parallel tasks."
            )
            
            task_split_text = task_split_result.content or ""
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            import re
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'\{[\s\S]*\}', task_split_text)
            if json_match:
                task_split_json = json.loads(json_match.group())
                tasks = task_split_json.get('tasks', [])
            else:
                # JSONì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì‘ì—… ì¶”ì¶œ ì‹œë„
                tasks = []
                lines = task_split_text.split('\n')
                current_task = None
                for line in lines:
                    line = line.strip()
                    if 'task_id' in line.lower() or 'task' in line.lower() and ':' in line:
                        if current_task:
                            tasks.append(current_task)
                        task_id_match = re.search(r'task[_\s]*(\d+)', line, re.IGNORECASE)
                        task_id = f"task_{task_id_match.group(1) if task_id_match else len(tasks) + 1}"
                        current_task = {
                            "task_id": task_id,
                            "description": "",
                            "search_queries": [],
                            "priority": len(tasks) + 1,
                            "estimated_time": "medium",
                            "dependencies": []
                        }
                    elif current_task:
                        if 'description' in line.lower() or 'ì„¤ëª…' in line:
                            desc_match = re.search(r':\s*(.+)', line)
                            if desc_match:
                                current_task["description"] = desc_match.group(1).strip()
                        elif 'query' in line.lower() or 'ì¿¼ë¦¬' in line:
                            query_match = re.search(r':\s*(.+)', line)
                            if query_match:
                                current_task["search_queries"].append(query_match.group(1).strip())
                
                if current_task:
                    tasks.append(current_task)
            
            # ì‘ì—…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ì—… ìƒì„±
            if not tasks:
                logger.warning(f"[{self.name}] Failed to parse tasks, creating default task")
                tasks = [{
                    "task_id": "task_1",
                    "description": state['user_query'],
                    "search_queries": [state['user_query']],
                    "priority": 1,
                    "estimated_time": "medium",
                    "dependencies": []
                }]
            
            # ê° ì‘ì—…ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, task in enumerate(tasks):
                if 'task_id' not in task:
                    task['task_id'] = f"task_{i + 1}"
                if 'description' not in task:
                    task['description'] = state['user_query']
                if 'search_queries' not in task or not task['search_queries']:
                    task['search_queries'] = [state['user_query']]
                if 'priority' not in task:
                    task['priority'] = i + 1
                if 'estimated_time' not in task:
                    task['estimated_time'] = "medium"
                if 'dependencies' not in task:
                    task['dependencies'] = []
            
            state['research_tasks'] = tasks
            logger.info(f"[{self.name}] âœ… Split research plan into {len(tasks)} parallel tasks")
            for task in tasks:
                logger.info(f"[{self.name}]   - {task.get('task_id')}: {task.get('description', '')[:50]}... ({len(task.get('search_queries', []))} queries)")
                
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Failed to split tasks: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ì—… ìƒì„±
            state['research_tasks'] = [{
                "task_id": "task_1",
                "description": state['user_query'],
                "search_queries": [state['user_query']],
                "priority": 1,
                "estimated_time": "medium",
                "dependencies": []
            }]
            logger.warning(f"[{self.name}] Using default single task")
        
        state['current_agent'] = self.name
        
        # Write to shared memory
        memory.write(
            key=f"plan_{state['session_id']}",
            value=plan,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        memory.write(
            key=f"tasks_{state['session_id']}",
            value=state['research_tasks'],
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Plan and tasks saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class ExecutorAgent:
    """Executor agent - executes research tasks using tools (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "executor"
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("research_executor")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a research execution agent."
    
    async def execute(self, state: AgentState, assigned_task: Optional[Dict[str, Any]] = None) -> AgentState:
        """Execute research tasks with detailed logging."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting research execution")
        logger.info(f"Agent ID: {self.context.agent_id}")
        logger.info(f"Query: {state['user_query']}")
        logger.info(f"Session: {state['session_id']}")
        logger.info(f"=" * 80)
        
        # ì‘ì—… í• ë‹¹: assigned_taskê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ stateì—ì„œ ì°¾ê¸°
        if assigned_task is None:
            # state['research_tasks']ì—ì„œ ì´ ì—ì´ì „íŠ¸ì—ê²Œ í• ë‹¹ëœ ì‘ì—… ì°¾ê¸°
            tasks = state.get('research_tasks', [])
            if tasks:
                # agent_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—… í• ë‹¹ (ë¼ìš´ë“œë¡œë¹ˆ)
                agent_id = self.context.agent_id
                if agent_id.startswith("executor_"):
                    try:
                        agent_index = int(agent_id.split("_")[1])
                        if agent_index < len(tasks):
                            assigned_task = tasks[agent_index]
                            logger.info(f"[{self.name}] Assigned task {assigned_task.get('task_id', 'unknown')} to {agent_id}")
                        else:
                            # ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì²« ë²ˆì§¸ ì‘ì—… í• ë‹¹
                            assigned_task = tasks[0]
                            logger.info(f"[{self.name}] Agent index out of range, using first task")
                    except (ValueError, IndexError):
                        assigned_task = tasks[0] if tasks else None
                        logger.info(f"[{self.name}] Using first task (fallback)")
                else:
                    # agent_idê°€ executor_ í˜•ì‹ì´ ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ ì‘ì—… ì‚¬ìš©
                    assigned_task = tasks[0] if tasks else None
            else:
                # ì‘ì—…ì´ ì—†ìœ¼ë©´ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
                memory = self.context.shared_memory
                tasks = memory.read(
                    key=f"tasks_{state['session_id']}",
                    scope=MemoryScope.SESSION,
                    session_id=state['session_id']
                ) or []
                if tasks:
                    assigned_task = tasks[0] if tasks else None
        
        # Read plan from shared memory
        memory = self.context.shared_memory
        plan = memory.read(
            key=f"plan_{state['session_id']}",
            scope=MemoryScope.SESSION,
            session_id=state['session_id']
        )
        
        logger.info(f"[{self.name}] Research plan loaded: {plan is not None}")
        if plan:
            logger.info(f"[{self.name}] Plan preview: {plan[:200]}...")
        
        # ì‹¤ì œ ì—°êµ¬ ì‹¤í–‰ - MCP Hubë¥¼ í†µí•œ ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰
        query = state['user_query']
        results = []
        
        try:
            # MCP Hub ì´ˆê¸°í™” í™•ì¸
            from src.core.mcp_integration import get_mcp_hub, execute_tool, ToolCategory
            
            hub = get_mcp_hub()
            logger.info(f"[{self.name}] MCP Hub status: {len(hub.mcp_sessions) if hub.mcp_sessions else 0} servers connected")
            
            if not hub.mcp_sessions:
                logger.info(f"[{self.name}] Initializing MCP Hub...")
                await hub.initialize_mcp()
                logger.info(f"[{self.name}] MCP Hub initialized: {len(hub.mcp_sessions)} servers")
            
            # ì‘ì—… í• ë‹¹ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì‘ì—…ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš©
            search_queries = []
            if assigned_task:
                search_queries = assigned_task.get('search_queries', [])
                logger.info(f"[{self.name}] Using assigned task queries: {len(search_queries)} queries from task {assigned_task.get('task_id', 'unknown')}")
            
            # ì‘ì—… í• ë‹¹ì´ ì—†ê±°ë‚˜ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            if not search_queries:
                search_queries = [query]  # ê¸°ë³¸ ì¿¼ë¦¬
                if plan:
                    # LLMìœ¼ë¡œ ì—°êµ¬ ê³„íšì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ
                    query_generation_prompt = f"""ì—°êµ¬ ê³„íš:
{plan}

ì›ë˜ ì§ˆë¬¸: {query}

ìœ„ ì—°êµ¬ ê³„íšì„ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ì— ì‚¬ìš©í•  êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ 3-5ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì¸¡ë©´ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤.
ì‘ë‹µ í˜•ì‹: ê° ì¤„ì— í•˜ë‚˜ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ì‘ì„±í•˜ì„¸ìš”. ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì—†ì´ ì¿¼ë¦¬ë§Œ ì‘ì„±í•˜ì„¸ìš”."""
                
                try:
                    query_result = await execute_llm_task(
                        prompt=query_generation_prompt,
                        task_type=TaskType.PLANNING,
                        model_name=None,
                        system_message="You are a research query generator. Generate specific search queries based on research plans."
                    )
                    
                    generated_queries = query_result.content or ""
                    # ê° ì¤„ì„ ì¿¼ë¦¬ë¡œ íŒŒì‹±
                    for line in generated_queries.split('\n'):
                        line = line.strip()
                        if line and not line.startswith('#') and len(line) > 5:
                            search_queries.append(line)
                    
                    # ì¤‘ë³µ ì œê±°
                    search_queries = list(dict.fromkeys(search_queries))[:5]  # ìµœëŒ€ 5ê°œ
                    logger.info(f"[{self.name}] Generated {len(search_queries)} search queries from plan")
                except Exception as e:
                    logger.warning(f"[{self.name}] Failed to generate search queries from plan: {e}, using original query only")
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            logger.info(f"[{self.name}] Executing {len(search_queries)} searches in parallel...")
            
            async def execute_single_search(search_query: str, query_index: int) -> Dict[str, Any]:
                """ë‹¨ì¼ ê²€ìƒ‰ ì‹¤í–‰."""
                try:
                    logger.info(f"[{self.name}] Search {query_index + 1}/{len(search_queries)}: '{search_query}'")
                    search_result = await execute_tool(
                        "g-search",
                        {"query": search_query, "max_results": 10}
                    )
                    return {
                        "query": search_query,
                        "index": query_index,
                        "result": search_result,
                        "success": search_result.get('success', False)
                    }
                except Exception as e:
                    logger.error(f"[{self.name}] Search {query_index + 1} failed: {e}")
                    return {
                        "query": search_query,
                        "index": query_index,
                        "result": {"success": False, "error": str(e)},
                        "success": False
                    }
            
            # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            search_tasks = [execute_single_search(q, i) for i, q in enumerate(search_queries)]
            search_results_list = await asyncio.gather(*search_tasks)
            
            logger.info(f"[{self.name}] âœ… Completed {len(search_results_list)} parallel searches")
            
            # ëª¨ë“  ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
            successful_results = [sr for sr in search_results_list if sr.get('success') and sr.get('result', {}).get('data')]
            
            if not successful_results:
                error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                logger.error(f"[{self.name}] âŒ {error_msg}")
                raise RuntimeError(error_msg)
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•© (í•˜ë“œì½”ë”© ì œê±°, ë™ì  í†µí•©)
            all_search_data = []
            for sr in successful_results:
                result_data = sr['result'].get('data', {})
                if isinstance(result_data, dict):
                    items = result_data.get('results', result_data.get('items', []))
                    if isinstance(items, list):
                        all_search_data.extend(items)
                elif isinstance(result_data, list):
                    all_search_data.extend(result_data)
            
            # í†µí•©ëœ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            search_result = {
                'success': True,
                'data': {
                    'results': all_search_data,
                    'total_results': len(all_search_data),
                    'source': 'parallel_search'
                }
            }
            
            logger.info(f"[{self.name}] âœ… Integrated {len(all_search_data)} results from {len(successful_results)} successful searches")
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ 
            if self.context.shared_results_manager:
                shared_count = 0
                for sr in search_results_list:
                    if sr.get('success'):
                        task_id = f"search_{sr['index']}"
                        result_id = await self.context.shared_results_manager.share_result(
                            task_id=task_id,
                            agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                            result=sr['result'],
                            metadata={"query": sr['query'], "index": sr['index']},
                            confidence=1.0 if sr.get('success') else 0.0
                        )
                        shared_count += 1
                        logger.info(f"[{self.name}] ğŸ”— Shared search result for query: '{sr['query'][:50]}...' (result_id: {result_id[:8]}..., agent_id: {self.context.agent_id})")

                # ê³µìœ  í†µê³„ ë¡œê¹…
                total_results = len([sr for sr in search_results_list if sr.get('success')])
                logger.info(f"[{self.name}] ğŸ“¤ Shared {shared_count}/{total_results} successful search results with other agents")
                logger.info(f"[{self.name}] ğŸ¤ Agent communication: {shared_count} results shared via SharedResultsManager")
            
            logger.info(f"[{self.name}] Search completed: success={search_result.get('success')}, total_results={search_result.get('data', {}).get('total_results', 0)}")
            logger.info(f"[{self.name}] Search result type: {type(search_result)}, keys: {list(search_result.keys()) if isinstance(search_result, dict) else 'N/A'}")
            
            if search_result.get('success') and search_result.get('data'):
                data = search_result.get('data', {})
                logger.info(f"[{self.name}] Data type: {type(data)}, keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
                
                # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± - ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                search_results = []
                if isinstance(data, dict):
                    # í‘œì¤€ í˜•ì‹: {"query": "...", "results": [...], "total_results": N, "source": "..."}
                    search_results = data.get('results', [])
                    logger.info(f"[{self.name}] Found 'results' key: {len(search_results)} items")
                    
                    if not search_results:
                        # ë‹¤ë¥¸ í‚¤ ì‹œë„
                        search_results = data.get('items', data.get('data', []))
                        logger.info(f"[{self.name}] Tried 'items' or 'data' keys: {len(search_results)} items")
                    
                    # data ìì²´ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì¤‘ì²©ëœ ê²½ìš°)
                    if not search_results and isinstance(data, dict):
                        # dataì˜ ê°’ ì¤‘ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                        for key, value in data.items():
                            if isinstance(value, list) and len(value) > 0:
                                # ì²« ë²ˆì§¸ í•­ëª©ì´ dictì¸ì§€ í™•ì¸
                                if value and isinstance(value[0], dict):
                                    search_results = value
                                    logger.info(f"[{self.name}] Found list in key '{key}': {len(search_results)} items")
                                    break
                elif isinstance(data, list):
                    search_results = data
                    logger.info(f"[{self.name}] Data is directly a list: {len(search_results)} items")
                
                logger.info(f"[{self.name}] âœ… Parsed {len(search_results)} search results")
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
                if search_results and len(search_results) > 0:
                    first_result = search_results[0]
                    logger.info(f"[{self.name}] First result type: {type(first_result)}, sample: {str(first_result)[:200]}")
                
                if search_results and len(search_results) > 0:
                    # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    unique_results = []
                    seen_urls = set()
                    
                    logger.info(f"[{self.name}] Processing {len(search_results)} results...")
                    
                    for i, result in enumerate(search_results, 1):
                        # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                        if isinstance(result, dict):
                            title = result.get('title', result.get('name', result.get('Title', 'No title')))
                            snippet = result.get('snippet', result.get('content', result.get('summary', result.get('description', result.get('abstract', '')))))
                            url = result.get('url', result.get('link', result.get('href', result.get('URL', ''))))
                            
                            # snippetì— ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì—¬ëŸ¬ ê²°ê³¼ê°€ ë“¤ì–´ìˆëŠ” ê²½ìš° íŒŒì‹±
                            if snippet and ("Found" in snippet or "search results" in snippet.lower() or "\n1." in snippet):
                                logger.info(f"[{self.name}] Detected markdown format in snippet, parsing...")
                                import re
                                parsed_results = []
                                lines = snippet.split('\n')
                                current_result = None
                                
                                for line in lines:
                                    original_line = line
                                    line = line.strip()
                                    if not line:
                                        continue
                                    
                                    # íŒ¨í„´ 1: ë§ˆí¬ë‹¤ìš´ ë§í¬ "1. [Title](URL)"
                                    link_match = re.match(r'^\d+\.\s*\[([^\]]+)\]\(([^\)]+)\)', line)
                                    # íŒ¨í„´ 2: ë²ˆí˜¸ì™€ ì œëª©ë§Œ "1. [Title]" ë˜ëŠ” "1. Title"
                                    title_match = re.match(r'^\d+\.\s*(?:\[([^\]]+)\]|(.+?))(?:\s*$|:)', line)
                                    # íŒ¨í„´ 3: URL ì¤„ "   URL: https://..."
                                    url_match = re.search(r'URL:\s*(https?://[^\s]+)', line, re.IGNORECASE)
                                    # íŒ¨í„´ 4: Summary ì¤„ "   Summary: ..."
                                    summary_match = re.search(r'Summary:\s*(.+)$', line, re.IGNORECASE)
                                    
                                    if link_match:
                                        # ì´ì „ ê²°ê³¼ ì €ì¥
                                        if current_result and current_result.get('title'):
                                            parsed_results.append(current_result)
                                        
                                        title_parsed = link_match.group(1)
                                        url_parsed = link_match.group(2)
                                        current_result = {
                                            "title": title_parsed,
                                            "url": url_parsed,
                                            "snippet": ""
                                        }
                                    elif title_match and not current_result:
                                        # ë²ˆí˜¸ì™€ ì œëª©ë§Œ ìˆëŠ” ê²½ìš° (ë‹¤ìŒ ì¤„ì— URLì´ ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
                                        title_parsed = title_match.group(1) or title_match.group(2)
                                        if title_parsed:
                                            current_result = {
                                                "title": title_parsed.strip(),
                                                "url": "",
                                                "snippet": ""
                                            }
                                    elif url_match:
                                        # URLì´ ë³„ë„ ì¤„ì— ìˆëŠ” ê²½ìš°
                                        if current_result:
                                            current_result["url"] = url_match.group(1)
                                        else:
                                            # URLë§Œ ìˆê³  ì œëª©ì´ ì—†ëŠ” ê²½ìš° (ì´ì „ ê²°ê³¼ì— ì¶”ê°€)
                                            if parsed_results:
                                                parsed_results[-1]["url"] = url_match.group(1)
                                    elif summary_match and current_result:
                                        # Summary ì¤„
                                        current_result["snippet"] = summary_match.group(1).strip()
                                    elif current_result and line and not any([
                                        line.startswith('URL:'), 
                                        line.startswith('Summary:'),
                                        line.startswith('Found'),
                                        'search results' in line.lower()
                                    ]):
                                        # ì„¤ëª… í…ìŠ¤íŠ¸ (ë“¤ì—¬ì“°ê¸°ëœ ê²½ìš°)
                                        if original_line.startswith('   ') or original_line.startswith('\t'):
                                            if current_result["snippet"]:
                                                current_result["snippet"] += " " + line
                                            else:
                                                current_result["snippet"] = line
                                
                                # ë§ˆì§€ë§‰ ê²°ê³¼ ì¶”ê°€
                                if current_result and current_result.get('title'):
                                    parsed_results.append(current_result)
                                
                                if parsed_results:
                                    logger.info(f"[{self.name}] Parsed {len(parsed_results)} results from markdown snippet")
                                    # íŒŒì‹±ëœ ê²°ê³¼ë“¤ì„ unique_resultsì— ì¶”ê°€
                                    for parsed_result in parsed_results:
                                        parsed_url = parsed_result.get('url', '')
                                        if parsed_url and parsed_url in seen_urls:
                                            continue
                                        if parsed_url:
                                            seen_urls.add(parsed_url)
                                        
                                        unique_results.append({
                                            "index": len(unique_results) + 1,
                                            "title": parsed_result.get('title', ''),
                                            "snippet": parsed_result.get('snippet', '')[:500],
                                            "url": parsed_url,
                                            "source": "search"
                                        })
                                        logger.info(f"[{self.name}] Parsed result: {parsed_result.get('title', '')[:50]}... (URL: {parsed_url[:50] if parsed_url else 'N/A'}...)")
                                    
                                    # ì›ë³¸ ê²°ê³¼ëŠ” ê±´ë„ˆë›°ê¸°
                                    continue
                            
                            logger.debug(f"[{self.name}] Result {i}: title={title[:50] if title else 'N/A'}, url={url[:50] if url else 'N/A'}")
                        elif isinstance(result, str):
                            # ë¬¸ìì—´ í˜•ì‹ì¸ ê²½ìš° íŒŒì‹± ì‹œë„ (ë§ˆí¬ë‹¤ìš´ ë§í¬ í˜•ì‹)
                            import re
                            link_match = re.match(r'^\d+\.\s*\[([^\]]+)\]\(([^\)]+)\)', result.strip())
                            if link_match:
                                title = link_match.group(1)
                                url = link_match.group(2)
                                snippet = ""
                                logger.info(f"[{self.name}] Parsed string result {i} as markdown: {title[:50]}")
                            else:
                                logger.warning(f"[{self.name}] Result {i} is string but not markdown format, skipping: {result[:100]}")
                                continue
                        else:
                            logger.warning(f"[{self.name}] Unknown result format for result {i}: {type(result)}, value: {str(result)[:100]}")
                            continue
                        
                        # URL ì¤‘ë³µ ì œê±°
                        if url and url in seen_urls:
                            logger.debug(f"[{self.name}] Duplicate URL skipped: {url}")
                            continue
                        if url:
                            seen_urls.add(url)
                        
                        # êµ¬ì¡°í™”ëœ ê²°ê³¼ ì €ì¥
                        result_dict = {
                            "index": len(unique_results) + 1,
                            "title": title,
                            "snippet": snippet[:500] if snippet else "",
                            "url": url,
                            "source": "search"
                        }
                        unique_results.append(result_dict)
                        
                        logger.info(f"[{self.name}] Result {i}: {title[:50]}... (URL: {url[:50] if url else 'N/A'}...)")
                    
                    # ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    if unique_results:
                        results = unique_results
                        logger.info(f"[{self.name}] âœ… Collected {len(results)} unique results")
                    else:
                        error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        logger.error(f"[{self.name}] âŒ {error_msg}")
                        raise RuntimeError(error_msg)
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ - ì‹¤íŒ¨ ì²˜ë¦¬
                    error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    logger.error(f"[{self.name}] âŒ {error_msg}")
                    raise RuntimeError(error_msg)
            else:
                # ê²€ìƒ‰ ì‹¤íŒ¨ - ì—ëŸ¬ ë°˜í™˜
                error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. {search_result.get('error', 'Unknown error')}"
                logger.error(f"[{self.name}] âŒ {error_msg}")
                raise RuntimeError(error_msg)
                
        except Exception as e:
            # ì‹¤ì œ ì˜¤ë¥˜ ë°œìƒ - ì‹¤íŒ¨ ì²˜ë¦¬
            error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            
            # ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡
            state['research_results'] = []
            state['current_agent'] = self.name
            state['error'] = error_msg
            state['research_failed'] = True
            
            # ë©”ëª¨ë¦¬ì— ì‹¤íŒ¨ ì •ë³´ ê¸°ë¡
            memory.write(
                key=f"execution_error_{state['session_id']}",
                value=error_msg,
                scope=MemoryScope.SESSION,
                session_id=state['session_id'],
                agent_id=self.name
            )
            
            # ì‹¤íŒ¨ ìƒíƒœ ë°˜í™˜ (ë”ë¯¸ ë°ì´í„° ì—†ì´)
            return state
        
        # ì„±ê³µì ìœ¼ë¡œ ê²°ê³¼ ìˆ˜ì§‘ëœ ê²½ìš°
        state['research_results'] = results  # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ (ë®ì–´ì“°ê¸°)
        state['current_agent'] = self.name
        state['research_failed'] = False
        
        logger.info(f"[{self.name}] âœ… Research execution completed: {len(results)} results")
        
        # Write to shared memory (êµ¬ì¡°í™”ëœ í˜•ì‹)
        memory.write(
            key=f"research_results_{state['session_id']}",
            value=results,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Results saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class VerifierAgent:
    """Verifier agent - verifies research results (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "verifier"
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("evaluator")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a verification agent."
    
    async def execute(self, state: AgentState) -> AgentState:
        """Verify research results with LLM-based verification."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting verification")
        logger.info(f"=" * 80)
        
        # ì—°êµ¬ ì‹¤íŒ¨ í™•ì¸
        if state.get('research_failed'):
            logger.error(f"[{self.name}] âŒ Research execution failed, skipping verification")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['current_agent'] = self.name
            return state
        
        memory = self.context.shared_memory
        
        # Read results from state or shared memory
        results = state.get('research_results', [])
        if not results:
            results = memory.read(
                key=f"research_results_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        # SharedResultsManagerì—ì„œ ë‹¤ë¥¸ Executorì˜ ê²°ê³¼ë„ ê°€ì ¸ì˜¤ê¸°
        if self.context.shared_results_manager:
            shared_results = await self.context.shared_results_manager.get_shared_results(
                exclude_agent_id=self.name
            )
            logger.info(f"[{self.name}] ğŸ” Found {len(shared_results)} shared results from other agents")

            # ê³µìœ ëœ ê²°ê³¼ë¥¼ resultsì— ì¶”ê°€
            shared_data_count = 0
            for shared_result in shared_results:
                if isinstance(shared_result.result, dict) and shared_result.result.get('data'):
                    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
                    data = shared_result.result.get('data', {})
                    if isinstance(data, dict):
                        shared_search_results = data.get('results', data.get('items', []))
                        if isinstance(shared_search_results, list):
                            results.extend(shared_search_results)
                            shared_data_count += len(shared_search_results)
                    elif isinstance(data, list):
                        results.extend(data)
                        shared_data_count += len(data)

            logger.info(f"[{self.name}] ğŸ“¥ Retrieved {shared_data_count} additional results from {len(shared_results)} shared agent results")
            logger.info(f"[{self.name}] ğŸ¤ Agent communication: Retrieved results from agents: {[r.agent_id for r in shared_results]}")
        
        logger.info(f"[{self.name}] Found {len(results)} results to verify (including shared results)")
        
        if not results or len(results) == 0:
            error_msg = "ê²€ì¦ ì‹¤íŒ¨: ê²€ì¦í•  ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"[{self.name}] âŒ {error_msg}")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['error'] = error_msg
            state['current_agent'] = self.name
            return state
        
        # LLMì„ ì‚¬ìš©í•œ ì‹¤ì œ ê²€ì¦
        from src.core.llm_manager import execute_llm_task, TaskType
        
        verified = []
        for i, result in enumerate(results, 1):
            if isinstance(result, dict):
                title = result.get('title', '')
                snippet = result.get('snippet', '')
                url = result.get('url', '')
                
                # LLMìœ¼ë¡œ ê²€ì¦
                verification_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ì„¸ìš”:

ì œëª©: {title}
ë‚´ìš©: {snippet[:300]}
URL: {url}

ì›ë˜ ì¿¼ë¦¬: {state['user_query']}

ì´ ê²°ê³¼ê°€ ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”.
ì‘ë‹µ í˜•ì‹: "VERIFIED" ë˜ëŠ” "REJECTED"ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ í•œ ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
                
                try:
                    verification_result = await execute_llm_task(
                        prompt=verification_prompt,
                        task_type=TaskType.VERIFICATION,
                        model_name=None,
                        system_message="You are a verification agent. Verify if search results are relevant and reliable."
                    )
                    
                    verification_text = verification_result.content or "UNKNOWN"
                    is_verified = "VERIFIED" in verification_text.upper() or "REJECT" not in verification_text.upper()
                    
                    if is_verified:
                        verified.append({
                            "index": i,
                            "title": title,
                            "snippet": snippet,
                            "url": url,
                            "status": "verified",
                            "verification_note": verification_text[:200]
                        })
                        logger.info(f"[{self.name}] âœ… Result {i} verified: {title[:50]}...")
                    else:
                        logger.info(f"[{self.name}] âš ï¸ Result {i} rejected: {title[:50]}...")
                        continue
                except Exception as e:
                    logger.warning(f"[{self.name}] Verification failed for result {i}: {e}, including anyway")
                    verified.append({
                        "index": i,
                        "title": title,
                        "snippet": snippet,
                        "url": url,
                        "status": "partial",
                        "verification_note": "Verification failed, but included"
                    })
            else:
                logger.warning(f"[{self.name}] Unknown result format: {type(result)}")
                continue
        
        logger.info(f"[{self.name}] âœ… Verification completed: {len(verified)}/{len(results)} results verified")
        
        # ê²€ì¦ ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ 
        if self.context.shared_results_manager:
            shared_verification_count = 0
            for verified_result in verified:
                task_id = f"verification_{verified_result.get('index', 0)}"
                result_id = await self.context.shared_results_manager.share_result(
                    task_id=task_id,
                    agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                    result=verified_result,
                    metadata={"status": verified_result.get('status', 'unknown')},
                    confidence=1.0 if verified_result.get('status') == 'verified' else 0.5
                )
                shared_verification_count += 1
                logger.info(f"[{self.name}] ğŸ”— Shared verification result {verified_result.get('index', 0)} (result_id: {result_id[:8]}..., status: {verified_result.get('status', 'unknown')})")

            logger.info(f"[{self.name}] ğŸ“¤ Shared {shared_verification_count} verification results with other agents")

            # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê²€ì¦ ê²°ê³¼ì™€ í† ë¡  (ê²€ì¦ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš°)
            if self.context.discussion_manager and len(verified) > 0:
                other_verified = await self.context.shared_results_manager.get_shared_results(
                    agent_id=None,  # ëª¨ë“  ì—ì´ì „íŠ¸
                    exclude_agent_id=self.context.agent_id  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                )

                # ê²€ì¦ëœ ê²°ê³¼ë§Œ í•„í„°ë§
                other_verified_results = [r for r in other_verified if isinstance(r.result, dict) and r.result.get('status') == 'verified']

                if other_verified_results:
                    logger.info(f"[{self.name}] ğŸ’¬ Found {len(other_verified_results)} verified results from other agents for discussion")

                    # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ 
                    first_verified = verified[0]
                    result_id = f"verification_{first_verified.get('index', 0)}"
                    logger.info(f"[{self.name}] ğŸ’¬ Starting discussion on verification result {first_verified.get('index', 0)} with {len(other_verified_results[:3])} other agents")

                    discussion = await self.context.discussion_manager.agent_discuss_result(
                        result_id=result_id,
                        agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                        other_agent_results=other_verified_results[:3]  # ìµœëŒ€ 3ê°œ
                    )
                    if discussion:
                        logger.info(f"[{self.name}] ğŸ’¬ Discussion completed: {discussion[:150]}... (agent_id: {self.context.agent_id})")
                        logger.info(f"[{self.name}] ğŸ¤ Agent discussion: Analyzed verification consistency with {len(other_verified_results[:3])} peer agents")
                    else:
                        logger.info(f"[{self.name}] ğŸ’¬ No discussion generated for verification result")
                else:
                    logger.info(f"[{self.name}] ğŸ’¬ No other verified results found for discussion")
            else:
                logger.info(f"[{self.name}] Agent discussion disabled or no verified results to discuss")
        
        state['verified_results'] = verified
        state['current_agent'] = self.name
        state['verification_failed'] = False if verified else True
        
        # Write to shared memory
        memory.write(
            key=f"verified_{state['session_id']}",
            value=verified,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Verified results saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class GeneratorAgent:
    """Generator agent - creates final report (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "generator"
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("synthesizer")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a report generation agent."
    
    async def execute(self, state: AgentState) -> AgentState:
        """Generate final report."""
        logger.info(f"[{self.name}] Generating final report...")
        
        # ì—°êµ¬ ë˜ëŠ” ê²€ì¦ ì‹¤íŒ¨ í™•ì¸ - Fallback ì œê±°, ëª…í™•í•œ ì—ëŸ¬ë§Œ ë°˜í™˜
        if state.get('research_failed') or state.get('verification_failed'):
            error_msg = state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            logger.error(f"[{self.name}] âŒ Research or verification failed: {error_msg}")
            state['final_report'] = None
            state['current_agent'] = self.name
            state['report_failed'] = True
            state['error'] = error_msg
            return state
        
        memory = self.context.shared_memory
        
        # Read verified results from state or shared memory
        verified_results = state.get('verified_results', [])
        if not verified_results:
            verified_results = memory.read(
                key=f"verified_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        # SharedResultsManagerì—ì„œ ëª¨ë“  ê³µìœ ëœ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        if self.context.shared_results_manager:
            all_shared_results = await self.context.shared_results_manager.get_shared_results()
            logger.info(f"[{self.name}] ğŸ” Found {len(all_shared_results)} total shared results from all agents")

            # ê³µìœ  ê²°ê³¼ í†µê³„
            verification_results = [r for r in all_shared_results if isinstance(r.result, dict) and r.result.get('status') == 'verified']
            search_results = [r for r in all_shared_results if not isinstance(r.result, dict) or r.result.get('status') != 'verified']

            logger.info(f"[{self.name}] ğŸ“Š Shared results breakdown: {len(verification_results)} verified, {len(search_results)} search results")

            # ê²€ì¦ëœ ê²°ê³¼ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ê°€
            added_from_shared = 0
            for shared_result in all_shared_results:
                if isinstance(shared_result.result, dict):
                    # ê²€ì¦ëœ ê²°ê³¼ì¸ ê²½ìš°
                    if shared_result.result.get('status') == 'verified':
                        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
                        existing_urls = {r.get('url', '') for r in verified_results if isinstance(r, dict)}
                        result_url = shared_result.result.get('url', '')
                        if result_url and result_url not in existing_urls:
                            verified_results.append(shared_result.result)
                            added_from_shared += 1
                            logger.info(f"[{self.name}] â• Added shared verified result from agent {shared_result.agent_id}: {shared_result.result.get('title', '')[:50]}...")

            logger.info(f"[{self.name}] ğŸ“¥ Added {added_from_shared} verified results from shared agent communications")
            logger.info(f"[{self.name}] ğŸ¤ Agent communication: Incorporated results from agents: {list(set(r.agent_id for r in all_shared_results))}")
        
        logger.info(f"[{self.name}] Found {len(verified_results)} verified results for report generation (including shared results)")
        
        if not verified_results or len(verified_results) == 0:
            # Fallback ì œê±° - ëª…í™•í•œ ì—ëŸ¬ë§Œ ë°˜í™˜
            error_msg = "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"[{self.name}] âŒ {error_msg}")
            state['final_report'] = None
            state['current_agent'] = self.name
            state['report_failed'] = True
            state['error'] = error_msg
            return state
        
        # ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° LLMìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±
        logger.info(f"[{self.name}] Generating report with LLM from {len(verified_results)} verified results...")
        
        # ê²€ì¦ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        verified_text = ""
        for result in verified_results:
            if isinstance(result, dict):
                verified_text += f"\n- {result.get('title', '')}: {result.get('snippet', '')[:200]}... (Source: {result.get('url', '')})\n"
            else:
                verified_text += f"\n- {str(result)}\n"
        
        # LLMìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ìƒì„±
        from src.core.llm_manager import execute_llm_task, TaskType
        
        # ì‚¬ìš©ì ìš”ì²­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬ - LLMì´ í˜•ì‹ì„ ê²°ì •í•˜ë„ë¡
        generation_prompt = f"""ì‚¬ìš©ì ìš”ì²­: {state['user_query']}

ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼:
{verified_text}

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³ , ìš”ì²­í•œ í˜•ì‹ì— ë§ê²Œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ë³´ê³ ì„œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ
- ì½”ë“œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ
- ë¬¸ì„œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ

ìš”ì²­ëœ í˜•ì‹ì— ë§ê²Œ ì™„ì „í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        try:
            report_result = await execute_llm_task(
                prompt=generation_prompt,
                task_type=TaskType.GENERATION,
                model_name=None,
                system_message="You are an expert assistant. Generate results in the exact format requested by the user. If they ask for a report, create a report. If they ask for code, create executable code. Follow the user's request precisely without adding unnecessary templates or structures."
            )
            
            report = report_result.content or f"# Report: {state['user_query']}\n\nNo report generated."
            
            # Safety filter ì°¨ë‹¨ í™•ì¸ - Fallback ì œê±°, ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
            if "blocked by safety" in report.lower() or "content blocked" in report.lower() or len(report) < 100:
                error_msg = "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: Safety filterì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
                logger.error(f"[{self.name}] âŒ {error_msg}")
                state['final_report'] = None
                state['report_failed'] = True
                state['error'] = error_msg
                state['current_agent'] = self.name
                return state
            else:
                logger.info(f"[{self.name}] âœ… Report generated: {len(report)} characters")
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Report generation failed: {e}")
            # Fallback ì œê±° - ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
            error_msg = f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state['final_report'] = None
            state['report_failed'] = True
            state['error'] = error_msg
            state['current_agent'] = self.name
            return state
        
        state['final_report'] = report
        state['current_agent'] = self.name
        state['report_failed'] = False
        
        # Write to shared memory
        memory.write(
            key=f"report_{state['session_id']}",
            value=report,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] âœ… Report saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


###################
# Orchestrator
###################

class AgentOrchestrator:
    """Orchestrator for multi-agent workflow."""
    
    def __init__(self, config: Any = None):
        """Initialize orchestrator."""
        self.config = config
        self.shared_memory = get_shared_memory()
        self.skill_manager = get_skill_manager()
        self.agent_config = get_agent_config()
        self.graph = None
        # GraphëŠ” ì²« ì‹¤í–‰ ì‹œ ì¿¼ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë¹Œë“œ
        
        # SharedResultsManagerì™€ AgentDiscussionManagerëŠ” execute ì‹œì ì— ì´ˆê¸°í™”
        # (objective_idê°€ í•„ìš”í•˜ë¯€ë¡œ)
        self.shared_results_manager: Optional[SharedResultsManager] = None
        self.discussion_manager: Optional[AgentDiscussionManager] = None
        
        logger.info("AgentOrchestrator initialized")
    
    def _build_graph(self, user_query: Optional[str] = None, session_id: Optional[str] = None) -> None:
        """Build LangGraph workflow with Skills auto-selection."""
        
        # Create context for all agents
        context = AgentContext(
            agent_id="orchestrator",
            session_id=session_id or "default",
            shared_memory=self.shared_memory,
            config=self.config,
            shared_results_manager=self.shared_results_manager,
            discussion_manager=self.discussion_manager
        )
        
        # Skills ìë™ ì„ íƒ (ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´)
        selected_skills = {}
        if user_query:
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(user_query)
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # Initialize agents with Skills
        self.planner = PlannerAgent(context, selected_skills.get("research_planner"))
        self.executor = ExecutorAgent(context, selected_skills.get("research_executor"))
        self.verifier = VerifierAgent(context, selected_skills.get("evaluator"))
        self.generator = GeneratorAgent(context, selected_skills.get("synthesizer"))
        
        # Build graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("executor", self._executor_node)  # Legacy
        workflow.add_node("parallel_executor", self._parallel_executor_node)  # New parallel executor
        workflow.add_node("verifier", self._verifier_node)  # Legacy
        workflow.add_node("parallel_verifier", self._parallel_verifier_node)  # New parallel verifier
        workflow.add_node("generator", self._generator_node)
        workflow.add_node("end", self._end_node)
        
        # Define edges - ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ ì‚¬ìš©
        workflow.set_entry_point("planner")
        workflow.add_edge("planner", "parallel_executor")  # ë³‘ë ¬ ì‹¤í–‰ ì‚¬ìš©
        workflow.add_edge("parallel_executor", "parallel_verifier")  # ë³‘ë ¬ ê²€ì¦ ì‚¬ìš©
        workflow.add_edge("parallel_verifier", "generator")
        workflow.add_edge("generator", "end")
        
        # Compile graph
        self.graph = workflow.compile()
        
        logger.info("LangGraph workflow built")
    
    async def _planner_node(self, state: AgentState) -> AgentState:
        """Planner node execution with tracking."""
        logger.info("=" * 80)
        logger.info("ğŸ”µ [WORKFLOW] â†’ Planner Node")
        logger.info("=" * 80)
        result = await self.planner.execute(state)
        logger.info(f"ğŸ”µ [WORKFLOW] âœ“ Planner completed: {result.get('current_agent')}")
        return result
    
    async def _executor_node(self, state: AgentState) -> AgentState:
        """Executor node execution with tracking (legacy - for backward compatibility)."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¢ [WORKFLOW] â†’ Executor Node (legacy)")
        logger.info("=" * 80)
        result = await self.executor.execute(state)
        logger.info(f"ğŸŸ¢ [WORKFLOW] âœ“ Executor completed: {len(result.get('research_results', []))} results")
        return result
    
    async def _parallel_executor_node(self, state: AgentState) -> AgentState:
        """Parallel executor node - runs multiple ExecutorAgent instances simultaneously."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¢ [WORKFLOW] â†’ Parallel Executor Node")
        logger.info("=" * 80)
        
        # ì‘ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        tasks = state.get('research_tasks', [])
        if not tasks:
            # ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
            memory = self.shared_memory
            tasks = memory.read(
                key=f"tasks_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        if not tasks:
            logger.warning("[WORKFLOW] No tasks found, falling back to single executor")
            return await self._executor_node(state)
        
        logger.info(f"[WORKFLOW] Executing {len(tasks)} tasks in parallel with {len(tasks)} ExecutorAgent instances")
        
        # ë™ì  ë™ì‹œì„± ê´€ë¦¬ í†µí•©
        from src.core.concurrency_manager import get_concurrency_manager
        concurrency_manager = get_concurrency_manager()
        max_concurrent = concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        max_concurrent = min(max_concurrent, len(tasks))  # ì‘ì—… ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡
        
        logger.info(f"[WORKFLOW] Using concurrency limit: {max_concurrent} (from concurrency_manager)")
        
        # Skills ìë™ ì„ íƒ
        selected_skills = {}
        if state.get('user_query'):
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(state['user_query'])
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # ì—¬ëŸ¬ ExecutorAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def execute_single_task(task: Dict[str, Any], task_index: int) -> AgentState:
            """ë‹¨ì¼ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ExecutorAgent."""
            agent_id = f"executor_{task_index}"
            context = AgentContext(
                agent_id=agent_id,
                session_id=state['session_id'],
                shared_memory=self.shared_memory,
                config=self.config,
                shared_results_manager=self.shared_results_manager,
                discussion_manager=self.discussion_manager
            )
            
            executor_agent = ExecutorAgent(context, selected_skills.get("research_executor"))
            
            try:
                logger.info(f"[WORKFLOW] ExecutorAgent {agent_id} starting task {task.get('task_id', 'unknown')}")
                result_state = await executor_agent.execute(state, assigned_task=task)
                logger.info(f"[WORKFLOW] ExecutorAgent {agent_id} completed: {len(result_state.get('research_results', []))} results")
                return result_state
            except Exception as e:
                logger.error(f"[WORKFLOW] ExecutorAgent {agent_id} failed: {e}")
                # ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸ì˜ ìƒíƒœ ë°˜í™˜
                failed_state = state.copy()
                failed_state['research_results'] = []
                failed_state['research_failed'] = True
                failed_state['error'] = f"Task {task.get('task_id', 'unknown')} failed: {str(e)}"
                failed_state['current_agent'] = agent_id
                return failed_state
        
        # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ë™ì  ë™ì‹œì„± ì œí•œ ì ìš©)
        if max_concurrent < len(tasks):
            # Semaphoreë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def execute_with_limit(task: Dict[str, Any], task_index: int) -> AgentState:
                async with semaphore:
                    return await execute_single_task(task, task_index)
            
            executor_tasks = [execute_with_limit(task, i) for i, task in enumerate(tasks)]
        else:
            # ë™ì‹œì„± ì œí•œì´ ì‘ì—… ìˆ˜ë³´ë‹¤ í¬ë©´ ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
            executor_tasks = [execute_single_task(task, i) for i, task in enumerate(tasks)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        executor_results = await asyncio.gather(*executor_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° í†µì‹  ìƒíƒœ í™•ì¸
        all_results = []
        all_failed = False
        errors = []
        communication_stats = {
            'agents_contributed': 0,
            'results_shared': 0,
            'communication_errors': 0
        }

        for i, result in enumerate(executor_results):
            if isinstance(result, Exception):
                logger.error(f"[WORKFLOW] ExecutorAgent {i} raised exception: {result}")
                all_failed = True
                errors.append(f"Task {tasks[i].get('task_id', 'unknown')}: {str(result)}")
                communication_stats['communication_errors'] += 1
            elif isinstance(result, dict):
                # ê²°ê³¼ ìˆ˜ì§‘
                task_results = result.get('research_results', [])
                if task_results:
                    all_results.extend(task_results)
                    communication_stats['agents_contributed'] += 1
                    logger.info(f"[WORKFLOW] ExecutorAgent {i} contributed {len(task_results)} results")

                # SharedResultsManager í†µì‹  ìƒíƒœ í™•ì¸
                if self.shared_results_manager:
                    agent_id = f"executor_{i}"
                    agent_results = await self.shared_results_manager.get_shared_results(agent_id=agent_id)
                    if agent_results:
                        communication_stats['results_shared'] += len(agent_results)
                        logger.info(f"[WORKFLOW] ğŸ¤ ExecutorAgent {agent_id} shared {len(agent_results)} results via SharedResultsManager")

                # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸
                if result.get('research_failed'):
                    all_failed = True
                    if result.get('error'):
                        errors.append(result['error'])
                        communication_stats['communication_errors'] += 1
        
        # í†µí•©ëœ ìƒíƒœ ìƒì„±
        final_state = state.copy()
        final_state['research_results'] = all_results
        final_state['research_failed'] = all_failed
        final_state['current_agent'] = "parallel_executor"
        
        if errors:
            final_state['error'] = "; ".join(errors)
        
        logger.info(f"[WORKFLOW] âœ… Parallel execution completed: {len(all_results)} total results from {len(tasks)} tasks")
        logger.info(f"[WORKFLOW] ğŸ¤ Agent communication summary: {communication_stats['agents_contributed']} agents contributed, {communication_stats['results_shared']} results shared")
        if communication_stats['communication_errors'] > 0:
            logger.warning(f"[WORKFLOW] âš ï¸ Communication errors: {communication_stats['communication_errors']}")
        logger.info(f"[WORKFLOW] Failed: {all_failed}")
        
        return final_state
    
    async def _verifier_node(self, state: AgentState) -> AgentState:
        """Verifier node execution with tracking (legacy - for backward compatibility)."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¡ [WORKFLOW] â†’ Verifier Node (legacy)")
        logger.info("=" * 80)
        result = await self.verifier.execute(state)
        logger.info(f"ğŸŸ¡ [WORKFLOW] âœ“ Verifier completed: {len(result.get('verified_results', []))} verified")
        return result
    
    async def _parallel_verifier_node(self, state: AgentState) -> AgentState:
        """Parallel verifier node - runs multiple VerifierAgent instances simultaneously."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¡ [WORKFLOW] â†’ Parallel Verifier Node")
        logger.info("=" * 80)
        
        # ì—°êµ¬ ì‹¤íŒ¨ í™•ì¸
        if state.get('research_failed'):
            logger.error("[WORKFLOW] Research execution failed, skipping verification")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['current_agent'] = "parallel_verifier"
            return state
        
        # ê²€ì¦í•  ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        results = state.get('research_results', [])
        if not results:
            memory = self.shared_memory
            results = memory.read(
                key=f"research_results_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        if not results:
            logger.warning("[WORKFLOW] No results to verify, falling back to single verifier")
            return await self._verifier_node(state)
        
        # ê²°ê³¼ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ VerifierAgentì— í• ë‹¹
        num_verifiers = min(len(results), self.agent_config.max_concurrent_research_units or 3)
        chunk_size = max(1, len(results) // num_verifiers)
        result_chunks = [results[i:i + chunk_size] for i in range(0, len(results), chunk_size)]
        
        logger.info(f"[WORKFLOW] Verifying {len(results)} results with {len(result_chunks)} VerifierAgent instances")
        
        # ë™ì  ë™ì‹œì„± ê´€ë¦¬ í†µí•©
        from src.core.concurrency_manager import get_concurrency_manager
        concurrency_manager = get_concurrency_manager()
        max_concurrent = concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        max_concurrent = min(max_concurrent, len(result_chunks))
        
        logger.info(f"[WORKFLOW] Using concurrency limit: {max_concurrent} (from concurrency_manager)")
        
        # Skills ìë™ ì„ íƒ
        selected_skills = {}
        if state.get('user_query'):
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(state['user_query'])
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # ì—¬ëŸ¬ VerifierAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def verify_single_chunk(chunk: List[Dict[str, Any]], chunk_index: int) -> List[Dict[str, Any]]:
            """ë‹¨ì¼ ì²­í¬ë¥¼ ê²€ì¦í•˜ëŠ” VerifierAgent."""
            agent_id = f"verifier_{chunk_index}"
            logger.info(f"[WORKFLOW] ğŸ’¬ Creating VerifierAgent {agent_id} for {len(chunk)} results")
            context = AgentContext(
                agent_id=agent_id,
                session_id=state['session_id'],
                shared_memory=self.shared_memory,
                config=self.config,
                shared_results_manager=self.shared_results_manager,
                discussion_manager=self.discussion_manager
            )
            
            verifier_agent = VerifierAgent(context, selected_skills.get("evaluator"))
            
            # ì²­í¬ë§Œ í¬í•¨í•˜ëŠ” ì„ì‹œ state ìƒì„±
            chunk_state = state.copy()
            chunk_state['research_results'] = chunk
            
            try:
                logger.info(f"[WORKFLOW] VerifierAgent {agent_id} starting verification of {len(chunk)} results")
                result_state = await verifier_agent.execute(chunk_state)
                verified_chunk = result_state.get('verified_results', [])
                logger.info(f"[WORKFLOW] VerifierAgent {agent_id} completed: {len(verified_chunk)} verified")
                return verified_chunk
            except Exception as e:
                logger.error(f"[WORKFLOW] VerifierAgent {agent_id} failed: {e}")
                return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        # ëª¨ë“  ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ê²€ì¦ (ë™ì  ë™ì‹œì„± ì œí•œ ì ìš©)
        if max_concurrent < len(result_chunks):
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def verify_with_limit(chunk: List[Dict[str, Any]], chunk_index: int) -> List[Dict[str, Any]]:
                async with semaphore:
                    return await verify_single_chunk(chunk, chunk_index)
            
            verifier_tasks = [verify_with_limit(chunk, i) for i, chunk in enumerate(result_chunks)]
        else:
            verifier_tasks = [verify_single_chunk(chunk, i) for i, chunk in enumerate(result_chunks)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        verifier_results = await asyncio.gather(*verifier_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° í†µì‹  ìƒíƒœ í™•ì¸
        all_verified = []
        communication_stats = {
            'verifiers_contributed': 0,
            'verification_results_shared': 0,
            'discussion_participants': 0
        }

        for i, result in enumerate(verifier_results):
            if isinstance(result, Exception):
                logger.error(f"[WORKFLOW] VerifierAgent {i} raised exception: {result}")
            elif isinstance(result, list):
                all_verified.extend(result)
                communication_stats['verifiers_contributed'] += 1
                logger.info(f"[WORKFLOW] VerifierAgent {i} contributed {len(result)} verified results")

                # SharedResultsManager í†µì‹  ìƒíƒœ í™•ì¸
                if self.shared_results_manager:
                    agent_id = f"verifier_{i}"
                    agent_results = await self.shared_results_manager.get_shared_results(agent_id=agent_id)
                    verification_shared = [r for r in agent_results if isinstance(r.result, dict) and r.result.get('status') == 'verified']
                    if verification_shared:
                        communication_stats['verification_results_shared'] += len(verification_shared)
                        logger.info(f"[WORKFLOW] ğŸ¤ VerifierAgent {agent_id} shared {len(verification_shared)} verification results")

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_verified = []
        for verified_result in all_verified:
            if isinstance(verified_result, dict):
                url = verified_result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_verified.append(verified_result)
                elif not url:
                    unique_verified.append(verified_result)

        logger.info(f"[WORKFLOW] ğŸ“Š Verification deduplication: {len(all_verified)} â†’ {len(unique_verified)} unique results")

        # ì—¬ëŸ¬ VerifierAgent ê°„ í† ë¡  (ê²€ì¦ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš°)
        if self.discussion_manager and len(unique_verified) > 0:
            # ë‹¤ë¥¸ VerifierAgentì˜ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            if self.shared_results_manager:
                other_verified = await self.shared_results_manager.get_shared_results()
                other_verified_results = [r for r in other_verified if isinstance(r.result, dict) and r.result.get('status') == 'verified']

                if other_verified_results:
                    communication_stats['discussion_participants'] = len(set(r.agent_id for r in other_verified_results))
                    logger.info(f"[WORKFLOW] ğŸ’¬ Starting inter-verifier discussion with {len(other_verified_results)} results from {communication_stats['discussion_participants']} agents")

                    # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ 
                    first_verified = unique_verified[0]
                    result_id = f"verification_{first_verified.get('index', 0)}"
                    discussion = await self.discussion_manager.agent_discuss_result(
                        result_id=result_id,
                        agent_id="parallel_verifier",
                        other_agent_results=other_verified_results[:3]
                    )
                    if discussion:
                        logger.info(f"[WORKFLOW] ğŸ’¬ Inter-verifier discussion completed: {discussion[:150]}...")
                        logger.info(f"[WORKFLOW] ğŸ¤ Agent discussion: {communication_stats['discussion_participants']} verifiers participated in result validation")
                    else:
                        logger.info(f"[WORKFLOW] ğŸ’¬ No discussion generated between verifiers")
                else:
                    logger.info(f"[WORKFLOW] ğŸ’¬ No other verified results available for inter-verifier discussion")
        
        # í†µí•©ëœ ìƒíƒœ ìƒì„±
        final_state = state.copy()
        final_state['verified_results'] = unique_verified
        final_state['verification_failed'] = False if unique_verified else True
        final_state['current_agent'] = "parallel_verifier"
        
        logger.info(f"[WORKFLOW] âœ… Parallel verification completed: {len(unique_verified)} total verified results from {len(result_chunks)} verifiers")
        logger.info(f"[WORKFLOW] ğŸ¤ Agent communication summary: {communication_stats['verifiers_contributed']} verifiers contributed, {communication_stats['verification_results_shared']} verification results shared")
        if communication_stats['discussion_participants'] > 0:
            logger.info(f"[WORKFLOW] ğŸ’¬ Inter-verifier discussion: {communication_stats['discussion_participants']} agents participated")
        
        return final_state
    
    async def _generator_node(self, state: AgentState) -> AgentState:
        """Generator node execution with tracking."""
        logger.info("=" * 80)
        logger.info("ğŸŸ£ [WORKFLOW] â†’ Generator Node")
        logger.info("=" * 80)
        result = await self.generator.execute(state)
        logger.info(f"ğŸŸ£ [WORKFLOW] âœ“ Generator completed: report_length={len(result.get('final_report', ''))}")
        return result
    
    async def _end_node(self, state: AgentState) -> AgentState:
        """End node - final state with summary."""
        logger.info("=" * 80)
        logger.info("âœ… [WORKFLOW] â†’ End Node - Workflow Completed")
        logger.info("=" * 80)
        logger.info(f"Session: {state.get('session_id')}")
        logger.info(f"Final Agent: {state.get('current_agent')}")
        logger.info(f"Research Results: {len(state.get('research_results', []))}")
        logger.info(f"Verified Results: {len(state.get('verified_results', []))}")
        logger.info(f"Report Generated: {bool(state.get('final_report'))}")
        logger.info(f"Failed: {state.get('research_failed') or state.get('verification_failed') or state.get('report_failed')}")
        logger.info("=" * 80)
        return state
    
    async def execute(self, user_query: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Execute multi-agent workflow with Skills auto-selection.
        
        Args:
            user_query: User's research query
            session_id: Session ID
            
        Returns:
            Final result from the workflow
        """
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"Starting workflow for query: {user_query}")
        
        # Objective ID ìƒì„± (ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ê³µìœ ìš©)
        objective_id = f"objective_{session_id}"
        
        # SharedResultsManagerì™€ AgentDiscussionManager ì´ˆê¸°í™” (ë³‘ë ¬ ì‹¤í–‰ í™œì„±í™” ì‹œ)
        if self.agent_config.enable_agent_communication:
            self.shared_results_manager = SharedResultsManager(objective_id=objective_id)
            self.discussion_manager = AgentDiscussionManager(
                objective_id=objective_id,
                shared_results_manager=self.shared_results_manager
            )
            logger.info("âœ… Agent result sharing and discussion enabled")
            logger.info(f"ğŸ¤ SharedResultsManager initialized for objective: {objective_id}")
            logger.info(f"ğŸ’¬ AgentDiscussionManager initialized with agent communication support")
        else:
            self.shared_results_manager = None
            self.discussion_manager = None
            logger.info("Agent communication disabled")
        
        # Graphê°€ ì—†ê±°ë‚˜ ì¿¼ë¦¬ ê¸°ë°˜ ì¬ë¹Œë“œê°€ í•„ìš”í•œ ê²½ìš° ë¹Œë“œ
        if self.graph is None:
            self._build_graph(user_query, session_id)
        
        # Initialize state
        initial_state = AgentState(
            messages=[],
            user_query=user_query,
            research_plan=None,
            research_tasks=[],
            research_results=[],
            verified_results=[],
            final_report=None,
            current_agent=None,
            iteration=0,
            session_id=session_id,
            research_failed=False,
            verification_failed=False,
            report_failed=False,
            error=None
        )
        
        # Execute workflow
        try:
            result = await self.graph.ainvoke(initial_state)
            logger.info("Workflow execution completed successfully")
            return result
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            raise
    
    async def stream(self, user_query: str, session_id: Optional[str] = None):
        """Stream workflow execution."""
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Initialize state
        initial_state = AgentState(
            messages=[],
            user_query=user_query,
            research_plan=None,
            research_tasks=[],
            research_results=[],
            verified_results=[],
            final_report=None,
            current_agent=None,
            iteration=0,
            session_id=session_id,
            research_failed=False,
            verification_failed=False,
            report_failed=False,
            error=None
        )
        
        # Stream execution
        async for event in self.graph.astream(initial_state):
            yield event


# Global orchestrator instance
_orchestrator: Optional[AgentOrchestrator] = None


def get_orchestrator(config: Any = None) -> AgentOrchestrator:
    """Get global orchestrator instance."""
    global _orchestrator
    
    if _orchestrator is None:
        _orchestrator = AgentOrchestrator(config=config)
    
    return _orchestrator


def init_orchestrator(config: Any = None) -> AgentOrchestrator:
    """Initialize orchestrator."""
    global _orchestrator
    
    _orchestrator = AgentOrchestrator(config=config)
    
    return _orchestrator

