"""
LangGraph Orchestrator (v2.0 - 8ëŒ€ í˜ì‹  í†µí•©)

Adaptive Supervisor, Hierarchical Compression, Multi-Model Orchestration,
Continuous Verification, Streaming Pipeline, Universal MCP Hub,
Adaptive Context Window, Production-Grade Reliabilityë¥¼ í†µí•©í•œ
ê³ ë„í™”ëœ LangGraph ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Tuple, TypedDict, Annotated
from datetime import datetime
import json
from pathlib import Path
import os

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import BaseTool

from researcher_config import get_llm_config, get_agent_config, get_research_config, get_mcp_config
from src.core.llm_manager import execute_llm_task, TaskType, get_best_model_for_task
from src.core.mcp_integration import execute_tool, ToolCategory, health_check
from src.core.reliability import execute_with_reliability, get_system_status
from src.core.compression import compress_data, get_compression_stats

logger = logging.getLogger(__name__)


class ResearchState(TypedDict):
    """LangGraph ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜ (8ëŒ€ í˜ì‹  í†µí•©)."""
    # Input
    user_request: str
    context: Optional[Dict[str, Any]]
    objective_id: str
    
    # Adaptive Supervisor (í˜ì‹  1)
    complexity_score: float
    allocated_researchers: int
    priority_queue: List[Dict[str, Any]]
    quality_threshold: float
    
    # Analysis
    analyzed_objectives: List[Dict[str, Any]]
    intent_analysis: Dict[str, Any]
    domain_analysis: Dict[str, Any]
    scope_analysis: Dict[str, Any]
    
    # Planning Agent (ìƒˆ í•„ë“œ)
    preliminary_research: Dict[str, Any]  # MCP ë„êµ¬ë¡œ ìˆ˜ì§‘í•œ ì‚¬ì „ ì¡°ì‚¬ ê²°ê³¼
    planned_tasks: List[Dict[str, Any]]  # ì„¸ë¶€ task ëª©ë¡
    agent_assignments: Dict[str, List[str]]  # agentë³„ í• ë‹¹ëœ task
    execution_plan: Dict[str, Any]  # ì‹¤í–‰ ì „ëµ (ìˆœì„œ, ë³‘ë ¬ì„±)
    plan_approved: bool  # Plan ê²€ì¦ í†µê³¼ ì—¬ë¶€
    plan_feedback: Optional[str]  # Plan ê²€ì¦ í”¼ë“œë°±
    plan_iteration: int  # Plan ì¬ì‘ì„± íšŸìˆ˜
    
    
    # Execution (Universal MCP Hub + Streaming Pipeline)
    execution_results: List[Dict[str, Any]]
    agent_status: Dict[str, Any]
    execution_metadata: Dict[str, Any]
    streaming_data: List[Dict[str, Any]]
    
    # Hierarchical Compression (í˜ì‹  2)
    compression_results: List[Dict[str, Any]]
    compression_metadata: Dict[str, Any]
    
    # Continuous Verification (í˜ì‹  4)
    verification_results: Dict[str, Any]
    confidence_scores: Dict[str, float]
    verification_stages: List[Dict[str, Any]]
    
    # Evaluation
    evaluation_results: Dict[str, Any]
    quality_metrics: Dict[str, float]
    improvement_areas: List[str]
    
    # Validation
    validation_results: Dict[str, Any]
    validation_score: float
    missing_elements: List[str]
    
    # Synthesis (Adaptive Context Window)
    final_synthesis: Dict[str, Any]
    deliverable_path: Optional[str]
    synthesis_metadata: Dict[str, Any]
    context_window_usage: Dict[str, Any]
    
    # Control Flow
    current_step: str
    iteration: int
    max_iterations: int
    should_continue: bool
    error_message: Optional[str]
    
    # Innovation Stats
    innovation_stats: Dict[str, Any]
    
    # Messages for LangGraph
    messages: Annotated[List[BaseMessage], "Messages in the conversation"]


class AutonomousOrchestrator:
    """8ëŒ€ í˜ì‹ ì„ í†µí•©í•œ LangGraph ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°."""
    
    def __init__(self):
        """ì´ˆê¸°í™”."""
        self.llm_config = get_llm_config()
        self.agent_config = get_agent_config()
        self.research_config = get_research_config()
        self.mcp_config = get_mcp_config()
        
        self.graph = None
        self._build_langgraph_workflow()
    
    def _build_langgraph_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•."""
        # StateGraph ìƒì„±
        workflow = StateGraph(ResearchState)
        
        # ë…¸ë“œ ì¶”ê°€ (8ëŒ€ í˜ì‹  í†µí•© + Planning Agent)
        workflow.add_node("analyze_objectives", self._analyze_objectives)
        workflow.add_node("planning_agent", self._planning_agent)
        workflow.add_node("verify_plan", self._verify_plan)
        workflow.add_node("adaptive_supervisor", self._adaptive_supervisor)
        workflow.add_node("execute_research", self._execute_research)
        workflow.add_node("hierarchical_compression", self._hierarchical_compression)
        workflow.add_node("continuous_verification", self._continuous_verification)
        workflow.add_node("evaluate_results", self._evaluate_results)
        workflow.add_node("validate_results", self._validate_results)
        workflow.add_node("synthesize_deliverable", self._synthesize_deliverable)
        
        # ì—£ì§€ ì¶”ê°€ (Planning Agent í†µí•©)
        workflow.set_entry_point("analyze_objectives")
        
        # Planning Agent ì›Œí¬í”Œë¡œìš°
        workflow.add_edge("analyze_objectives", "planning_agent")
        workflow.add_edge("planning_agent", "verify_plan")
        
        # Plan ê²€ì¦ í›„ ì¡°ê±´ë¶€ ë¶„ê¸°
        workflow.add_conditional_edges(
            "verify_plan",
            lambda state: "approved" if state.get("plan_approved", False) else "planning_agent",
            {
                "approved": "adaptive_supervisor",
                "planning_agent": "planning_agent"
            }
        )
        
        # ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° (Planning Agent í†µí•©)
        workflow.add_edge("adaptive_supervisor", "execute_research")
        workflow.add_edge("execute_research", "hierarchical_compression")
        workflow.add_edge("hierarchical_compression", "continuous_verification")
        workflow.add_edge("continuous_verification", "evaluate_results")
        workflow.add_edge("evaluate_results", "validate_results")
        workflow.add_edge("validate_results", "synthesize_deliverable")
        workflow.add_edge("synthesize_deliverable", END)
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.graph = workflow.compile()
    
    async def _analyze_objectives(self, state: ResearchState) -> ResearchState:
        """ëª©í‘œ ë¶„ì„ (Multi-Model Orchestration)."""
        logger.info("ğŸ” Analyzing objectives with Multi-Model Orchestration")
        logger.info(f"ğŸ“ Research Request: {state['user_request']}")
        logger.info(f"ğŸ“‹ Context: {state.get('context', {})}")
        
        analysis_prompt = f"""
        Analyze the following research request comprehensively:
        
        Request: {state['user_request']}
        Context: {state.get('context', {})}
        
        Provide detailed analysis including:
        1. Intent analysis (what the user wants to achieve)
        2. Domain analysis (relevant fields and expertise areas)
        3. Scope analysis (breadth and depth of research needed)
        4. Complexity assessment (1-10 scale)
        5. Resource requirements and constraints
        6. Success criteria and quality metrics
        
        Use production-level analysis with specific, actionable insights.
        Return the result in JSON format with the following structure:
        {{
            "objectives": [{{"id": "obj_1", "description": "Research objective", "priority": "high"}}],
            "intent": {{"primary": "research", "secondary": "analysis"}},
            "domain": {{"fields": ["technology", "research"], "expertise": "general"}},
            "scope": {{"breadth": "comprehensive", "depth": "detailed"}},
            "complexity": 7.0
        }}
        """
        
        try:
            # Multi-Model Orchestrationìœ¼ë¡œ ë¶„ì„
            result = await execute_llm_task(
                prompt=analysis_prompt,
                task_type=TaskType.ANALYSIS,
                system_message="You are an expert research analyst with comprehensive domain knowledge."
            )
            
            logger.info(f"âœ… Analysis completed using model: {result.model_used}")
            logger.info(f"ğŸ“Š Analysis confidence: {result.confidence}")
            
            # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
            analysis_data = self._parse_analysis_result(result.content)
            
            logger.info(f"ğŸ¯ Identified objectives: {len(analysis_data.get('objectives', []))}")
            logger.info(f"ğŸ§  Complexity score: {analysis_data.get('complexity', 5.0)}")
            logger.info(f"ğŸ·ï¸ Domain: {analysis_data.get('domain', {}).get('fields', [])}")
            
            state.update({
                "analyzed_objectives": analysis_data.get("objectives", []),
                "intent_analysis": analysis_data.get("intent", {}),
                "domain_analysis": analysis_data.get("domain", {}),
                "scope_analysis": analysis_data.get("scope", {}),
                "complexity_score": analysis_data.get("complexity", 5.0),
                "current_step": "planning_agent",
                "innovation_stats": {
                    "analysis_model": result.model_used,
                    "analysis_confidence": result.confidence,
                    "analysis_time": result.execution_time
                }
            })
            
        except Exception as e:
            logger.error(f"âŒ Analysis failed: {e}")
            state["error_message"] = str(e)
            state["should_continue"] = False
            raise  # Fail-fast
        
        return state
    
    async def _planning_agent(self, state: ResearchState) -> ResearchState:
        """Planning Agent: MCP ê¸°ë°˜ ì‚¬ì „ ì¡°ì‚¬ â†’ Task ë¶„í•´ â†’ Agent ë™ì  í• ë‹¹."""
        logger.info("ğŸ¯ Planning Agent: MCP-based research planning")
        logger.info(f"ğŸ“Š Complexity Score: {state.get('complexity_score', 5.0)}")
        logger.info(f"ğŸ¯ Objectives: {len(state.get('analyzed_objectives', []))}")
        
        try:
            # 1. MCP ë„êµ¬ë¡œ ì‚¬ì „ ì¡°ì‚¬
            preliminary_research = await self._conduct_preliminary_research(state)
            logger.info(f"ğŸ” Preliminary research completed: {preliminary_research.get('sources_count', 0)} sources")
            
            # 2. Task ë¶„í•´ (ë³µì¡ë„ ê¸°ë°˜)
            tasks = await self._decompose_into_tasks(state, preliminary_research)
            logger.info(f"ğŸ“‹ Tasks decomposed: {len(tasks)} tasks")
            
            # 3. Agent ë™ì  í• ë‹¹ (ë³µì¡ë„ ê¸°ë°˜)
            agent_assignments = await self._assign_agents_dynamically(tasks, state)
            logger.info(f"ğŸ‘¥ Agent assignments: {len(agent_assignments)} task-agent mappings")
            
            # 4. ì‹¤í–‰ ì „ëµ ìˆ˜ë¦½
            execution_plan = await self._create_execution_plan(tasks, agent_assignments)
            logger.info(f"ğŸ“ˆ Execution strategy: {execution_plan.get('strategy', 'sequential')}")
            
            # Planning ê²°ê³¼ë¥¼ stateì— ì €ì¥
            state.update({
                "preliminary_research": preliminary_research,
                "planned_tasks": tasks,
                "agent_assignments": agent_assignments,
                "execution_plan": execution_plan,
                "plan_approved": False,
                "plan_feedback": None,
                "plan_iteration": state.get("plan_iteration", 0) + 1,
                "current_step": "verify_plan",
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "planning_agent": "active",
                    "preliminary_sources": preliminary_research.get('sources_count', 0),
                    "planned_tasks_count": len(tasks),
                    "agent_assignments_count": len(agent_assignments),
                    "execution_strategy": execution_plan.get('strategy', 'sequential')
                }
            })
            
            logger.info("âœ… Planning Agent completed successfully")
            return state
            
        except Exception as e:
            logger.error(f"âŒ Planning Agent failed: {e}")
            state["error_message"] = str(e)
            state["should_continue"] = False
            raise  # Fail-fast
    
    async def _verify_plan(self, state: ResearchState) -> ResearchState:
        """Plan ê²€ì¦: LLM ê¸°ë°˜ plan íƒ€ë‹¹ì„± ê²€ì¦."""
        logger.info("âœ… Verifying research plan")
        logger.info(f"ğŸ“‹ Tasks to verify: {len(state.get('planned_tasks', []))}")
        logger.info(f"ğŸ‘¥ Agent assignments: {len(state.get('agent_assignments', {}))}")
        
        try:
            verification_prompt = f"""
            Verify the following research plan for quality and completeness:
            
            Research Request: {state.get('user_request', '')}
            Objectives: {state.get('analyzed_objectives', [])}
            Domain: {state.get('domain_analysis', {})}
            Complexity Score: {state.get('complexity_score', 5.0)}
            
            Planned Tasks: {state.get('planned_tasks', [])}
            Agent Assignments: {state.get('agent_assignments', {})}
            Execution Plan: {state.get('execution_plan', {})}
            
            Check the following criteria:
            1. Completeness: Are all research objectives covered by the tasks?
            2. Agent Allocation: Is the number of agents appropriate for task complexity?
            3. Execution Strategy: Is the execution order and parallelization logical?
            4. Resource Efficiency: Are the estimated costs and time reasonable?
            5. Dependencies: Are task dependencies properly handled?
            6. MCP Tools: Are appropriate tools assigned to each task?
            
            Return your assessment in JSON format:
            {{
                "approved": boolean,
                "confidence": float (0.0-1.0),
                "feedback": "detailed feedback string",
                "suggested_changes": ["list of specific improvements"],
                "critical_issues": ["list of blocking issues if any"]
            }}
            """
            
            result = await execute_llm_task(
                prompt=verification_prompt,
                task_type=TaskType.VERIFICATION,
                system_message="You are an expert research planner and quality auditor with deep knowledge of research methodologies and resource optimization."
            )
            
            logger.info(f"ğŸ” Plan verification completed using model: {result.model_used}")
            logger.info(f"ğŸ“Š Verification confidence: {result.confidence}")
            
            # ê²€ì¦ ê²°ê³¼ íŒŒì‹±
            verification = self._parse_verification_result(result.content)
            
            if verification.get("approved", False):
                state["plan_approved"] = True
                state["plan_feedback"] = verification.get("feedback", "Plan approved")
                logger.info("âœ… Plan approved by verification")
                logger.info(f"ğŸ’¬ Feedback: {verification.get('feedback', '')}")
            else:
                state["plan_approved"] = False
                state["plan_feedback"] = verification.get("feedback", "Plan rejected")
                logger.warning(f"âŒ Plan rejected: {verification.get('feedback')}")
                logger.warning(f"ğŸ”§ Suggested changes: {verification.get('suggested_changes', [])}")
                
                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                max_iterations = 3
                if state.get("plan_iteration", 0) >= max_iterations:
                    logger.error(f"âŒ Maximum plan iterations ({max_iterations}) reached. Proceeding with current plan.")
                    state["plan_approved"] = True
                    state["plan_feedback"] = f"Plan approved after {max_iterations} iterations (forced)"
            
            state.update({
                "current_step": "adaptive_supervisor" if state.get("plan_approved", False) else "planning_agent",
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "plan_verification": "completed",
                    "plan_approved": state.get("plan_approved", False),
                    "verification_confidence": verification.get("confidence", 0.0),
                    "verification_iteration": state.get("plan_iteration", 0)
                }
            })
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Plan verification failed: {e}")
            state["error_message"] = str(e)
            state["should_continue"] = False
            raise  # Fail-fast
    
    async def _adaptive_supervisor(self, state: ResearchState) -> ResearchState:
        """Adaptive Supervisor (í˜ì‹  1)."""
        logger.info("ğŸ¯ Adaptive Supervisor allocating resources")
        
        complexity = state.get("complexity_score", 5.0)
        available_budget = self.llm_config.budget_limit
        
        # ë™ì  ì—°êµ¬ì í• ë‹¹
        allocated_researchers = min(
            max(int(complexity), self.agent_config.min_researchers),
            self.agent_config.max_researchers,
            int(available_budget / 10)  # ì˜ˆìƒ ë¹„ìš© ê¸°ë°˜
        )
        
        # ìš°ì„ ìˆœìœ„ í ìƒì„±
        priority_queue = self._create_priority_queue(state)
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •
        quality_threshold = self.agent_config.quality_threshold
        
        logger.info(f"ğŸ§  Complexity Score: {complexity}")
        logger.info(f"ğŸ‘¥ Allocated Researchers: {allocated_researchers}")
        logger.info(f"ğŸ“Š Quality Threshold: {quality_threshold}")
        logger.info(f"ğŸ“‹ Priority Queue Size: {len(priority_queue)}")
        logger.info(f"ğŸ’° Available Budget: ${available_budget}")
        
        state.update({
            "allocated_researchers": allocated_researchers,
            "priority_queue": priority_queue,
            "quality_threshold": quality_threshold,
            "current_step": "execute_research",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "allocated_researchers": allocated_researchers,
                "complexity_score": complexity,
                "priority_queue_size": len(priority_queue)
            }
        })
        
        return state
    
    async def _execute_research(self, state: ResearchState) -> ResearchState:
        """ì—°êµ¬ ì‹¤í–‰ (Universal MCP Hub + Streaming Pipeline)."""
        logger.info("ğŸ” Executing research with Universal MCP Hub and Streaming Pipeline")
        
        # Planning Agentì—ì„œ ìƒì„±ëœ tasks ì‚¬ìš©
        tasks = state.get("planned_tasks", [])
        agent_assignments = state.get("agent_assignments", {})
        execution_plan = state.get("execution_plan", {})
        
        logger.info(f"ğŸ“‹ Executing {len(tasks)} planned tasks")
        logger.info(f"ğŸ‘¥ Agent assignments: {len(agent_assignments)} mappings")
        logger.info(f"ğŸ“ˆ Execution strategy: {execution_plan.get('strategy', 'sequential')}")
        
        execution_results = []
        streaming_data = []
        
        # ê° ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        for task in tasks:
            try:
                # MCP ë„êµ¬ ì„ íƒ
                tool_category = self._get_tool_category_for_task(task)
                best_tool = self._get_best_tool_for_category(tool_category)
                
                if best_tool:
                    # MCP ë„êµ¬ ì‹¤í–‰
                    logger.info(f"ğŸ”§ Executing MCP tool: {best_tool}")
                    tool_result = await execute_tool(
                        best_tool,
                        task.get("parameters", {})
                    )
                    logger.info(f"âœ… Tool '{best_tool}' executed successfully")
                    
                    if tool_result.get("success", False):
                        execution_results.append({
                            "task_id": task.get("id"),
                            "task_name": task.get("name"),
                            "tool_used": best_tool,
                            "result": tool_result.get("data"),
                            "execution_time": tool_result.get("execution_time", 0.0),
                            "confidence": tool_result.get("confidence", 0.0)
                        })

                        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì¶”ê°€
                        streaming_data.append({
                            "timestamp": datetime.now().isoformat(),
                            "task_id": task.get("id"),
                            "status": "completed",
                            "data": tool_result.get("data")
                        })
                    else:
                        logger.warning(f"Task {task.get('id')} failed: {tool_result.get('error', 'Unknown error')}")
                else:
                    logger.warning(f"No suitable tool found for task {task.get('id')}")
                    
            except Exception as e:
                logger.error(f"Error executing task {task.get('id')}: {e}")
        
        state.update({
            "execution_results": execution_results,
            "streaming_data": streaming_data,
            "current_step": "hierarchical_compression",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "tasks_executed": len(execution_results),
                "tools_used": len(set(r.get("tool_used", "") for r in execution_results if r.get("tool_used"))),
                "execution_success_rate": float(len(execution_results)) / max(len(tasks), 1)
            }
        })
        
        return state
    
    async def _hierarchical_compression(self, state: ResearchState) -> ResearchState:
        """Hierarchical Compression (í˜ì‹  2)."""
        logger.info("ğŸ—œï¸ Applying Hierarchical Compression")
        
        execution_results = state.get("execution_results", [])
        compression_results = []
        
        # ê° ì‹¤í–‰ ê²°ê³¼ì— ëŒ€í•´ ì••ì¶• ì ìš©
        for result in execution_results:
            try:
                # ë°ì´í„° ì••ì¶•
                compressed = await compress_data(result.get("result", {}))
                
                compression_results.append({
                    "task_id": result.get("task_id"),
                    "original_size": len(str(result.get("result", {}))),
                    "compressed_size": len(str(compressed.data)),
                    "compression_ratio": compressed.compression_ratio,
                    "validation_score": compressed.validation_score,
                    "compressed_data": compressed.data,
                    "important_info_preserved": compressed.important_info_preserved
                })
                
            except Exception as e:
                logger.warning(f"Compression failed for task {result.get('task_id')}: {e}")
                # ì••ì¶• ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                compression_results.append({
                    "task_id": result.get("task_id"),
                    "original_size": len(str(result.get("result", {}))),
                    "compressed_size": len(str(result.get("result", {}))),
                    "compression_ratio": 1.0,
                    "validation_score": 1.0,
                    "compressed_data": result.get("result", {}),
                    "important_info_preserved": []
                })
        
        # ì „ì²´ ì••ì¶• í†µê³„
        total_original = sum(c.get("original_size", 0) for c in compression_results)
        total_compressed = sum(c.get("compressed_size", 0) for c in compression_results)
        overall_compression_ratio = total_compressed / max(total_original, 1)
        
        state.update({
            "compression_results": compression_results,
            "compression_metadata": {
                "overall_compression_ratio": overall_compression_ratio,
                "total_original_size": total_original,
                "total_compressed_size": total_compressed,
                "compression_count": len(compression_results)
            },
            "current_step": "continuous_verification",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "compression_ratio": float(overall_compression_ratio),
                "compression_applied": len(compression_results)
            }
        })
        
        return state
    
    async def _continuous_verification(self, state: ResearchState) -> ResearchState:
        """Continuous Verification (í˜ì‹  4)."""
        logger.info("ğŸ”¬ Applying Continuous Verification")
        
        compression_results = state.get("compression_results", [])
        verification_stages = []
        confidence_scores = {}
        
        # 3ë‹¨ê³„ ê²€ì¦
        for i, result in enumerate(compression_results):
            task_id = result.get("task_id")
            
            # Stage 1: Self-Verification
            self_score = await self._self_verification(result)
            
            # Stage 2: Cross-Verification
            cross_score = await self._cross_verification(result, compression_results)
            
            # Stage 3: External Verification (ì„ íƒì )
            if self_score < 0.7 or cross_score < 0.7:
                external_score = await self._external_verification(result)
            else:
                external_score = 1.0
            
            # ì¢…í•© ì‹ ë¢°ë„ ì ìˆ˜
            final_score = (self_score * 0.3 + cross_score * 0.4 + external_score * 0.3)
            
            verification_stages.append({
                "task_id": task_id,
                "stage_1_self": self_score,
                "stage_2_cross": cross_score,
                "stage_3_external": external_score,
                "final_score": final_score
            })
            
            confidence_scores[task_id] = final_score
        
        state.update({
            "verification_stages": verification_stages,
            "confidence_scores": confidence_scores,
            "current_step": "evaluate_results",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "verification_applied": len(verification_stages),
                "avg_confidence": float(sum(confidence_scores.values())) / max(len(confidence_scores), 1)
            }
        })
        
        return state
    
    async def _evaluate_results(self, state: ResearchState) -> ResearchState:
        """ê²°ê³¼ í‰ê°€ (Multi-Model Orchestration)."""
        logger.info("ğŸ“Š Evaluating results with Multi-Model Orchestration")
        
        evaluation_prompt = f"""
        Evaluate the following research results comprehensively:
        
        Execution Results: {state.get('execution_results', [])}
        Compression Results: {state.get('compression_results', [])}
        Verification Results: {state.get('verification_stages', [])}
        Confidence Scores: {state.get('confidence_scores', {})}
        
        Provide detailed evaluation including:
        1. Quality assessment with metrics
        2. Completeness analysis
        3. Accuracy verification
        4. Improvement recommendations
        5. Risk assessment
        6. Overall satisfaction score
        
        Use production-level evaluation with specific, actionable insights.
        Return the result in JSON format with the following structure:
        {{
            "overall_score": 0.85,
            "metrics": {{"quality": 0.8, "completeness": 0.9, "accuracy": 0.85}},
            "improvements": ["Add more sources", "Improve analysis depth"]
        }}
        """
        
        # Multi-Model Orchestrationìœ¼ë¡œ í‰ê°€
        result = await execute_llm_task(
            prompt=evaluation_prompt,
            task_type=TaskType.VERIFICATION,
            system_message="You are an expert research evaluator with comprehensive quality assessment capabilities.",
            use_ensemble=True  # Weighted Ensemble ì‚¬ìš©
        )
        
        # í‰ê°€ ê²°ê³¼ íŒŒì‹±
        evaluation_data = self._parse_evaluation_result(result.content)
        
        state.update({
            "evaluation_results": evaluation_data,
            "quality_metrics": evaluation_data.get("metrics", {}),
            "improvement_areas": evaluation_data.get("improvements", []),
            "current_step": "validate_results",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "evaluation_model": result.model_used,
                "evaluation_confidence": result.confidence,
                "quality_score": evaluation_data.get("overall_score", 0.8)
            }
        })
        
        return state
    
    async def _validate_results(self, state: ResearchState) -> ResearchState:
        """ê²°ê³¼ ê²€ì¦."""
        logger.info("âœ… Validating results")
        
        # ê²€ì¦ ë¡œì§
        validation_score = self._calculate_validation_score(state)
        missing_elements = self._identify_missing_elements(state)
        
        state.update({
            "validation_score": validation_score,
            "missing_elements": missing_elements,
            "current_step": "synthesize_deliverable",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "validation_score": validation_score,
                "missing_elements_count": len(missing_elements)
            }
        })
        
        return state
    
    async def _synthesize_deliverable(self, state: ResearchState) -> ResearchState:
        """ìµœì¢… ê²°ê³¼ ì¢…í•© (Adaptive Context Window)."""
        logger.info("ğŸ“ Synthesizing final deliverable with Adaptive Context Window")
        
        synthesis_prompt = f"""
        Synthesize the following research findings into a comprehensive deliverable:
        
        User Request: {state.get('user_request', '')}
        Execution Results: {state.get('execution_results', [])}
        Compression Results: {state.get('compression_results', [])}
        Verification Results: {state.get('verification_stages', [])}
        Evaluation Results: {state.get('evaluation_results', {})}
        Quality Metrics: {state.get('quality_metrics', {})}
        
        Create a comprehensive synthesis including:
        1. Executive summary with key insights
        2. Detailed findings with evidence
        3. Analysis and interpretation
        4. Conclusions and recommendations
        5. Limitations and future work
        6. Appendices with supporting data
        
        Use adaptive context management for optimal content organization.
        """
        
        # Multi-Model Orchestrationìœ¼ë¡œ ì¢…í•©
        result = await execute_llm_task(
            prompt=synthesis_prompt,
            task_type=TaskType.SYNTHESIS,
            system_message="You are an expert research synthesizer with adaptive context window capabilities."
        )
        
        # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©ëŸ‰ ê³„ì‚°
        context_usage = self._calculate_context_usage(state, result.content)
        
        state.update({
            "final_synthesis": {
                "content": result.content,
                "model_used": result.model_used,
                "confidence": result.confidence,
                "execution_time": result.execution_time
            },
            "context_window_usage": context_usage,
            "current_step": "completed",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "synthesis_model": result.model_used,
                "synthesis_confidence": result.confidence,
                "context_window_usage": context_usage.get("usage_ratio", 1.0)
            }
        })
        
        return state
    
    # ==================== Planning Agent Helper Methods ====================
    
    async def _conduct_preliminary_research(self, state: ResearchState) -> Dict[str, Any]:
        """MCP ë„êµ¬ë¡œ ì‚¬ì „ ì¡°ì‚¬ ìˆ˜í–‰."""
        logger.info("ğŸ” Conducting preliminary research with MCP tools")
        
        objectives = state.get('analyzed_objectives', [])
        domain = state.get('domain_analysis', {})
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(objectives, domain)
        logger.info(f"ğŸ”‘ Extracted keywords: {keywords[:5]}")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
        
        # MCP ë„êµ¬ë¡œ ê²€ìƒ‰
        search_results = []
        search_tools = ["g-search", "tavily", "exa"]  # ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ë„êµ¬
        
        for i, keyword in enumerate(keywords[:3]):  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            tool_name = search_tools[i % len(search_tools)]  # ë„êµ¬ ìˆœí™˜ ì‚¬ìš©
            
            try:
                result = await execute_tool(
                    tool_name=tool_name,
                    parameters={"query": keyword, "max_results": 5}
                )
                
                if result.success:
                    search_results.append({
                        "keyword": keyword,
                        "tool": tool_name,
                        "data": result.data,
                        "sources_count": len(result.data) if isinstance(result.data, list) else 1
                    })
                    logger.info(f"âœ… {tool_name} search for '{keyword}': {len(result.data) if isinstance(result.data, list) else 1} results")
                else:
                    logger.warning(f"âš ï¸ {tool_name} search failed for '{keyword}': {result.error}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {tool_name} search error for '{keyword}': {e}")
        
        # í•™ìˆ  ê²€ìƒ‰ (arxiv, scholar)
        academic_results = []
        academic_tools = ["arxiv", "scholar"]
        
        for tool_name in academic_tools:
            try:
                result = await execute_tool(
                    tool_name=tool_name,
                    parameters={"query": " ".join(keywords[:2]), "max_results": 3}
                )
                
                if result.success:
                    academic_results.append({
                        "tool": tool_name,
                        "data": result.data,
                        "sources_count": len(result.data) if isinstance(result.data, list) else 1
                    })
                    logger.info(f"âœ… {tool_name} academic search: {len(result.data) if isinstance(result.data, list) else 1} results")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {tool_name} academic search error: {e}")
        
        return {
            "keywords": keywords,
            "search_results": search_results,
            "academic_results": academic_results,
            "sources_count": len(search_results) + len(academic_results),
            "total_results": sum(r.get("sources_count", 0) for r in search_results + academic_results)
        }
    
    def _extract_keywords(self, objectives: List[Dict[str, Any]], domain: Dict[str, Any]) -> List[str]:
        """ëª©í‘œì™€ ë„ë©”ì¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ."""
        keywords = []
        
        # Objectivesì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        for obj in objectives:
            description = obj.get('description', '')
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš©)
            words = description.lower().split()
            keywords.extend([w for w in words if len(w) > 3 and w not in ['the', 'and', 'for', 'with', 'from']])
        
        # Domainì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        fields = domain.get('fields', [])
        keywords.extend(fields)
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        from collections import Counter
        keyword_counts = Counter(keywords)
        return [kw for kw, count in keyword_counts.most_common(10)]
    
    async def _decompose_into_tasks(
        self, 
        state: ResearchState, 
        preliminary_research: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ë³µì¡ë„ ê¸°ë°˜ task ë¶„í•´."""
        logger.info("ğŸ“‹ Decomposing research into specific tasks")
        
        complexity = state.get('complexity_score', 5.0)
        
        # ë³µì¡ë„ì— ë”°ë¥¸ task ê°œìˆ˜ ê²°ì •
        if complexity <= 5:
            num_tasks = 3 + int(complexity)  # 3-8ê°œ
        elif complexity <= 8:
            num_tasks = 5 + int(complexity)  # 5-13ê°œ
        else:
            num_tasks = 8 + int(complexity * 0.5)  # 8-13ê°œ
        
        logger.info(f"ğŸ“Š Target task count: {num_tasks} (complexity: {complexity})")
        
        # LLMìœ¼ë¡œ task ìƒì„± (ì‚¬ì „ ì¡°ì‚¬ ê²°ê³¼ í¬í•¨)
        decomposition_prompt = f"""
        Based on preliminary research, decompose the research into {num_tasks} specific, executable tasks:
        
        Research Request: {state.get('user_request', '')}
        Objectives: {state.get('analyzed_objectives', [])}
        Domain: {state.get('domain_analysis', {})}
        Complexity Score: {complexity}
        
        Preliminary Research:
        - Keywords: {preliminary_research.get('keywords', [])}
        - Search Results: {len(preliminary_research.get('search_results', []))} sources
        - Academic Results: {len(preliminary_research.get('academic_results', []))} sources
        
        For each task, provide the following structure:
        {{
            "task_id": "task_1",
            "name": "Specific task name",
            "description": "Detailed task description",
            "type": "academic|market|technical|data|synthesis",
            "assigned_agent_type": "academic_researcher|market_analyst|technical_researcher|data_collector|synthesis_specialist",
            "required_tools": ["g-search", "arxiv", "tavily"],
            "dependencies": ["task_0"],
            "estimated_complexity": 1-10,
            "priority": "high|medium|low",
            "estimated_time": 30,
            "success_criteria": ["specific measurable criteria"]
        }}
        
        Ensure tasks cover all research objectives and have logical dependencies.
        Return as JSON array of task objects.
        """
        
        result = await execute_llm_task(
            prompt=decomposition_prompt,
            task_type=TaskType.PLANNING,
            system_message="You are an expert research project manager with deep knowledge of task decomposition and resource allocation."
        )
        
        logger.info(f"âœ… Task decomposition completed using model: {result.model_used}")
        
        # Task ê²°ê³¼ íŒŒì‹±
        tasks = self._parse_tasks_result(result.content)
        
        # Task ê²€ì¦ ë° ë¡œê¹…
        for i, task in enumerate(tasks):
            logger.info(f"  Task {i+1}: {task.get('name', 'Unknown')} ({task.get('type', 'research')}) - {task.get('assigned_agent_type', 'unknown')} agent")
        
        return tasks
    
    async def _assign_agents_dynamically(
        self,
        tasks: List[Dict[str, Any]],
        state: ResearchState
    ) -> Dict[str, List[str]]:
        """ë³µì¡ë„ ê¸°ë°˜ ë™ì  agent í• ë‹¹."""
        logger.info("ğŸ‘¥ Assigning agents dynamically based on task complexity")
        
        agent_assignments = {}
        available_researchers = state.get('allocated_researchers', 1)
        
        for task in tasks:
            task_id = task.get('task_id', 'unknown')
            complexity = task.get('estimated_complexity', 5)
            task_type = task.get('type', 'research')
            
            # ë³µì¡ë„ì— ë”°ë¥¸ agent ìˆ˜ ê²°ì •
            if complexity <= 3:
                num_agents = 1
            elif complexity <= 7:
                num_agents = min(2, available_researchers)
            else:
                num_agents = min(3, available_researchers)
            
            # Agent ìœ í˜• ê²°ì •
            agent_types = self._select_agent_types(task_type, num_agents)
            
            agent_assignments[task_id] = agent_types
            
            logger.info(f"  {task_id}: {num_agents} agents ({', '.join(agent_types)}) for complexity {complexity}")
        
        return agent_assignments
    
    def _select_agent_types(self, task_type: str, num_agents: int) -> List[str]:
        """Task ìœ í˜•ì— ë”°ë¥¸ agent ìœ í˜• ì„ íƒ."""
        agent_type_mapping = {
            "academic": ["academic_researcher"],
            "market": ["market_analyst"],
            "technical": ["technical_researcher"],
            "data": ["data_collector"],
            "synthesis": ["synthesis_specialist"],
            "research": ["academic_researcher", "technical_researcher"]
        }
        
        base_types = agent_type_mapping.get(task_type, ["academic_researcher"])
        
        # í•„ìš”í•œ ìˆ˜ë§Œí¼ agent ìœ í˜• ë°˜í™˜
        if num_agents <= len(base_types):
            return base_types[:num_agents]
        else:
            # ë¶€ì¡±í•œ ê²½ìš° ë‹¤ë¥¸ ìœ í˜• ì¶”ê°€
            additional_types = ["market_analyst", "technical_researcher", "data_collector", "synthesis_specialist"]
            result = base_types.copy()
            for agent_type in additional_types:
                if len(result) >= num_agents:
                    break
                if agent_type not in result:
                    result.append(agent_type)
            return result[:num_agents]
    
    async def _create_execution_plan(
        self,
        tasks: List[Dict[str, Any]],
        agent_assignments: Dict[str, List[str]]
    ) -> Dict[str, Any]:
        """ì‹¤í–‰ ì „ëµ ìˆ˜ë¦½."""
        logger.info("ğŸ“ˆ Creating execution plan")
        
        # ì˜ì¡´ì„± ë¶„ì„
        dependency_graph = self._build_dependency_graph(tasks)
        
        # ë³‘ë ¬ ê°€ëŠ¥í•œ task ê·¸ë£¹ ì‹ë³„
        parallel_groups = self._identify_parallel_groups(dependency_graph)
        
        # ì‹¤í–‰ ìˆœì„œ ê²°ì •
        execution_order = self._determine_execution_order(tasks, dependency_graph)
        
        # ì „ëµ ê²°ì •
        strategy = "hybrid" if parallel_groups else "sequential"
        
        # ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
        estimated_total_time = sum(task.get('estimated_time', 30) for task in tasks)
        
        execution_plan = {
            "strategy": strategy,
            "parallel_groups": parallel_groups,
            "execution_order": execution_order,
            "estimated_total_time": estimated_total_time,
            "dependency_graph": dependency_graph,
            "task_count": len(tasks),
            "agent_count": len(set(agent for agents in agent_assignments.values() for agent in agents))
        }
        
        logger.info(f"ğŸ“Š Execution plan: {strategy} strategy, {len(parallel_groups)} parallel groups, {estimated_total_time}min total")
        
        return execution_plan
    
    def _build_dependency_graph(self, tasks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """Task ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•."""
        graph = {}
        
        for task in tasks:
            task_id = task.get('task_id', '')
            dependencies = task.get('dependencies', [])
            graph[task_id] = dependencies
        
        return graph
    
    def _identify_parallel_groups(self, dependency_graph: Dict[str, List[str]]) -> List[List[str]]:
        """ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ task ê·¸ë£¹ ì‹ë³„."""
        # ê°„ë‹¨í•œ êµ¬í˜„: ì˜ì¡´ì„±ì´ ì—†ëŠ” taskë“¤ì„ ê·¸ë£¹í™”
        parallel_groups = []
        processed = set()
        
        for task_id, dependencies in dependency_graph.items():
            if task_id in processed:
                continue
                
            if not dependencies:  # ì˜ì¡´ì„±ì´ ì—†ëŠ” task
                group = [task_id]
                # ë‹¤ë¥¸ ì˜ì¡´ì„± ì—†ëŠ” taskë“¤ ì°¾ê¸°
                for other_task, other_deps in dependency_graph.items():
                    if other_task != task_id and other_task not in processed and not other_deps:
                        group.append(other_task)
                        processed.add(other_task)
                
                if len(group) > 1:
                    parallel_groups.append(group)
                    processed.update(group)
        
        return parallel_groups
    
    def _determine_execution_order(self, tasks: List[Dict[str, Any]], dependency_graph: Dict[str, List[str]]) -> List[str]:
        """ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ì‹¤í–‰ ìˆœì„œ ê²°ì •."""
        # ìœ„ìƒ ì •ë ¬ì„ ì‚¬ìš©í•œ ì‹¤í–‰ ìˆœì„œ ê²°ì •
        in_degree = {task_id: 0 for task_id in dependency_graph.keys()}
        
        # ì§„ì… ì°¨ìˆ˜ ê³„ì‚°
        for task_id, dependencies in dependency_graph.items():
            for dep in dependencies:
                if dep in in_degree:
                    in_degree[task_id] += 1
        
        # ìœ„ìƒ ì •ë ¬
        queue = [task_id for task_id, degree in in_degree.items() if degree == 0]
        result = []
        
        while queue:
            current = queue.pop(0)
            result.append(current)
            
            # í˜„ì¬ taskì— ì˜ì¡´í•˜ëŠ” taskë“¤ì˜ ì§„ì… ì°¨ìˆ˜ ê°ì†Œ
            for task_id, dependencies in dependency_graph.items():
                if current in dependencies:
                    in_degree[task_id] -= 1
                    if in_degree[task_id] == 0:
                        queue.append(task_id)
        
        return result
    
    # ==================== Helper Methods ====================
    
    def _parse_analysis_result(self, content: str) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ íŒŒì‹±."""
        try:
            import json
            # JSON íŒŒì‹± ì‹œë„
            if content.strip().startswith('{'):
                return json.loads(content)
            else:
                # JSONì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
                raise ValueError("Invalid JSON format in analysis result")
        except Exception as e:
            logger.error(f"âŒ Failed to parse analysis result: {e}")
            raise ValueError(f"Analysis parsing failed: {e}")
    
    def _parse_tasks_result(self, content: str) -> List[Dict[str, Any]]:
        """Task ë¶„í•´ ê²°ê³¼ íŒŒì‹±."""
        try:
            import json
            # JSON ë°°ì—´ë¡œ íŒŒì‹± ì‹œë„
            if content.strip().startswith('['):
                return json.loads(content)
            else:
                # JSONì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
                raise ValueError("Invalid JSON format in task decomposition result")
        except Exception as e:
            logger.error(f"âŒ Failed to parse tasks result: {e}")
            raise ValueError(f"Task parsing failed: {e}")
    
    def _parse_verification_result(self, content: str) -> Dict[str, Any]:
        """Plan ê²€ì¦ ê²°ê³¼ íŒŒì‹±."""
        try:
            import json
            # JSON íŒŒì‹± ì‹œë„
            if content.strip().startswith('{'):
                return json.loads(content)
            else:
                # JSONì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
                raise ValueError("Invalid JSON format in verification result")
        except Exception as e:
            logger.error(f"âŒ Failed to parse verification result: {e}")
            raise ValueError(f"Verification parsing failed: {e}")
    
    def _parse_evaluation_result(self, content: str) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±."""
        try:
            import json
            # JSON íŒŒì‹± ì‹œë„
            if content.strip().startswith('{'):
                return json.loads(content)
            else:
                # JSONì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
                raise ValueError("Invalid JSON format in evaluation result")
        except Exception as e:
            logger.error(f"âŒ Failed to parse evaluation result: {e}")
            raise ValueError(f"Evaluation parsing failed: {e}")
    
    def _create_priority_queue(self, state: ResearchState) -> List[Dict[str, Any]]:
        """ìš°ì„ ìˆœìœ„ í ìƒì„±."""
        tasks = state.get("planned_tasks", [])
        priority_queue = []
        
        for task in tasks:
            priority = 1 if task.get("priority") == "high" else 2 if task.get("priority") == "medium" else 3
            priority_queue.append({
                "task_id": task.get("task_id", ""),
                "priority": priority,
                "estimated_time": task.get("estimated_time", 30),
                "complexity": task.get("estimated_complexity", 5)
            })
        
        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        priority_queue.sort(key=lambda x: (x["priority"], x["complexity"]))
        return priority_queue
    
    def _get_tool_category_for_task(self, task: Dict[str, Any]) -> ToolCategory:
        """ì‘ì—…ì— ì í•©í•œ ë„êµ¬ ì¹´í…Œê³ ë¦¬ ë°˜í™˜."""
        task_type = task.get("type", "research").lower()
        if "search" in task_type:
            return ToolCategory.SEARCH
        elif "academic" in task_type:
            return ToolCategory.ACADEMIC
        elif "data" in task_type:
            return ToolCategory.DATA
        else:
            return ToolCategory.SEARCH  # RESEARCH ëŒ€ì‹  SEARCH ì‚¬ìš©
    
    def _get_best_tool_for_category(self, category: ToolCategory) -> Optional[str]:
        """ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ìµœì ì˜ ë„êµ¬ ë°˜í™˜."""
        tool_mapping = {
            ToolCategory.SEARCH: "g-search",
            ToolCategory.DATA: "fetch",
            ToolCategory.CODE: "python_coder",
            ToolCategory.ACADEMIC: "arxiv",
            ToolCategory.BUSINESS: "crunchbase"
        }
        return tool_mapping.get(category, "g-search")  # ê¸°ë³¸ê°’ìœ¼ë¡œ g-search ì‚¬ìš©
    
    async def _self_verification(self, result: Dict[str, Any]) -> float:
        """ìì²´ ê²€ì¦."""
        try:
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            data = result.get("compressed_data", {})
            if not data:
                return 0.0
            
            # ê¸°ë³¸ì ì¸ ë°ì´í„° ê²€ì¦
            score = 0.5
            
            # ë°ì´í„° ì™„ì„±ë„ ê²€ì¦
            if isinstance(data, dict) and len(data) > 0:
                score += 0.2
            
            # ì¤‘ìš” ì •ë³´ ë³´ì¡´ ê²€ì¦
            important_info = result.get("important_info_preserved", [])
            if important_info and len(important_info) > 0:
                score += 0.3
            
            return min(score, 1.0)
        except Exception as e:
            logger.error(f"âŒ Self verification failed: {e}")
            return 0.0
    
    async def _cross_verification(self, result: Dict[str, Any], all_results: List[Dict[str, Any]]) -> float:
        """êµì°¨ ê²€ì¦."""
        try:
            if not all_results or len(all_results) < 2:
                return 0.5
            
            # ë‹¤ë¥¸ ê²°ê³¼ì™€ì˜ ì¼ì¹˜ë„ ê²€ì‚¬
            current_data = result.get("compressed_data", {})
            if not current_data:
                return 0.0
            
            consistency_score = 0.0
            comparison_count = 0
            
            for other_result in all_results:
                if other_result.get("task_id") == result.get("task_id"):
                    continue
                
                other_data = other_result.get("compressed_data", {})
                if not other_data:
                    continue
                
                # ê°„ë‹¨í•œ ì¼ì¹˜ë„ ê²€ì‚¬ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”)
                if isinstance(current_data, dict) and isinstance(other_data, dict):
                    common_keys = set(current_data.keys()) & set(other_data.keys())
                    if common_keys:
                        consistency_score += len(common_keys) / max(len(current_data.keys()), len(other_data.keys()))
                        comparison_count += 1
            
            if comparison_count > 0:
                return consistency_score / comparison_count
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"âŒ Cross verification failed: {e}")
            return 0.0
    
    async def _external_verification(self, result: Dict[str, Any]) -> float:
        """ì™¸ë¶€ ê²€ì¦."""
        try:
            # MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì™¸ë¶€ ê²€ì¦
            task_id = result.get("task_id", "")
            data = result.get("compressed_data", {})
            
            if not data or not task_id:
                return 0.5
            
            # ê°„ë‹¨í•œ ì™¸ë¶€ ê²€ì¦ (ì‹¤ì œë¡œëŠ” MCP ë„êµ¬ í™œìš©)
            # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ë°ì´í„° ìœ íš¨ì„±ë§Œ ê²€ì‚¬
            if isinstance(data, dict) and len(data) > 0:
                return 0.8
            elif isinstance(data, list) and len(data) > 0:
                return 0.7
            else:
                return 0.6
                
        except Exception as e:
            logger.error(f"âŒ External verification failed: {e}")
            return 0.0
    
    def _calculate_validation_score(self, state: ResearchState) -> float:
        """ê²€ì¦ ì ìˆ˜ ê³„ì‚°."""
        try:
            confidence_scores = state.get("confidence_scores", {})
            if not confidence_scores:
                return 0.0
            
            # í‰ê·  ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            total_score = sum(confidence_scores.values())
            avg_score = total_score / len(confidence_scores)
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°˜ì˜
            quality_metrics = state.get("quality_metrics", {})
            if quality_metrics:
                quality_score = quality_metrics.get("overall_quality", 0.8)
                avg_score = (avg_score + quality_score) / 2
            
            return min(avg_score, 1.0)
        except Exception as e:
            logger.error(f"âŒ Validation score calculation failed: {e}")
            return 0.0
    
    def _identify_missing_elements(self, state: ResearchState) -> List[str]:
        """ëˆ„ë½ëœ ìš”ì†Œ ì‹ë³„."""
        try:
            missing_elements = []
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
            required_fields = ["analyzed_objectives", "planned_tasks", "execution_results"]
            for field in required_fields:
                if not state.get(field):
                    missing_elements.append(f"Missing {field}")
            
            # ì‹¤í–‰ ê²°ê³¼ ê²€ì‚¬
            execution_results = state.get("execution_results", [])
            if not execution_results:
                missing_elements.append("No execution results found")
            
            # ì••ì¶• ê²°ê³¼ ê²€ì‚¬
            compression_results = state.get("compression_results", [])
            if not compression_results:
                missing_elements.append("No compression results found")
            
            # ê²€ì¦ ê²°ê³¼ ê²€ì‚¬
            verification_stages = state.get("verification_stages", [])
            if not verification_stages:
                missing_elements.append("No verification results found")
            
            return missing_elements
        except Exception as e:
            logger.error(f"âŒ Missing elements identification failed: {e}")
            return ["Error in missing elements analysis"]
    
    def _calculate_context_usage(self, state: ResearchState, content: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©ëŸ‰ ê³„ì‚°."""
        try:
            # ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í† í°í™” í•„ìš”)
            estimated_tokens = len(content.split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ìˆ˜
            
            # ìµœëŒ€ í† í° ìˆ˜ (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
            max_tokens = 100000  # ê¸°ë³¸ê°’
            
            usage_ratio = min(estimated_tokens / max_tokens, 1.0)
            
            return {
                "usage_ratio": usage_ratio,
                "tokens_used": int(estimated_tokens),
                "max_tokens": max_tokens,
                "efficiency": 1.0 - usage_ratio
            }
        except Exception as e:
            logger.error(f"âŒ Context usage calculation failed: {e}")
            return {
                "usage_ratio": 0.0,
                "tokens_used": 0,
                "max_tokens": 100000,
                "efficiency": 1.0
            }
    
    async def run_research(self, user_request: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì—°êµ¬ ì‹¤í–‰ (Production-Grade Reliability)."""
        logger.info(f"ğŸš€ Starting research with 8 core innovations: {user_request}")
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = ResearchState(
            user_request=user_request,
            context=context or {},
            objective_id=f"obj_{int(datetime.now().timestamp())}",
            analyzed_objectives=[],
            intent_analysis={},
            domain_analysis={},
            scope_analysis={},
            # Planning Agent í•„ë“œ
            preliminary_research={},
            planned_tasks=[],
            agent_assignments={},
            execution_plan={},
            plan_approved=False,
            plan_feedback=None,
            plan_iteration=0,
            execution_results=[],
            agent_status={},
            execution_metadata={},
            streaming_data=[],
            compression_results=[],
            compression_metadata={},
            verification_results={},
            confidence_scores={},
            verification_stages=[],
            evaluation_results={},
            quality_metrics={},
            improvement_areas=[],
            validation_results={},
            validation_score=0.0,
            missing_elements=[],
            final_synthesis={},
            deliverable_path=None,
            synthesis_metadata={},
            context_window_usage={},
            current_step="analyze_objectives",
            iteration=0,
            max_iterations=10,
            should_continue=True,
            error_message=None,
            innovation_stats={},
            messages=[]
        )

        # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        logger.info("ğŸ”„ Executing LangGraph workflow with 8 core innovations")
        final_state = await self.graph.ainvoke(initial_state)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result = {
            "content": final_state.get("final_synthesis", {}).get("content", "Research completed"),
            "metadata": {
                "model_used": final_state.get("final_synthesis", {}).get("model_used", "unknown"),
                "execution_time": final_state.get("final_synthesis", {}).get("execution_time", 0.0),
                "cost": 0.0,
                "confidence": final_state.get("final_synthesis", {}).get("confidence", 0.9)
            },
            "synthesis_results": {
                "content": final_state.get("final_synthesis", {}).get("content", ""),
                "original_length": len(str(final_state.get("execution_results", []))),
                "compressed_length": len(str(final_state.get("compression_results", []))),
                "compression_ratio": final_state.get("compression_metadata", {}).get("overall_compression_ratio", 1.0)
            },
            "innovation_stats": final_state.get("innovation_stats", {}),
            "system_health": {"overall_status": "healthy", "health_score": 95},
            "detailed_results": {
                "analyzed_objectives": final_state.get("analyzed_objectives", []),
                "planned_tasks": final_state.get("planned_tasks", []),
                "execution_results": final_state.get("execution_results", []),
                "compression_results": final_state.get("compression_results", []),
                "verification_stages": final_state.get("verification_stages", []),
                "evaluation_results": final_state.get("evaluation_results", {}),
                "quality_metrics": final_state.get("quality_metrics", {})
            }
        }
        
        logger.info("âœ… Research completed successfully with 8 core innovations")
        return result


# Global orchestrator instance
orchestrator = AutonomousOrchestrator()


async def run_research(user_request: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """ì—°êµ¬ ì‹¤í–‰."""
    return await orchestrator.run_research(user_request, context)

