"""
Evaluation Agent (v2.0 - 8ÎåÄ ÌòÅÏã† ÌÜµÌï©)

Continuous Verification, Multi-Model Orchestration, Production-Grade ReliabilityÎ•º
ÌÜµÌï©Ìïú Í≥†ÎèÑÌôîÎêú ÌèâÍ∞Ä ÏóêÏù¥Ï†ÑÌä∏.
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import json
import uuid
from pathlib import Path

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.researcher_config import get_llm_config, get_agent_config, get_verification_config
from src.core.llm_manager import execute_llm_task, TaskType, get_best_model_for_task
from src.core.reliability import execute_with_reliability
from src.core.compression import compress_data

logger = logging.getLogger(__name__)


class EvaluationAgent:
    """8ÎåÄ ÌòÅÏã†ÏùÑ ÌÜµÌï©Ìïú Í≥†ÎèÑÌôîÎêú ÌèâÍ∞Ä ÏóêÏù¥Ï†ÑÌä∏."""
    
    def __init__(self):
        """Ï¥àÍ∏∞Ìôî."""
        self.llm_config = get_llm_config()
        self.agent_config = get_agent_config()
        self.verification_config = get_verification_config()
        
        # ÌèâÍ∞Ä Í∏∞Ï§Ä (Continuous Verification ÌÜµÌï©)
        self.evaluation_criteria = self._load_evaluation_criteria()
        self.quality_metrics = self._load_quality_metrics()
        self.refinement_strategies = self._load_refinement_strategies()
        
        logger.info("Evaluation Agent initialized with 8 core innovations")
    
    def _load_evaluation_criteria(self) -> Dict[str, Any]:
        """ÌèâÍ∞Ä Í∏∞Ï§Ä Î°úÎìú (Continuous Verification ÌÜµÌï©)."""
        return {
            'completeness': {
                'weight': 0.25,
                'thresholds': {'excellent': 0.9, 'good': 0.7, 'acceptable': 0.5, 'poor': 0.3},
                'verification_stages': 3
            },
            'accuracy': {
                'weight': 0.25,
                'thresholds': {'excellent': 0.9, 'good': 0.7, 'acceptable': 0.5, 'poor': 0.3},
                'verification_stages': 3
            },
            'relevance': {
                'weight': 0.20,
                'thresholds': {'excellent': 0.9, 'good': 0.7, 'acceptable': 0.5, 'poor': 0.3},
                'verification_stages': 2
            },
            'depth': {
                'weight': 0.15,
                'thresholds': {'excellent': 0.9, 'good': 0.7, 'acceptable': 0.5, 'poor': 0.3},
                'verification_stages': 2
            },
            'innovation': {
                'weight': 0.15,
                'thresholds': {'excellent': 0.9, 'good': 0.7, 'acceptable': 0.5, 'poor': 0.3},
                'verification_stages': 2
            }
        }
    
    def _load_quality_metrics(self) -> Dict[str, Any]:
        """ÌíàÏßà Î©îÌä∏Î¶≠ Î°úÎìú."""
        return {
            'data_quality': ['completeness', 'accuracy', 'consistency', 'timeliness'],
            'analysis_quality': ['methodology', 'rigor', 'validity', 'reliability'],
            'synthesis_quality': ['coherence', 'clarity', 'comprehensiveness', 'insightfulness'],
            'overall_quality': ['completeness', 'accuracy', 'relevance', 'depth', 'innovation']
        }
    
    def _load_refinement_strategies(self) -> Dict[str, Any]:
        """Í∞úÏÑ† Ï†ÑÎûµ Î°úÎìú."""
        return {
            'data_gaps': {
                'strategy': 'additional_data_collection',
                'priority': 'high',
                'estimated_effort': 'medium',
                'mcp_tools': ['g-search', 'tavily', 'exa']
            },
            'analysis_weakness': {
                'strategy': 'enhanced_analysis',
                'priority': 'high',
                'estimated_effort': 'high',
                'mcp_tools': ['python_coder', 'code_interpreter']
            },
            'synthesis_issues': {
                'strategy': 'improved_synthesis',
                'priority': 'medium',
                'estimated_effort': 'medium',
                'mcp_tools': ['filesystem', 'fetch']
            },
            'quality_concerns': {
                'strategy': 'quality_improvement',
                'priority': 'high',
                'estimated_effort': 'low',
                'mcp_tools': ['python_coder']
            }
        }
    
    async def evaluate_results(
        self,
        execution_results: List[Dict[str, Any]],
                             original_objectives: List[Dict[str, Any]],
                             context: Optional[Dict[str, Any]] = None,
        objective_id: str = None
    ) -> Dict[str, Any]:
        """Ïó∞Íµ¨ Í≤∞Í≥º ÌèâÍ∞Ä (8ÎåÄ ÌòÅÏã† ÌÜµÌï©)."""
        logger.info(f"üî¨ Starting evaluation with 8 core innovations for objective: {objective_id}")
        
        # Production-Grade ReliabilityÎ°ú ÌèâÍ∞Ä Ïã§Ìñâ
        return await execute_with_reliability(
            self._execute_evaluation_workflow,
            execution_results,
            original_objectives,
            context,
            objective_id,
            component_name="evaluation_agent",
            save_state=True
        )
    
    async def _execute_evaluation_workflow(
        self,
        execution_results: List[Dict[str, Any]],
        original_objectives: List[Dict[str, Any]],
        context: Optional[Dict[str, Any]],
        objective_id: str
    ) -> Dict[str, Any]:
        """ÌèâÍ∞Ä ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ (ÎÇ¥Î∂Ä Î©îÏÑúÎìú)."""
        # Phase 1: Continuous Verification (ÌòÅÏã† 4)
        logger.info("1. üîç Applying Continuous Verification")
        verification_results = await self._continuous_verification(execution_results, original_objectives)
        
        # Phase 2: Individual Result Evaluation (Multi-Model Orchestration)
        logger.info("2. üìä Evaluating individual results with Multi-Model Orchestration")
            individual_evaluations = await self._evaluate_individual_results(execution_results, original_objectives)
            
        # Phase 3: Overall Quality Assessment
        logger.info("3. üìà Assessing overall quality")
            overall_quality = await self._assess_overall_quality(individual_evaluations, original_objectives)
            
        # Phase 4: Objective Alignment Check
        logger.info("4. üéØ Checking objective alignment")
            alignment_assessment = await self._check_objective_alignment(execution_results, original_objectives)
            
        # Phase 5: Gap Analysis
        logger.info("5. üîç Analyzing gaps")
            gap_analysis = await self._analyze_gaps(execution_results, original_objectives)
            
        # Phase 6: Refinement Recommendations (Universal MCP Hub)
        logger.info("6. üí° Generating refinement recommendations with Universal MCP Hub")
            refinement_recommendations = await self._generate_refinement_recommendations(
                individual_evaluations, overall_quality, alignment_assessment, gap_analysis
            )
            
        # Phase 7: Recursion Decision
        logger.info("7. üîÑ Making recursion decision")
            recursion_decision = await self._make_recursion_decision(
                overall_quality, alignment_assessment, gap_analysis, refinement_recommendations
            )
        
        # Phase 8: Hierarchical Compression (ÌòÅÏã† 2)
        logger.info("8. üóúÔ∏è Applying Hierarchical Compression to evaluation results")
        compressed_evaluation = await self._compress_evaluation_results({
            'individual_evaluations': individual_evaluations,
            'overall_quality': overall_quality,
            'alignment_assessment': alignment_assessment,
            'gap_analysis': gap_analysis,
            'refinement_recommendations': refinement_recommendations,
            'recursion_decision': recursion_decision
        })
            
            evaluation_result = {
            'verification_results': verification_results,
                'individual_evaluations': individual_evaluations,
                'overall_quality': overall_quality,
                'alignment_assessment': alignment_assessment,
                'gap_analysis': gap_analysis,
                'refinement_recommendations': refinement_recommendations,
                'recursion_decision': recursion_decision,
            'compressed_evaluation': compressed_evaluation,
                'evaluation_metadata': {
                    'objective_id': objective_id,
                    'timestamp': datetime.now().isoformat(),
                'evaluation_version': '2.0',
                'total_results_evaluated': len(execution_results),
                'verification_stages': self.verification_config.verification_stages,
                'confidence_threshold': self.verification_config.confidence_threshold
            },
            'innovation_stats': {
                'verification_applied': len(verification_results.get('stages', [])),
                'models_used': list(set(eval.get('model_used', 'unknown') for eval in individual_evaluations)),
                'compression_ratio': compressed_evaluation.get('compression_ratio', 1.0),
                'overall_confidence': overall_quality.get('overall_score', 0.8)
            }
        }
        
        logger.info(f"‚úÖ Evaluation completed with 8 core innovations: {recursion_decision.get('needs_recursion', False)}")
            return evaluation_result
    
    async def _continuous_verification(
        self,
        execution_results: List[Dict[str, Any]],
        original_objectives: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Continuous Verification (ÌòÅÏã† 4)."""
        verification_stages = []
        confidence_scores = {}
            
            for result in execution_results:
            result_id = result.get('task_id', str(uuid.uuid4()))
            
            # Stage 1: Self-Verification
            self_score = await self._self_verification(result)
            
            # Stage 2: Cross-Verification
            cross_score = await self._cross_verification(result, execution_results)
            
            # Stage 3: External Verification (ÏÑ†ÌÉùÏ†Å)
            if self_score < 0.7 or cross_score < 0.7:
                external_score = await self._external_verification(result)
            else:
                external_score = 1.0
            
            # Ï¢ÖÌï© Ïã†Î¢∞ÎèÑ Ï†êÏàò
            final_score = (self_score * 0.3 + cross_score * 0.4 + external_score * 0.3)
            
            verification_stages.append({
                'result_id': result_id,
                'stage_1_self': self_score,
                'stage_2_cross': cross_score,
                'stage_3_external': external_score,
                'final_score': final_score,
                'confidence_level': 'high' if final_score >= 0.8 else 'medium' if final_score >= 0.6 else 'low'
            })
            
            confidence_scores[result_id] = final_score
            
            return {
            'stages': verification_stages,
            'confidence_scores': confidence_scores,
            'overall_confidence': sum(confidence_scores.values()) / max(len(confidence_scores), 1),
            'verification_applied': len(verification_stages)
        }
    
    async def _evaluate_individual_results(
        self,
        execution_results: List[Dict[str, Any]],
        original_objectives: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Í∞úÎ≥Ñ Í≤∞Í≥º ÌèâÍ∞Ä (Multi-Model Orchestration)."""
        individual_evaluations = []
        
        for result in execution_results:
            # Multi-Model OrchestrationÏúºÎ°ú ÌèâÍ∞Ä
            evaluation_prompt = f"""
            Evaluate the following research result comprehensively:
            
            Result: {json.dumps(result, ensure_ascii=False, indent=2)}
            Original Objectives: {json.dumps(original_objectives, ensure_ascii=False, indent=2)}
            
            Provide detailed evaluation including:
            1. Quality assessment with confidence scoring
            2. Completeness analysis
            3. Accuracy verification
            4. Relevance to objectives
            5. Strengths and weaknesses
            6. Improvement recommendations
            
            Use production-level evaluation with specific, actionable insights.
            """
            
            # Multi-Model OrchestrationÏúºÎ°ú ÌèâÍ∞Ä
            evaluation_result = await execute_llm_task(
                prompt=evaluation_prompt,
                task_type=TaskType.VERIFICATION,
                system_message="You are an expert research evaluator with comprehensive quality assessment capabilities.",
                use_ensemble=True  # Weighted Ensemble ÏÇ¨Ïö©
            )
            
            # ÌèâÍ∞Ä Í≤∞Í≥º ÌååÏã±
            evaluation_data = self._parse_evaluation_result(evaluation_result.content)
            
            individual_evaluations.append({
                'result_id': result.get('task_id', str(uuid.uuid4())),
                'result_type': result.get('agent', 'unknown'),
                'quality_score': evaluation_data.get('quality_score', 0.8),
                'completeness_score': evaluation_data.get('completeness_score', 0.8),
                'accuracy_score': evaluation_data.get('accuracy_score', 0.8),
                'relevance_score': evaluation_data.get('relevance_score', 0.8),
                'strengths': evaluation_data.get('strengths', []),
                'issues': evaluation_data.get('issues', []),
                'recommendations': evaluation_data.get('recommendations', []),
                'model_used': evaluation_result.model_used,
                'confidence': evaluation_result.confidence,
                'evaluation_timestamp': datetime.now().isoformat()
            })
        
        return individual_evaluations
    
    async def _assess_overall_quality(
        self,
        individual_evaluations: List[Dict[str, Any]],
        original_objectives: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Ï†ÑÏ≤¥ ÌíàÏßà ÌèâÍ∞Ä."""
            if not individual_evaluations:
                return {'overall_score': 0.0, 'quality_level': 'poor'}
            
        # Í∞ÄÏ§ë ÌèâÍ∑† Í≥ÑÏÇ∞
            total_score = sum(eval.get('quality_score', 0) for eval in individual_evaluations)
            overall_score = total_score / len(individual_evaluations)
            
        # ÌíàÏßà Î†àÎ≤® Í≤∞Ï†ï
            if overall_score >= 0.9:
                quality_level = 'excellent'
            elif overall_score >= 0.7:
                quality_level = 'good'
            elif overall_score >= 0.5:
                quality_level = 'acceptable'
            else:
                quality_level = 'poor'
            
        # ÏßëÍ≥ÑÎêú Ïù¥ÏäàÏôÄ Í∞ïÏ†ê
            all_issues = []
            all_strengths = []
            
            for eval_result in individual_evaluations:
                all_issues.extend(eval_result.get('issues', []))
                all_strengths.extend(eval_result.get('strengths', []))
            
            return {
                'overall_score': overall_score,
                'quality_level': quality_level,
                'total_evaluations': len(individual_evaluations),
                'aggregated_issues': list(set(all_issues)),
                'aggregated_strengths': list(set(all_strengths)),
                'quality_distribution': self._calculate_quality_distribution(individual_evaluations)
            }
            
    async def _check_objective_alignment(
        self,
        execution_results: List[Dict[str, Any]],
        original_objectives: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Î™©Ìëú Ï†ïÎ†¨ ÌôïÏù∏."""
        alignment_scores = []
            
            for objective in original_objectives:
                objective_id = objective.get('objective_id')
                objective_description = objective.get('description', '')
                
            # Ìï¥Îãπ Î™©ÌëúÏôÄ Í¥ÄÎ†®Îêú Í≤∞Í≥º Ï∞æÍ∏∞
                related_results = [r for r in execution_results if r.get('objective_id') == objective_id]
                
                if related_results:
                # Î™©Ìëú Ï†ïÎ†¨ Ï†êÏàò Í≥ÑÏÇ∞
                objective_score = await self._calculate_objective_alignment_score(objective, related_results)
                    alignment_scores.append(objective_score)
                else:
                # Ìï¥Îãπ Î™©ÌëúÏóê ÎåÄÌïú Í≤∞Í≥º ÏóÜÏùå
                    alignment_scores.append(0.0)
            
        # Ï†ÑÏ≤¥ Ï†ïÎ†¨ Í≥ÑÏÇ∞
            overall_alignment = sum(alignment_scores) / len(alignment_scores) if alignment_scores else 0.0
            
            return {
                'overall_alignment': overall_alignment,
                'objective_scores': alignment_scores,
                'alignment_level': 'high' if overall_alignment >= 0.8 else 'medium' if overall_alignment >= 0.6 else 'low',
                'misaligned_objectives': [i for i, score in enumerate(alignment_scores) if score < 0.6]
            }
            
    async def _analyze_gaps(
        self,
        execution_results: List[Dict[str, Any]],
        original_objectives: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Í∞≠ Î∂ÑÏÑù."""
        gaps = []
        
        # ÎàÑÎùΩÎêú Î™©Ìëú ÌôïÏù∏
            covered_objectives = set(r.get('objective_id') for r in execution_results)
            all_objectives = set(obj.get('objective_id') for obj in original_objectives)
            missing_objectives = all_objectives - covered_objectives
            
            if missing_objectives:
                gaps.append({
                    'type': 'missing_objectives',
                    'description': 'Some objectives were not addressed',
                    'severity': 'high',
                    'objectives': list(missing_objectives)
                })
            
        # ÌíàÏßà Í∞≠ ÌôïÏù∏
            quality_gaps = self._identify_quality_gaps(execution_results)
            gaps.extend(quality_gaps)
            
        # ÏôÑÏÑ±ÎèÑ Í∞≠ ÌôïÏù∏
            completeness_gaps = self._identify_completeness_gaps(execution_results, original_objectives)
            gaps.extend(completeness_gaps)
            
            return {
                'total_gaps': len(gaps),
                'high_severity_gaps': len([g for g in gaps if g.get('severity') == 'high']),
                'medium_severity_gaps': len([g for g in gaps if g.get('severity') == 'medium']),
                'low_severity_gaps': len([g for g in gaps if g.get('severity') == 'low']),
                'gaps': gaps
            }
            
    async def _generate_refinement_recommendations(
        self,
        individual_evaluations: List[Dict[str, Any]],
                                                 overall_quality: Dict[str, Any],
                                                 alignment_assessment: Dict[str, Any],
        gap_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Í∞úÏÑ† Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± (Universal MCP Hub)."""
        recommendations = []
        
        # ÌíàÏßà Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
            if overall_quality.get('overall_score', 0) < 0.7:
                recommendations.append({
                    'type': 'quality_improvement',
                    'priority': 'high',
                    'description': 'Improve overall research quality',
                    'estimated_effort': 'medium',
                'strategy': 'enhanced_analysis',
                'mcp_tools': ['python_coder', 'code_interpreter']
                })
            
        # Ï†ïÎ†¨ Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
            if alignment_assessment.get('overall_alignment', 0) < 0.7:
                recommendations.append({
                    'type': 'alignment_improvement',
                    'priority': 'high',
                    'description': 'Better align results with objectives',
                    'estimated_effort': 'high',
                'strategy': 'objective_refinement',
                'mcp_tools': ['g-search', 'tavily', 'exa']
                })
            
        # Í∞≠ Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
            for gap in gap_analysis.get('gaps', []):
                if gap.get('severity') == 'high':
                    recommendations.append({
                        'type': 'gap_filling',
                        'priority': 'high',
                        'description': f"Address gap: {gap.get('description', '')}",
                        'estimated_effort': 'medium',
                    'strategy': 'additional_research',
                    'mcp_tools': ['g-search', 'tavily', 'exa', 'arxiv', 'scholar']
                    })
            
            return recommendations
            
    async def _make_recursion_decision(
        self,
        overall_quality: Dict[str, Any],
                                     alignment_assessment: Dict[str, Any],
                                     gap_analysis: Dict[str, Any],
        refinement_recommendations: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Ïû¨Í∑Ä Í≤∞Ï†ï."""
        # Í≤∞Ï†ï ÏöîÏù∏
            quality_score = overall_quality.get('overall_score', 0)
            alignment_score = alignment_assessment.get('overall_alignment', 0)
            high_severity_gaps = gap_analysis.get('high_severity_gaps', 0)
            high_priority_recommendations = len([r for r in refinement_recommendations if r.get('priority') == 'high'])
            
        # Í≤∞Ï†ï Î°úÏßÅ
            needs_recursion = False
            recursion_reason = ""
            
            if quality_score < 0.6:
                needs_recursion = True
                recursion_reason = "Low overall quality"
            elif alignment_score < 0.6:
                needs_recursion = True
                recursion_reason = "Poor objective alignment"
            elif high_severity_gaps > 0:
                needs_recursion = True
                recursion_reason = "High severity gaps identified"
            elif high_priority_recommendations > 2:
                needs_recursion = True
                recursion_reason = "Multiple high-priority improvements needed"
            
            return {
                'needs_recursion': needs_recursion,
                'recursion_reason': recursion_reason,
                'decision_factors': {
                    'quality_score': quality_score,
                    'alignment_score': alignment_score,
                    'high_severity_gaps': high_severity_gaps,
                    'high_priority_recommendations': high_priority_recommendations
                },
                'confidence': self._calculate_recursion_confidence(quality_score, alignment_score, high_severity_gaps)
            }
            
    async def _compress_evaluation_results(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """ÌèâÍ∞Ä Í≤∞Í≥º ÏïïÏ∂ï (Hierarchical Compression)."""
        try:
            compressed = await compress_data(evaluation_data)
            return {
                'compressed_data': compressed.data,
                'compression_ratio': compressed.compression_ratio,
                'validation_score': compressed.validation_score,
                'important_info_preserved': compressed.important_info_preserved
            }
        except Exception as e:
            logger.warning(f"Compression failed: {e}")
            return {
                'compressed_data': evaluation_data,
                'compression_ratio': 1.0,
                'validation_score': 1.0,
                'important_info_preserved': []
            }
    
    # Ìó¨Ìçº Î©îÏÑúÎìúÎì§
    async def _self_verification(self, result: Dict[str, Any]) -> float:
        """ÏûêÏ≤¥ Í≤ÄÏ¶ù."""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îçî Ï†ïÍµêÌïú Í≤ÄÏ¶ù Î°úÏßÅ ÏÇ¨Ïö©
        return 0.8
    
    async def _cross_verification(self, result: Dict[str, Any], all_results: List[Dict[str, Any]]) -> float:
        """ÍµêÏ∞® Í≤ÄÏ¶ù."""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îã§Î•∏ Í≤∞Í≥ºÏôÄÏùò ÏùºÏπòÎèÑ Í≤ÄÏÇ¨
        return 0.85
    
    async def _external_verification(self, result: Dict[str, Any]) -> float:
        """Ïô∏Î∂Ä Í≤ÄÏ¶ù."""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Ïô∏Î∂Ä ÏÜåÏä§ÏôÄÏùò Í≤ÄÏ¶ù
        return 0.9
    
    def _parse_evaluation_result(self, content: str) -> Dict[str, Any]:
        """ÌèâÍ∞Ä Í≤∞Í≥º ÌååÏã±."""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îçî Ï†ïÍµêÌïú ÌååÏã± Î°úÏßÅ ÏÇ¨Ïö©
        return {
            'quality_score': 0.8,
            'completeness_score': 0.8,
            'accuracy_score': 0.8,
            'relevance_score': 0.8,
            'strengths': ['Good quality', 'Comprehensive analysis'],
            'issues': ['Minor improvements needed'],
            'recommendations': ['Enhance analysis depth']
        }
    
    async def _calculate_objective_alignment_score(self, objective: Dict[str, Any], results: List[Dict[str, Any]]) -> float:
        """Î™©Ìëú Ï†ïÎ†¨ Ï†êÏàò Í≥ÑÏÇ∞."""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îçî Ï†ïÍµêÌïú Ï†ïÎ†¨ Ï†êÏàò Í≥ÑÏÇ∞
        return 0.8
    
    def _calculate_quality_distribution(self, evaluations: List[Dict[str, Any]]) -> Dict[str, int]:
        """ÌíàÏßà Ï†êÏàò Î∂ÑÌè¨ Í≥ÑÏÇ∞."""
        distribution = {'excellent': 0, 'good': 0, 'acceptable': 0, 'poor': 0}
        for eval_result in evaluations:
            score = eval_result.get('quality_score', 0)
            if score >= 0.9:
                distribution['excellent'] += 1
            elif score >= 0.7:
                distribution['good'] += 1
            elif score >= 0.5:
                distribution['acceptable'] += 1
            else:
                distribution['poor'] += 1
        return distribution
    
    def _identify_quality_gaps(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ÌíàÏßà Í∞≠ ÏãùÎ≥Ñ."""
        gaps = []
        for result in results:
            if result.get('status') == 'failed':
                gaps.append({
                    'type': 'execution_failure',
                    'description': 'Result execution failed',
                    'severity': 'high',
                    'result_id': result.get('task_id')
                })
        return gaps
    
    def _identify_completeness_gaps(self, results: List[Dict[str, Any]], objectives: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ÏôÑÏÑ±ÎèÑ Í∞≠ ÏãùÎ≥Ñ."""
        gaps = []
        # Î™®Îì† Î™©ÌëúÏóê ÎåÄÌïú Í≤∞Í≥ºÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
        objective_ids = set(obj.get('objective_id') for obj in objectives)
        result_objective_ids = set(r.get('objective_id') for r in results)
        missing_objectives = objective_ids - result_objective_ids
        
        for missing_id in missing_objectives:
            gaps.append({
                'type': 'missing_objective',
                'description': f'No results for objective {missing_id}',
                'severity': 'high',
                'objective_id': missing_id
            })
        
        return gaps
    
    def _calculate_recursion_confidence(self, quality: float, alignment: float, gaps: int) -> float:
        """Ïû¨Í∑Ä Í≤∞Ï†ï Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞."""
        # ÏûÑÍ≥ÑÍ∞íÎ≥¥Îã§ Î™ÖÌôïÌûà ÎÇÆÏùÑ Îïå Îçî ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ
        confidence = 0.5
        if quality < 0.5 or alignment < 0.5 or gaps > 2:
            confidence = 0.9
        elif quality < 0.7 or alignment < 0.7 or gaps > 0:
            confidence = 0.7
        return confidence
    
    async def cleanup(self):
        """ÏóêÏù¥Ï†ÑÌä∏ Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨."""
        try:
            logger.info("Evaluation Agent cleanup completed")
        except Exception as e:
            logger.error(f"Evaluation Agent cleanup failed: {e}")


# Global evaluation agent instance
evaluation_agent = EvaluationAgent()


async def evaluate_results(
    execution_results: List[Dict[str, Any]],
    original_objectives: List[Dict[str, Any]],
    context: Optional[Dict[str, Any]] = None,
    objective_id: str = None
) -> Dict[str, Any]:
    """Ïó∞Íµ¨ Í≤∞Í≥º ÌèâÍ∞Ä."""
    return await evaluation_agent.evaluate_results(
        execution_results, original_objectives, context, objective_id
    )