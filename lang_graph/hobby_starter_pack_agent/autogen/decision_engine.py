import json
from typing import Dict, Any, List
from datetime import datetime
import asyncio
import hashlib
import logging
from .agents import HSPAutoGenAgents

# Import LLM provider configuration
from core.llm import get_llm_config, get_llm_manager, LLMClientFactory

# Logger ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AutoGenDecisionEngine:
    """ìµœì í™”ëœ AutoGen ì˜ì‚¬ê²°ì • ì—”ì§„"""
    
    def __init__(self):
        self.agents = HSPAutoGenAgents()
        self.llm_cache = {}  # LLM ì‘ë‹µ ìºì‹œ
        self._llm_client = None
        self._llm_config = None
        self._initialize_llm()
        logger.info("AutoGenDecisionEngine ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_llm(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” - OpenRouter, Groq, Cerebras, OpenAI ì§€ì› (ëœë¤ ë¬´ë£Œ ê¸°ë³¸)"""
        try:
            # DEFAULT: Use random free provider (OpenRouter, Groq, Cerebras)
            from core.llm import get_random_free_config
            config = get_random_free_config()
            
            if config:
                self._llm_client = LLMClientFactory.create_async_client(config)
                self._llm_config = config
                logger.info(f"ğŸ² LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (random free): {config.provider.value} - {config.model}")
            else:
                # Fallback to primary provider
                from core.llm import get_llm_config
                config = get_llm_config()
                if config:
                    self._llm_client = LLMClientFactory.create_async_client(config)
                    self._llm_config = config
                    logger.info(f"LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (fallback): {config.provider.value} - {config.model}")
                else:
                    # Fallback to legacy OpenAI
                    import openai
                    import os
                    api_key = os.getenv("OPENAI_API_KEY")
                    if api_key:
                        self._llm_client = openai.AsyncOpenAI(api_key=api_key)
                        self._llm_config = None
                        logger.warning("Using legacy OpenAI client - no free provider configured")
                    else:
                        logger.warning("No LLM provider configured")
        except Exception as e:
            logger.error(f"LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def _call_llm_with_cache(self, prompt: str, agent_type: str = "general") -> str:
        """ìºì‹œê°€ ì ìš©ëœ LLM í˜¸ì¶œ - ë‹¤ì¤‘ Provider ì§€ì›"""
        cache_key = hashlib.md5(f"{prompt}:{agent_type}".encode()).hexdigest()
        
        if cache_key in self.llm_cache:
            return self.llm_cache[cache_key]
        
        try:
            if self._llm_client is None:
                raise ValueError("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # Determine model and parameters
            if self._llm_config:
                model = self._llm_config.model
                temperature = self._llm_config.temperature
                max_tokens = self._llm_config.max_tokens
            else:
                # Legacy fallback
                import os
                model = os.getenv("LLM_MODEL", "gpt-4o")
                temperature = 0.7
                max_tokens = 500
            
            # OpenAI-compatible API call (works with OpenRouter, Groq, Cerebras)
            response = await self._llm_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": f"{agent_type} ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            result = response.choices[0].message.content
            
            # ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤)
            if len(self.llm_cache) < 100:
                self.llm_cache[cache_key] = result
                
            return result
            
        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise ValueError(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    
    async def analyze_user_profile(self, user_input: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”„ë¡œí•„ ë¶„ì„ - ë³‘ë ¬ ì²˜ë¦¬"""
        try:
            # ë³‘ë ¬ë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë¶„ì„
            analysis_tasks = [
                self._call_llm_with_cache(
                    f"ë‹¤ìŒ ì…ë ¥ì—ì„œ ê´€ì‹¬ì‚¬ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {user_input}", 
                    "profile_analyst"
                ),
                self._call_llm_with_cache(
                    f"ë‹¤ìŒ ì‚¬ìš©ìì˜ ê²½í—˜ ìˆ˜ì¤€ì„ í‰ê°€í•´ì£¼ì„¸ìš”: {user_input}", 
                    "skill_assessor"
                ),
                self._call_llm_with_cache(
                    f"ì‹œê°„ ê°€ìš©ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {user_input}", 
                    "schedule_analyst"
                )
            ]
            
            results = await asyncio.gather(*analysis_tasks)
            
            # ê²°ê³¼ êµ¬ì¡°í™”
            return {
                "interests": self._extract_interests(results[0]),
                "skill_level": self._extract_skill_level(results[1]),
                "time_availability": self._extract_time_availability(results[2]),
                "raw_analysis": results
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œí•„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise ValueError(f"í”„ë¡œí•„ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def filter_hobbies(self, hobby_list: List[Dict], user_profile: Dict) -> List[Dict]:
        """ì·¨ë¯¸ í•„í„°ë§ ë° ê°œì¸í™”"""
        try:
            # ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ í•„í„°ë§ í”„ë¡¬í”„íŠ¸
            filter_prompt = f"""
            ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}
            ì·¨ë¯¸ ëª©ë¡: {hobby_list}
            
            ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ì·¨ë¯¸ 5ê°œë¥¼ ì„ ë³„í•˜ê³  ê°ê°ì˜ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
            """
            
            filtered_result = await self._call_llm_with_cache(filter_prompt, "hobby_discoverer")
            
            # ê²°ê³¼ íŒŒì‹± ë° êµ¬ì¡°í™”
            return self._parse_hobby_recommendations(filtered_result, hobby_list)
            
        except Exception as e:
            logger.error(f"ì·¨ë¯¸ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì·¨ë¯¸ í•„í„°ë§ ì‹¤íŒ¨: {e}")
    
    def _extract_interests(self, analysis_text: str) -> List[str]:
        """ê´€ì‹¬ì‚¬ ì¶”ì¶œ ë¡œì§"""
        # í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ê´€ì‹¬ì‚¬ ì¶”ì¶œ
        interest_keywords = {
            "music": ["ìŒì•…", "ì•…ê¸°", "ë…¸ë˜", "ë°´ë“œ"],
            "sports": ["ìš´ë™", "ìŠ¤í¬ì¸ ", "í—¬ìŠ¤", "í”¼íŠ¸ë‹ˆìŠ¤"],
            "art": ["ê·¸ë¦¼", "ë¯¸ìˆ ", "ì°½ì‘", "ë””ìì¸"],
            "technology": ["ì½”ë”©", "í”„ë¡œê·¸ë˜ë°", "IT", "ê°œë°œ"],
            "cooking": ["ìš”ë¦¬", "ë² ì´í‚¹", "ìŒì‹", "ë ˆì‹œí”¼"]
        }
        
        detected_interests = []
        for category, keywords in interest_keywords.items():
            if any(keyword in analysis_text for keyword in keywords):
                detected_interests.append(category)
        
        return detected_interests if detected_interests else ["general"]
    
    def _extract_skill_level(self, analysis_text: str) -> str:
        """ê²½í—˜ ìˆ˜ì¤€ ì¶”ì¶œ"""
        if any(word in analysis_text for word in ["ì´ˆë³´", "ì²˜ìŒ", "ì‹œì‘"]):
            return "beginner"
        elif any(word in analysis_text for word in ["ì¤‘ê¸‰", "ì–´ëŠì •ë„", "ê²½í—˜"]):
            return "intermediate"
        elif any(word in analysis_text for word in ["ê³ ê¸‰", "ì „ë¬¸", "ìˆ™ë ¨"]):
            return "advanced"
        return "beginner"
    
    def _extract_time_availability(self, analysis_text: str) -> str:
        """ì‹œê°„ ê°€ìš©ì„± ì¶”ì¶œ"""
        if any(word in analysis_text for word in ["ì£¼ë§", "í† ìš”ì¼", "ì¼ìš”ì¼"]):
            return "weekend"
        elif any(word in analysis_text for word in ["ì €ë…", "í‡´ê·¼", "ë°¤"]):
            return "evening"
        elif any(word in analysis_text for word in ["í‰ì¼", "ì˜¤ì „", "ì ì‹¬"]):
            return "weekday"
        return "flexible"
    
    def _parse_hobby_recommendations(self, llm_result: str, original_list: List[Dict]) -> List[Dict]:
        """LLM ê²°ê³¼ì—ì„œ ì·¨ë¯¸ ì¶”ì²œ íŒŒì‹±"""
        try:
            # LLM ì‘ë‹µì—ì„œ ì¶”ì²œëœ ì·¨ë¯¸ëª… ì¶”ì¶œ
            recommended_names = []
            lines = llm_result.split('\n')
            
            for line in lines:
                # "1. ë…ì„œ", "- ë“±ì‚°" ê°™ì€ íŒ¨í„´ì—ì„œ ì·¨ë¯¸ëª… ì¶”ì¶œ
                if any(hobby['name'] in line for hobby in original_list):
                    for hobby in original_list:
                        if hobby['name'] in line and hobby not in recommended_names:
                            recommended_names.append(hobby)
                            break
            
            # ì¶”ì²œëœ ì·¨ë¯¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒìœ„ 3ê°œ
            if not recommended_names and original_list:
                recommended_names = original_list[:3]
            
            return recommended_names
            
        except Exception as e:
            logger.error(f"ì·¨ë¯¸ ì¶”ì²œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return original_list[:3] if original_list else []
    
    async def generate_final_recommendations(self, user_profile: Dict, hobbies: List[Dict], 
                                           communities: List[Dict]) -> List[Dict]:
        """ìµœì¢… ì¶”ì²œ ìƒì„±"""
        try:
            final_prompt = f"""
            ì‚¬ìš©ì í”„ë¡œí•„, ì·¨ë¯¸ í›„ë³´, ì»¤ë®¤ë‹ˆí‹° ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì¶”ì²œì„ ìƒì„±í•´ì£¼ì„¸ìš”:
            
            ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}
            ì·¨ë¯¸ í›„ë³´: {hobbies}  
            ì»¤ë®¤ë‹ˆí‹°: {communities}
            
            ìµœì¢… ì¶”ì²œ 3ê°œë¥¼ ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”.
            """
            
            final_result = await self._call_llm_with_cache(final_prompt, "final_coordinator")
            
            return {
                "final_recommendations": final_result,
                "recommended_hobbies": hobbies[:3],
                "matched_communities": communities[:2]
            }
            
        except Exception as e:
            logger.error(f"ìµœì¢… ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}")
            raise ValueError(f"ìµœì¢… ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}")

class AgentDecisionEngine:
    """ëª¨ë“  íŒë‹¨ì„ ì—ì´ì „íŠ¸ê°€ LLM í˜¸ì¶œë¡œ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„"""
    
    def __init__(self, llm_config: Dict[str, Any]):
        self.llm_config = llm_config
        self.decision_history = []
    
    async def make_hobby_recommendation_decision(self, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """ì·¨ë¯¸ ì¶”ì²œ ì˜ì‚¬ê²°ì • - í•˜ë“œì½”ë”© ì—†ì´ ìˆœìˆ˜ LLM íŒë‹¨"""
        
        prompt = f"""
        ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì·¨ë¯¸ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
        
        ì‚¬ìš©ì ì •ë³´:
        {json.dumps(user_context, ensure_ascii=False, indent=2)}
        
        ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ê³ ë ¤í•˜ì—¬ ì¶”ì²œí•´ì£¼ì„¸ìš”:
        1. ì‚¬ìš©ìì˜ ì„±ê²©ê³¼ ì„±í–¥
        2. í˜„ì¬ ìƒí™œ íŒ¨í„´ê³¼ ìŠ¤ì¼€ì¤„
        3. ê´€ì‹¬ì‚¬ì™€ ì„ í˜¸ë„
        4. ì˜ˆì‚°ê³¼ ì‹œê°„ ì œì•½
        5. ì§€ì—­ì  ì ‘ê·¼ì„±
        
        ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•íƒœë¡œ í•´ì£¼ì„¸ìš”:
        {{
            "recommendations": [
                {{
                    "hobby_name": "ì·¨ë¯¸ ì´ë¦„",
                    "reason": "ì¶”ì²œ ì´ìœ ",
                    "difficulty": "ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰",
                    "time_commitment": "ì‹œê°„ íˆ¬ì ì •ë„",
                    "budget_range": "ì˜ˆì‚° ë²”ìœ„",
                    "confidence_score": 0.85
                }}
            ],
            "reasoning": "ì „ì²´ì ì¸ ì¶”ì²œ ê·¼ê±°"
        }}
        
        ë§Œì•½ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        try:
            # LLM í˜¸ì¶œí•˜ì—¬ ë™ì  ê²°ì •
            response = await self._call_llm(prompt)
            result = json.loads(response) if response else {"recommendations": [], "reasoning": ""}
            
            # ì˜ì‚¬ê²°ì • ì´ë ¥ ì €ì¥
            self.decision_history.append({
                "decision_type": "hobby_recommendation",
                "timestamp": datetime.now().isoformat(),
                "input_context": user_context,
                "result": result
            })
            
            return result
            
        except Exception as e:
            # ì˜ˆì™¸ ì‹œ ë¹ˆ ê°’ ë°˜í™˜
            return {"recommendations": [], "reasoning": ""}
    
    async def analyze_schedule_compatibility(self, schedule_data: Dict[str, Any], hobby_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """ìŠ¤ì¼€ì¤„ í˜¸í™˜ì„± ë¶„ì„ - LLM ê¸°ë°˜ ë™ì  íŒë‹¨"""
        
        prompt = f"""
        ì‚¬ìš©ìì˜ ìŠ¤ì¼€ì¤„ê³¼ ì·¨ë¯¸ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ í˜¸í™˜ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        í˜„ì¬ ìŠ¤ì¼€ì¤„:
        {json.dumps(schedule_data, ensure_ascii=False, indent=2)}
        
        ì·¨ë¯¸ ìš”êµ¬ì‚¬í•­:
        {json.dumps(hobby_requirements, ensure_ascii=False, indent=2)}
        
        ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        1. ì‹œê°„ì  í˜¸í™˜ì„±
        2. ì—ë„ˆì§€ ë ˆë²¨ ë§¤ì¹­
        3. ì¼ì • ì¶©ëŒ ê°€ëŠ¥ì„±
        4. ìµœì  ì‹œê°„ëŒ€ ì œì•ˆ
        
        ì‘ë‹µì€ JSON í˜•íƒœë¡œ:
        {{
            "compatibility_score": 0.8,
            "available_time_slots": [
                {{
                    "day": "ì›”ìš”ì¼",
                    "time_range": "19:00-21:00",
                    "confidence": 0.9
                }}
            ],
            "potential_conflicts": ["ë¦¬ìŠ¤íŠ¸"],
            "optimization_suggestions": ["ì œì•ˆì‚¬í•­ë“¤"],
            "integration_strategy": "í†µí•© ì „ëµ"
        }}
        
        ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ë¹ˆ ê°ì²´ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = await self._call_llm(prompt)
            return json.loads(response) if response else {}
        except Exception:
            return {}
    
    async def evaluate_community_match(self, user_profile: Dict[str, Any], community_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì»¤ë®¤ë‹ˆí‹° ë§¤ì¹­ í‰ê°€ - LLM ê¸°ë°˜ ë™ì  íŒë‹¨"""
        
        prompt = f"""
        ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì»¤ë®¤ë‹ˆí‹° ì •ë³´ë¥¼ ë¹„êµí•˜ì—¬ ë§¤ì¹­ ì í•©ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”.
        
        ì‚¬ìš©ì í”„ë¡œí•„:
        {json.dumps(user_profile, ensure_ascii=False, indent=2)}
        
        ì»¤ë®¤ë‹ˆí‹° ì •ë³´:
        {json.dumps(community_data, ensure_ascii=False, indent=2)}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€:
        1. ê´€ì‹¬ì‚¬ ì¼ì¹˜ë„
        2. í™œë™ ìŠ¤íƒ€ì¼ í˜¸í™˜ì„±
        3. ê²½í—˜ ë ˆë²¨ ì í•©ì„±
        4. ì§€ì—­ì  ì ‘ê·¼ì„±
        5. ì»¤ë®¤ë‹ˆí‹° í™œì„±ë„
        
        JSON ì‘ë‹µ:
        {{
            "match_score": 0.85,
            "strength_areas": ["ê°•ì  ì˜ì—­ë“¤"],
            "concern_areas": ["ìš°ë ¤ ì˜ì—­ë“¤"],
            "recommendation": "ì¶”ì²œ/ë³´ë¥˜/ë¹„ì¶”ì²œ",
            "integration_tips": ["ì°¸ì—¬ íŒë“¤"]
        }}
        
        í‰ê°€ê°€ ì–´ë ¤ìš°ë©´ ë¹ˆ ê°ì²´ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = await self._call_llm(prompt)
            return json.loads(response) if response else {}
        except Exception:
            return {}
    
    async def generate_weekly_insights(self, activity_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„± - LLM ê¸°ë°˜ ë™ì  ìƒì„±"""
        
        prompt = f"""
        ì‚¬ìš©ìì˜ ì£¼ê°„ í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        í™œë™ ë°ì´í„°:
        {json.dumps(activity_data, ensure_ascii=False, indent=2)}
        
        ë‹¤ìŒì„ í¬í•¨í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±:
        1. ì„±ì·¨ë„ ë¶„ì„
        2. íŒ¨í„´ ë°œê²¬
        3. ê°œì„  ì˜ì—­
        4. ë‹¤ìŒ ì£¼ ëª©í‘œ ì œì•ˆ
        5. ë™ê¸°ë¶€ì—¬ ë©”ì‹œì§€
        
        JSON ì‘ë‹µ:
        {{
            "achievement_score": 0.75,
            "key_insights": ["ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë“¤"],
            "progress_trends": ["ì§„í–‰ íŠ¸ë Œë“œ"],
            "improvement_areas": ["ê°œì„  ì˜ì—­"],
            "next_week_goals": ["ë‹¤ìŒ ì£¼ ëª©í‘œë“¤"],
            "motivational_message": "ê²©ë ¤ ë©”ì‹œì§€",
            "personalized_journal": "ê°œì¸í™”ëœ ì¼ì§€ ë‚´ìš©"
        }}
        
        ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¹ˆ ê°’ë“¤ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = await self._call_llm(prompt)
            return json.loads(response) if response else {
                "achievement_score": 0,
                "key_insights": [],
                "progress_trends": [],
                "improvement_areas": [],
                "next_week_goals": [],
                "motivational_message": "",
                "personalized_journal": ""
            }
        except Exception:
            return {
                "achievement_score": 0,
                "key_insights": [],
                "progress_trends": [],
                "improvement_areas": [],
                "next_week_goals": [],
                "motivational_message": "",
                "personalized_journal": ""
            }
    
    async def _call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ - ë‹¤ì¤‘ Provider ì§€ì› (OpenRouter, Groq, Cerebras, OpenAI, Anthropic, Google)"""
        try:
            config = get_llm_config()
            if config is None:
                logger.warning("No LLM provider configured")
                return ""
            
            client = LLMClientFactory.create_async_client(config)
            
            response = await client.chat.completions.create(
                model=config.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
            
            return response.choices[0].message.content or ""
        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "" 