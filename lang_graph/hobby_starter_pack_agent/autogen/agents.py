import asyncio
import json
import logging
from typing import Dict, Any, List, Optional
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
from datetime import datetime

# Import LLM provider configuration
from core.llm import get_llm_config, get_llm_manager, LLMProvider

# Logger ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class HSPAutoGenAgents:
    """AutoGen ê¸°ë°˜ ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤"""
    
    def __init__(self, llm_config: Optional[Dict[str, Any]] = None, use_random_free: bool = True):
        """
        Initialize AutoGen agents
        
        Args:
            llm_config: Optional dict config (legacy support)
            use_random_free: If True (default), uses random free provider (OpenRouter, Groq, Cerebras)
                            If False, uses primary provider from environment
        """
        # Support both legacy dict config and new LLM provider system
        if llm_config is None:
            if use_random_free:
                # DEFAULT: Use random free provider (OpenRouter, Groq, Cerebras)
                from core.llm import get_random_free_config
                config = get_random_free_config()
                if config:
                    self.llm_config = {
                        "model": config.model,
                        "temperature": config.temperature,
                        "base_url": config.base_url,
                        "api_key": config.api_key
                    }
                    logger.info(f"ğŸ² Using random free provider: {config.provider.value} - {config.model}")
                else:
                    # Fallback to primary provider
                    from core.llm import get_llm_config
                    config = get_llm_config()
                    if config:
                        self.llm_config = {
                            "model": config.model,
                            "temperature": config.temperature,
                            "base_url": config.base_url,
                            "api_key": config.api_key
                        }
                        logger.info(f"Using primary provider: {config.provider.value} - {config.model}")
                    else:
                        self.llm_config = {"model": "gemini-4", "temperature": 0.7}
                        logger.warning("No LLM provider configured, using default: gemini-4")
            else:
                # Use primary provider from environment
                from core.llm import get_llm_config
                config = get_llm_config()
                if config:
                    self.llm_config = {
                        "model": config.model,
                        "temperature": config.temperature,
                        "base_url": config.base_url,
                        "api_key": config.api_key
                    }
                    logger.info(f"Using primary provider: {config.provider.value} - {config.model}")
                else:
                    self.llm_config = {"model": "gemini-4", "temperature": 0.7}
                    logger.warning("No LLM provider configured, using default: gemini-4")
        else:
            # Legacy support for dict config
            self.llm_config = llm_config
        
        self.agents = self._initialize_agents()
        logger.info("HSPAutoGenAgents ì´ˆê¸°í™” ì™„ë£Œ")
        
    def _initialize_agents(self) -> Dict[str, AssistantAgent]:
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™” - ëª¨ë“  ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì—ì´ì „íŠ¸ê°€ LLMìœ¼ë¡œ ë™ì  ìƒì„±"""
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ë¶„ì„ ì „ë¬¸ê°€
        profile_analyst = AssistantAgent(
            name="ProfileAnalyst",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ì‚¬ìš©ìì˜ ì„±ê²©, ê´€ì‹¬ì‚¬, ë¼ì´í”„ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• í”„ë¡œí•„ì„ ìƒì„±"
        )
        
        # ì·¨ë¯¸ ë°œê²¬ ì „ë¬¸ê°€
        hobby_discoverer = AssistantAgent(
            name="HobbyDiscoverer",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ê°œì¸ì—ê²Œ ìµœì í™”ëœ ìƒˆë¡œìš´ ì·¨ë¯¸ í™œë™ì„ ë°œê²¬í•˜ê³  ì¶”ì²œ"
        )
        
        # ìŠ¤ì¼€ì¤„ í†µí•© ì „ë¬¸ê°€
        schedule_integrator = AssistantAgent(
            name="ScheduleIntegrator",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ì¼ìƒ ìŠ¤ì¼€ì¤„ê³¼ ì·¨ë¯¸ í™œë™ì„ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©"
        )
        
        # ì»¤ë®¤ë‹ˆí‹° ë§¤ì¹­ ì „ë¬¸ê°€
        community_matcher = AssistantAgent(
            name="CommunityMatcher",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ì·¨ë¯¸ ê¸°ë°˜ ì»¤ë®¤ë‹ˆí‹°ì™€ ë™ë£Œë¥¼ ì°¾ì•„ì„œ ì—°ê²°"
        )
        
        # ì§„í–‰ìƒí™© ì¶”ì  ì „ë¬¸ê°€
        progress_tracker = AssistantAgent(
            name="ProgressTracker",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ì·¨ë¯¸ í™œë™ ì§„í–‰ìƒí™©ì„ ì¶”ì í•˜ê³  ë™ê¸°ë¶€ì—¬ ì œê³µ"
        )
        
        # ì˜ì‚¬ê²°ì • ì¤‘ì¬ì
        decision_moderator = AssistantAgent(
            name="DecisionModerator",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ì—ì´ì „íŠ¸ ê°„ ì˜ê²¬ ì°¨ì´ë¥¼ ì¡°ìœ¨í•˜ê³  ìµœì¢… ì˜ì‚¬ê²°ì • ì§€ì›"
        )
        
        # ëŒ€í™” ì£¼ë„ ì „ë¬¸ ì—ì´ì „íŠ¸
        conversation_agent = AssistantAgent(
            name="ConversationAgent",
            system_message="",  # ë¹ˆ ê°’, LLMì´ ë™ì ìœ¼ë¡œ ê²°ì •
            llm_config=self.llm_config,
            description="ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ì£¼ë„í•˜ì—¬ ì·¨ë¯¸ ì„ í˜¸ë„ì™€ ìš”êµ¬ì‚¬í•­ì„ ìˆ˜ì§‘í•˜ëŠ” ì „ë¬¸ê°€"
        )
        
        return {
            "profile_analyst": profile_analyst,
            "hobby_discoverer": hobby_discoverer,
            "schedule_integrator": schedule_integrator,
            "community_matcher": community_matcher,
            "progress_tracker": progress_tracker,
            "decision_moderator": decision_moderator,
            "conversation_agent": conversation_agent
        }
    
    def create_consensus_chat(self, relevant_agents: List[str]) -> GroupChat:
        """íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ì—ì´ì „íŠ¸ í•©ì˜ ì±„íŒ… ìƒì„±"""
        selected_agents = [self.agents[name] for name in relevant_agents if name in self.agents]
        
        # UserProxyAgent ì¶”ê°€ (ì‹¤í–‰ ê¶Œí•œ)
        user_proxy = UserProxyAgent(
            name="UserProxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=0,
            code_execution_config=False
        )
        
        selected_agents.append(user_proxy)
        
        return GroupChat(
            agents=selected_agents,
            messages=[],
            max_round=10,  # ì—ì´ì „íŠ¸ê°€ ë™ì ìœ¼ë¡œ ì¡°ì •
            speaker_selection_method="auto"  # ìë™ ì„ íƒ
        )
    
    async def run_consensus(self, agents: List[str], topic: str, context: Dict[str, Any], 
                           user_profile: Optional[Dict[str, Any]] = None, 
                           bridge=None, session_id: str = None) -> Dict[str, Any]:
        """A2A ë¸Œë¦¬ì§€ë¥¼ í†µí•œ ì—ì´ì „íŠ¸ í•©ì˜ ì‹¤í–‰"""
        try:
            # A2A ë¸Œë¦¬ì§€ì— ì—ì´ì „íŠ¸ë“¤ ë“±ë¡
            if bridge:
                for agent_name in agents:
                    await bridge.register_agent(
                        agent_id=agent_name,
                        agent_type=agent_name.lower(),
                        framework="autogen"
                    )
            
            # ê·¸ë£¹ ì±„íŒ… ìƒì„±
            group_chat = self.create_consensus_chat(agents)
            
            # ê·¸ë£¹ ì±„íŒ… ë§¤ë‹ˆì € ìƒì„±
            manager = GroupChatManager(
                groupchat=group_chat,
                llm_config=self.llm_config
            )
            
            # ì´ˆê¸° ë©”ì‹œì§€ êµ¬ì„±
            initial_message = self._construct_initial_message(topic, context, user_profile)
            
            # A2A ë©”ì‹œì§€ë¡œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì—ê²Œ ì•Œë¦¼
            if bridge and session_id:
                from ..bridge.a2a_bridge import A2AMessage
                
                for agent_name in agents:
                    message = A2AMessage(
                        sender_agent="AutoGenConsensus",
                        receiver_agent=agent_name,
                        message_type="consensus_start",
                        payload={
                            "topic": topic,
                            "context": context,
                            "user_profile": user_profile
                        },
                        timestamp=datetime.now().isoformat(),
                        session_id=session_id
                    )
                    await bridge.send_message(message)
            
            # ê·¸ë£¹ ì±„íŒ… ì‹œì‘ (ë¹„ë™ê¸° ì‹¤í–‰)
            user_proxy = next(agent for agent in group_chat.agents if agent.name == "UserProxy")
            
            # ì±„íŒ… ì‹¤í–‰ì„ ìœ„í•œ ë¹„ë™ê¸° wrapper
            consensus_result = await self._run_async_chat(user_proxy, manager, initial_message)
            
            # ê²°ê³¼ë¥¼ A2A ë¸Œë¦¬ì§€ë¥¼ í†µí•´ LangGraphë¡œ ì „ì†¡
            if bridge and session_id:
                result_message = A2AMessage(
                    sender_agent="AutoGenConsensus",
                    receiver_agent="LangGraphWorkflow",
                    message_type="consensus_result",
                    payload={
                        "consensus": consensus_result,
                        "participants": agents,
                        "topic": topic
                    },
                    timestamp=datetime.now().isoformat(),
                    session_id=session_id
                )
                await bridge.send_message(result_message)
            
            return consensus_result
            
        except Exception as e:
            return {"error": "Consensus execution failed", "details": str(e)}
    
    def _construct_initial_message(self, topic: str, context: Dict[str, Any], 
                                  user_profile: Optional[Dict[str, Any]]) -> str:
        """ì´ˆê¸° í•©ì˜ ë©”ì‹œì§€ êµ¬ì„±"""
        message_parts = [
            f"ì£¼ì œ: {topic}",
            f"ì»¨í…ìŠ¤íŠ¸: {json.dumps(context, ensure_ascii=False, indent=2)}"
        ]
        
        if user_profile:
            message_parts.append(f"ì‚¬ìš©ì í”„ë¡œí•„: {json.dumps(user_profile, ensure_ascii=False, indent=2)}")
        
        message_parts.append("ê° ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ ì „ë¬¸ ì˜ì—­ì—ì„œ ì˜ê²¬ì„ ì œì‹œí•˜ê³ , ìµœì¢… í•©ì˜ì•ˆì„ ë„ì¶œí•´ì£¼ì„¸ìš”.")
        
        return "\n\n".join(message_parts)
    
    async def _run_async_chat(self, user_proxy, manager, message: str) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì±„íŒ… ì‹¤í–‰"""
        try:
            # AutoGenì˜ ì±„íŒ…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                lambda: user_proxy.initiate_chat(manager, message=message)
            )
            
            # ì±„íŒ… ê²°ê³¼ì—ì„œ í•©ì˜ ë‚´ìš© ì¶”ì¶œ
            return self._extract_consensus_from_chat(manager.groupchat.messages)
            
        except Exception as e:
            return {"error": "Chat execution failed", "details": str(e)}
    
    def _extract_consensus_from_chat(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì±„íŒ… ë©”ì‹œì§€ì—ì„œ í•©ì˜ ê²°ê³¼ ì¶”ì¶œ"""
        try:
            # ë§ˆì§€ë§‰ ëª‡ ê°œ ë©”ì‹œì§€ì—ì„œ í•©ì˜ ë‚´ìš© ì°¾ê¸°
            consensus_messages = messages[-5:] if len(messages) >= 5 else messages
            
            # ê° ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì˜ê²¬ ìˆ˜ì§‘
            agent_opinions = {}
            final_consensus = ""
            
            for msg in reversed(consensus_messages):
                agent_name = msg.get("name", "unknown")
                content = msg.get("content", "")
                
                if agent_name not in agent_opinions and content:
                    agent_opinions[agent_name] = content
                
                # DecisionModeratorì˜ ìµœì¢… ê²°ë¡  ì°¾ê¸°
                if agent_name == "DecisionModerator" and "ìµœì¢…" in content:
                    final_consensus = content
                    break
            
            return {
                "final_consensus": final_consensus,
                "agent_opinions": agent_opinions,
                "message_count": len(messages),
                "consensus_reached": bool(final_consensus)
            }
            
        except Exception as e:
            return {"error": "Consensus extraction failed", "details": str(e)}
    
    async def generate_adaptive_question(self, conversation_history: List[Dict[str, Any]], 
                                        collected_preferences: Dict[str, Any],
                                        user_input: str) -> Dict[str, Any]:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì‘í˜• ì§ˆë¬¸ ìƒì„±
        
        Args:
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            collected_preferences: í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì„ í˜¸ë„ ì •ë³´
            user_input: ì‚¬ìš©ì ìµœê·¼ ì…ë ¥
            
        Returns:
            ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì •ë³´
        """
        try:
            from ..core.question_generator import QuestionGenerator
            
            question_generator = QuestionGenerator()
            
            # ì‚¬ìš©ì ì…ë ¥ ë¶„ì„
            if conversation_history:
                # ë§ˆì§€ë§‰ ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
                last_question = conversation_history[-1].get("question", "")
                last_category = conversation_history[-1].get("category", "basic_info")
                
                # ì‚¬ìš©ì ë‹µë³€ ë¶„ì„
                analysis_result = question_generator.analyze_user_response(
                    user_input, last_category, collected_preferences
                )
                
                return {
                    "next_question": analysis_result["next_question"],
                    "category": analysis_result["next_category"],
                    "collected_preferences": analysis_result["updated_preferences"],
                    "completeness_score": analysis_result["completeness_score"],
                    "should_continue": question_generator.should_continue_conversation(
                        analysis_result["completeness_score"]
                    )
                }
            else:
                # ì²« ì§ˆë¬¸ ìƒì„±
                initial_question = question_generator.generate_initial_question(user_input)
                return {
                    "next_question": initial_question["question"],
                    "category": initial_question["category"],
                    "collected_preferences": initial_question["collected_preferences"],
                    "completeness_score": 0.0,
                    "should_continue": True
                }
                
        except Exception as e:
            logger.error(f"ì ì‘í˜• ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "next_question": "ì•ˆë…•í•˜ì„¸ìš”! ì·¨ë¯¸ë¥¼ ì°¾ëŠ” ë° ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ê°„ë‹¨í•œ ì§ˆë¬¸ë¶€í„° ì‹œì‘í• ê²Œìš”.",
                "category": "basic_info",
                "collected_preferences": collected_preferences,
                "completeness_score": 0.0,
                "should_continue": True
            } 